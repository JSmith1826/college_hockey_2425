{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH BOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scorigami - Annimated Gif Code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "image_folder = os.path.join('..', 'TEMP', 'IMAGES', 'stich_folder')\n",
    "\n",
    "\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import re  # For extracting numbers from filenames\n",
    "\n",
    "# def create_animated_gif(image_folder, output_gif, total_duration=5, transition_frames=10):\n",
    "#     \"\"\"\n",
    "#     Create an animated GIF from a sequence of images with fade transitions.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - image_folder: Path to the folder containing images (named with leading numbers, e.g., 1_*.png, 2_*.png).\n",
    "#     - output_gif: Path for the output GIF.\n",
    "#     - total_duration: Total duration of the animation in seconds.\n",
    "#     - transition_frames: Number of intermediate frames for transitions between images.\n",
    "#     \"\"\"\n",
    "#     # Load images sorted by the leading number in filenames\n",
    "#     images = sorted(\n",
    "#         [Image.open(os.path.join(image_folder, img)) for img in os.listdir(image_folder) if img.endswith(\".png\")],\n",
    "#         key=lambda x: int(re.match(r\"(\\d+)\", os.path.basename(x.filename)).group(1))  # Extract leading numbers\n",
    "#     )\n",
    "\n",
    "#     # Resize images to a suitable size while maintaining the aspect ratio\n",
    "#     # Set max width and height\n",
    "#     max_width = 800\n",
    "#     max_height = 1080\n",
    "\n",
    "#     for i, img in enumerate(images):\n",
    "#         width, height = img.size\n",
    "#         if width > max_width or height > max_height:\n",
    "#             # Resize the image\n",
    "#             ratio = min(max_width / width, max_height / height)\n",
    "#             new_size = (int(width * ratio), int(height * ratio))\n",
    "#             images[i] = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     # Calculate total frames and duration per frame\n",
    "#     num_images = len(images)\n",
    "#     frames_per_image = total_duration * 1000 // (num_images + (num_images - 1) * transition_frames)\n",
    "#     frame_duration = int(frames_per_image)  # Duration of each frame in milliseconds\n",
    "\n",
    "#     all_frames = []\n",
    "#     for i in range(num_images - 1):\n",
    "#         # Add the current image\n",
    "#         all_frames.append(images[i])\n",
    "        \n",
    "#         # Create transition frames (fade to next image)\n",
    "#         for t in range(1, transition_frames + 1):\n",
    "#             alpha = t / (transition_frames + 1)\n",
    "#             blend_frame = Image.blend(images[i], images[i + 1], alpha)\n",
    "#             all_frames.append(blend_frame)\n",
    "\n",
    "#     # Add the final image\n",
    "#     all_frames.append(images[-1])\n",
    "\n",
    "#     # Save all frames as a GIF\n",
    "#     all_frames[0].save(\n",
    "#         output_gif,\n",
    "#         save_all=True,\n",
    "#         append_images=all_frames[1:],\n",
    "#         duration=frame_duration,\n",
    "#         loop=1\n",
    "#     )\n",
    "\n",
    "# def create_animated_gif(image_folder, output_gif, total_duration=5, transition_frames=10, reverse_order=False):\n",
    "#     \"\"\"\n",
    "#     Create an animated GIF from a sequence of images with fade transitions.\n",
    "\n",
    "#     Parameters:\n",
    "#     - image_folder: Path to the folder containing images (named with leading numbers, e.g., 1_*.png, 2_*.png).\n",
    "#     - output_gif: Path for the output GIF.\n",
    "#     - total_duration: Total duration of the animation in seconds.\n",
    "#     - transition_frames: Number of intermediate frames for transitions between images.\n",
    "#     - reverse_order: If True, creates the GIF in reverse order.\n",
    "#     \"\"\"\n",
    "#     # Load images sorted by the leading number in filenames\n",
    "#     images = sorted(\n",
    "#         [Image.open(os.path.join(image_folder, img)) for img in os.listdir(image_folder) if img.endswith(\".png\")],\n",
    "#         key=lambda x: int(re.match(r\"(\\d+)\", os.path.basename(x.filename)).group(1))  # Extract leading numbers\n",
    "#     )\n",
    "\n",
    "#     # Reverse the order if requested\n",
    "#     if reverse_order:\n",
    "#         images.reverse()\n",
    "\n",
    "#     # Resize images to a suitable size while maintaining the aspect ratio\n",
    "#     max_width = 800\n",
    "#     max_height = 1080\n",
    "#     for i, img in enumerate(images):\n",
    "#         width, height = img.size\n",
    "#         if width > max_width or height > max_height:\n",
    "#             ratio = min(max_width / width, max_height / height)\n",
    "#             new_size = (int(width * ratio), int(height * ratio))\n",
    "#             images[i] = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "#     # Calculate total frames and duration per frame\n",
    "#     num_images = len(images)\n",
    "#     frames_per_image = total_duration * 1000 // (num_images + (num_images - 1) * transition_frames)\n",
    "#     frame_duration = int(frames_per_image)\n",
    "\n",
    "#     all_frames = []\n",
    "#     for i in range(num_images - 1):\n",
    "#         all_frames.append(images[i])\n",
    "#         for t in range(1, transition_frames + 1):\n",
    "#             alpha = t / (transition_frames + 1)\n",
    "#             blend_frame = Image.blend(images[i], images[i + 1], alpha)\n",
    "#             all_frames.append(blend_frame)\n",
    "\n",
    "#     all_frames.append(images[-1])\n",
    "\n",
    "#     # Save all frames as a GIF\n",
    "#     all_frames[0].save(\n",
    "#         output_gif,\n",
    "#         save_all=True,\n",
    "#         append_images=all_frames[1:],\n",
    "#         duration=frame_duration,\n",
    "#         loop=0\n",
    "#     )\n",
    "\n",
    "def create_animated_gif(image_folder, output_gif, total_duration=5, transition_frames=10, reverse_order=False):\n",
    "    \"\"\"\n",
    "    Create an animated GIF with smooth transitions and precise duration control.\n",
    "\n",
    "    Parameters:\n",
    "    - image_folder: Path to the folder containing images (named with leading numbers, e.g., 1_*.png, 2_*.png).\n",
    "    - output_gif: Path for the output GIF.\n",
    "    - total_duration: Total duration of the animation in seconds.\n",
    "    - transition_frames: Number of intermediate frames for transitions between images.\n",
    "    - reverse_order: If True, creates the GIF in reverse order.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from PIL import Image\n",
    "    import re\n",
    "\n",
    "    # Load images sorted by the leading number in filenames\n",
    "    images = sorted(\n",
    "        [Image.open(os.path.join(image_folder, img)) for img in os.listdir(image_folder) if img.endswith(\".png\")],\n",
    "        key=lambda x: int(re.match(r\"(\\d+)\", os.path.basename(x.filename)).group(1))\n",
    "    )\n",
    "\n",
    "    # Reverse the order if requested\n",
    "    if reverse_order:\n",
    "        images.reverse()\n",
    "\n",
    "    # Resize images to maintain aspect ratio\n",
    "    max_width, max_height = 800, 1080\n",
    "    for i, img in enumerate(images):\n",
    "        width, height = img.size\n",
    "        if width > max_width or height > max_height:\n",
    "            ratio = min(max_width / width, max_height / height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            images[i] = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Total frames calculation\n",
    "    num_images = len(images)\n",
    "    total_frames = num_images + (num_images - 1) * transition_frames\n",
    "\n",
    "    # Frame duration in milliseconds\n",
    "    frame_duration = (total_duration * 1000) // total_frames\n",
    "\n",
    "    all_frames = []\n",
    "    durations = []\n",
    "\n",
    "    for i in range(num_images - 1):\n",
    "        # Add the current image as a static frame\n",
    "        all_frames.append(images[i])\n",
    "        durations.append(frame_duration)\n",
    "\n",
    "        # Create transition frames\n",
    "        for t in range(1, transition_frames + 1):\n",
    "            alpha = t / (transition_frames + 1)\n",
    "            blend_frame = Image.blend(images[i], images[i + 1], alpha)\n",
    "            all_frames.append(blend_frame)\n",
    "            durations.append(frame_duration)\n",
    "\n",
    "    # Add the final image\n",
    "    all_frames.append(images[-1])\n",
    "    durations.append(frame_duration)\n",
    "\n",
    "    # Save all frames as a GIF\n",
    "    all_frames[0].save(\n",
    "        output_gif,\n",
    "        save_all=True,\n",
    "        append_images=all_frames[1:],\n",
    "        duration=durations,\n",
    "        # loop=0  # Infinite loop\n",
    "        loop=1  # Loop once\n",
    "    )\n",
    "\n",
    "\n",
    "# # image_folder = \"/path/to/your/image/folder\"\n",
    "output_gif_reverse = os.path.join('..', 'TEMP', 'IMAGES', 'stich_folder', 'scorigami_all_time_reverse_animated.gif')\n",
    "output_gif = os.path.join('..', 'TEMP', 'IMAGES', 'stich_folder', 'scorigami_all_time_animated.gif')\n",
    "# total_duration = 20  # Total duration of the animation in seconds\n",
    "# transition_frames = 15  # Number of fade frames per transition\n",
    "\n",
    "\n",
    "\n",
    "## Reverse Order\n",
    "create_animated_gif(image_folder, output_gif_reverse, total_duration=10, transition_frames=0, reverse_order=False)\n",
    "\n",
    "## Reverse Order\n",
    "create_animated_gif(image_folder, output_gif, total_duration=10, transition_frames=0, reverse_order=True)\n",
    "\n",
    "\n",
    "# # Example Usage\n",
    "# # Set the folder containing images and output GIF path\n",
    "\n",
    "\n",
    "# create_animated_gif(image_folder, output_gif, total_duration, transition_frames)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load Schedult/results data and compare conferences\n",
    "path = os.path.join('..', 'data', 'schedule', 'Week 1 Scores.csv')\n",
    "\n",
    "# Load the data\n",
    "schedule_df = pd.read_csv(path)\n",
    "\n",
    "# filter out exhibition games\n",
    "schedule_df = schedule_df[schedule_df['Conference'] != 'Exhibition']\n",
    "# Clean up Team names (remove ' and periods)\n",
    "schedule_df['Away_Team'] = schedule_df['Away_Team'].str.replace(\"'\", \"\").str.replace(\".\", \"\")\n",
    "schedule_df['Home_Team'] = schedule_df['Home_Team'].str.replace(\"'\", \"\").str.replace(\".\", \"\")\n",
    "# strip leading and trailing spaces\n",
    "schedule_df['Away_Team'] = schedule_df['Away_Team'].str.strip()\n",
    "schedule_df['Home_Team'] = schedule_df['Home_Team'].str.strip()\n",
    "# Drop any rows containing a / or TBD\n",
    "schedule_df = schedule_df[~schedule_df['Away_Team'].str.contains('/')]\n",
    "schedule_df = schedule_df[~schedule_df['Home_Team'].str.contains('/')]\n",
    "schedule_df = schedule_df[~schedule_df['Away_Team'].str.contains('TBD')]\n",
    "schedule_df = schedule_df[~schedule_df['Home_Team'].str.contains('TBD')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the conferences\n",
    "conference_teams = {\n",
    "    'Atlantic': ['Air Force', \"American Intl\", 'Army', 'Bentley', 'Canisius', 'Holy Cross', 'Mercyhurst', \n",
    "                 'Niagara', 'RIT', 'Robert Morris', 'Sacred Heart'],\n",
    "    'Big Ten': ['Michigan', 'Michigan State', 'Minnesota', 'Notre Dame', 'Ohio State', 'Penn State', 'Wisconsin'],\n",
    "    'CCHA': ['Augustana', 'Bemidji State', 'Bowling Green', 'Ferris State', 'Lake Superior', 'Michigan Tech', \n",
    "             'Minnesota State', 'Northern Michigan', 'St Thomas'],\n",
    "    'ECAC': ['Brown', 'Clarkson', 'Colgate', 'Cornell', 'Dartmouth', 'Harvard', 'Princeton', 'Quinnipiac',\n",
    "             'Rensselaer', 'St Lawrence', 'Union', 'Yale'],\n",
    "    'Hockey East': ['Boston College', 'Boston University', 'Connecticut', 'Maine', 'Massachusetts', 'Mass Lowell',\n",
    "                    'Merrimack', 'New Hampshire', 'Northeastern', 'Providence', 'Vermont'],\n",
    "    'NCHC': ['Arizona State', 'Colorado College', 'Denver', 'Miami', 'Minnesota Duluth', 'North Dakota', 'Omaha', \n",
    "             'St Cloud State', 'Western Michigan'],\n",
    "    'Independents': ['Alaska Anchorage', 'Alaska', 'Lindenwood', 'Long Island', 'Stonehill']\n",
    "}\n",
    "\n",
    "# Function to get the conference of a team\n",
    "def get_conference(team):\n",
    "    for conference, teams in conference_teams.items():\n",
    "        if team in teams:\n",
    "            return conference\n",
    "    return 'Unknown'  # For teams not in the provided lists\n",
    "\n",
    "# Add columns for conference of both the away and home teams\n",
    "schedule_df['Away_Conference'] = schedule_df['Away_Team'].apply(get_conference)\n",
    "schedule_df['Home_Conference'] = schedule_df['Home_Team'].apply(get_conference)\n",
    "\n",
    "# Drop rows with Unknown conferences - Stonehill and Long Island annonmaly\n",
    "schedule_df = schedule_df[schedule_df['Away_Conference'] != 'Unknown']\n",
    "schedule_df = schedule_df[schedule_df['Home_Conference'] != 'Unknown']\n",
    "\n",
    "# Rename to completed_games_df\n",
    "completed_games_df = schedule_df\n",
    "\n",
    "# Matrix for away team wins\n",
    "away_wins_matrix = pd.crosstab(index=completed_games_df['Away_Conference'],\n",
    "                               columns=completed_games_df['Home_Conference'],\n",
    "                               values=(completed_games_df['Away_Score'] > completed_games_df['Home_Score']).astype(int),\n",
    "                               aggfunc='sum', dropna=False)\n",
    "\n",
    "# Matrix for home team wins\n",
    "home_wins_matrix = pd.crosstab(index=completed_games_df['Home_Conference'],\n",
    "                               columns=completed_games_df['Away_Conference'],\n",
    "                               values=(completed_games_df['Home_Score'] > completed_games_df['Away_Score']).astype(int),\n",
    "                               aggfunc='sum', dropna=False)\n",
    "\n",
    "# Transpose the home wins matrix so that it aligns with the away wins matrix for summation\n",
    "home_wins_matrix = home_wins_matrix.T\n",
    "\n",
    "# Sum both matrices to get the total wins\n",
    "total_wins_matrix = away_wins_matrix.add(home_wins_matrix, fill_value=0)\n",
    "# total_wins_matrix = total_wins_matrix.astype(int) # Convert to integers\n",
    "\n",
    "\n",
    "# Display the results matrix\n",
    "print(total_wins_matrix)\n",
    "# calculate and print the total number of games played\n",
    "total_games = total_wins_matrix.sum().sum()\n",
    "print(f'Total games played: {total_games}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the team without a conference\n",
    "# # Display rows with 'Unknown' in either column\n",
    "# unknown_teams = schedule_df[(schedule_df['Away_Conference'] == 'Unknown') | (schedule_df['Home_Conference'] == 'Unknown')]\n",
    "# print(len(unknown_teams))  # Number of rows with unknown teams\n",
    "\n",
    "# # value count of unknown teams\n",
    "# print(unknown_teams['Away_Team'].value_counts())\n",
    "# print(unknown_teams['Home_Team'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join 2023 Player Stats to 2024 Rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "## Path to the data\n",
    "roster_path = os.path.join(\"..\", \"data\", \"roster_2024_current_v3.csv\")\n",
    "stat_path = os.path.join(\"..\", \"data\", \"player_stats_2023_v1.csv\")\n",
    "\n",
    "# Load the data\n",
    "roster_df = pd.read_csv(roster_path)\n",
    "stat_df = pd.read_csv(stat_path)\n",
    "\n",
    "# Check the data\n",
    "roster_df.head()\n",
    "# stat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split stats Clean_Player into first and last name\n",
    "stat_df['First_Name'] = stat_df['Clean_Player'].str.split(\" \").str[0]\n",
    "stat_df['Last_Name'] = stat_df['Clean_Player'].str.split(\" \").str[1:]\n",
    "\n",
    "\n",
    "stat_df['Last_Name'] = stat_df['Last_Name'].str[0].str.replace('[','').str.replace(']','') # Remove the brackets from the last name\n",
    "# Remove periods dashes ect from both names\n",
    "stat_df['First_Name'] = stat_df['First_Name'].str.replace('.','').str.replace('-',' ')\n",
    "stat_df['Last_Name'] = stat_df['Last_Name'].str.replace('.','').str.replace('-',' ')\n",
    "roster_df['First_Name'] = roster_df['First_Name'].str.replace('.','').str.replace('-',' ')\n",
    "roster_df['Last_Name'] = roster_df['Last_Name'].str.replace('.','').str.replace('-',' ')\n",
    "# strip white space\n",
    "stat_df['First_Name'] = stat_df['First_Name'].str.strip()\n",
    "stat_df['Last_Name'] = stat_df['Last_Name'].str.strip()\n",
    "roster_df['First_Name'] = roster_df['First_Name'].str.strip()\n",
    "roster_df['Last_Name'] = roster_df['Last_Name'].str.strip()\n",
    "\n",
    "# Rename Team to Team_2023 for clarity\n",
    "stat_df.rename(columns={'Team':'Team_2023'}, inplace=True)\n",
    "# Rename Current_Team to Team_2024 for clarity\n",
    "roster_df.rename(columns={'Current Team':'Team_2024'}, inplace=True)\n",
    "\n",
    "stat_df.head()\n",
    "# OUTPUT THE DATA TO TEMP CSVs\n",
    "roster_df.to_csv(os.path.join(\"..\", \"TEMP\", \"TEST_roster_2024_current_v4.csv\"), index=False)\n",
    "stat_df.to_csv(os.path.join(\"..\", \"TEMP\", \"TEST_player_stats_2023_v2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try a quick merge\n",
    "merged_df = pd.merge(roster_df, stat_df, left_on=['First_Name', 'Last_Name'], right_on=['First_Name', 'Last_Name'], how='outer', suffixes=('_2024', '_2023'))\n",
    "merged_df.head()\n",
    "\n",
    "# Print report of the merge\n",
    "print(f\"Number of players in the roster: {len(roster_df)}\")\n",
    "print(f\"Number of players in the stats: {len(stat_df)}\")\n",
    "print(f\"Number of players in the merged data: {len(merged_df)}\")\n",
    "\n",
    "\n",
    "# Find Number Number of players whos Team_2023 does not match Team_2024\n",
    "mismatched_teams = merged_df[merged_df['Team_2023'] != merged_df['Team_2024']]\n",
    "print(f\"Number of players with mismatched teams: {len(mismatched_teams)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(merged_df.info())\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop players who aren't playing this year (No Team_2024)\n",
    "merged_df = merged_df.dropna(subset=['Team_2024'])\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert all number columns to int\n",
    "int_columns = ['No', 'Height_Inches', 'Wt', 'Draft_Year', 'D_Round', \n",
    "               'G', 'A', 'Pts', 'plus_minus', 'Sh', 'PIM', 'Games_Played']\n",
    "\n",
    "for col in int_columns:\n",
    "    merged_df[col] = merged_df[col].astype('Int64')\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUT CSV TO TEMP FOR INSPECTION\n",
    "merged_df.to_csv(os.path.join(\"..\", \"data\", \"roster_2024_with_2023_stats.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform congressional demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import geopandas as gpd\n",
    "\n",
    "\n",
    "# ## PATHS ##\n",
    "# # ## 118 Congress Shapefile\n",
    "# # shape_path = os.path.join('..', 'data', 'vault', '118th_congress', 'USA_118th_Congressional_Districts.shp')\n",
    "# # ## Load Shapefile\n",
    "# # gdf = gpd.read_file(shape_path)\n",
    "\n",
    "# # Income data table - 5 Year Average 2022\n",
    "# income_path = os.path.join('..', 'data', 'vault', '118th_congress', 'income_data', 'ACSST5Y2022.S1903-Data.csv')\n",
    "# income_df = pd.read_csv(income_path, skiprows=1) # Load Income Data\n",
    "\n",
    "# # Summary table with Populations and Representative Names\n",
    "# summary_path = os.path.join('..', 'data', 'vault', 'USA_118th_Congressional_Districts_info_table.csv')\n",
    "# summary_df = pd.read_csv(summary_path)\n",
    "\n",
    "# # Check \n",
    "# # gdf.head()\n",
    "# # income_df.head()\n",
    "# # summary_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check \n",
    "# gdf.head()\n",
    "# income_df.head()\n",
    "# # summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate Image icons so they are all 300 x 300 px squares\n",
    "- making sure they are all squares will make resizing issues easier later on\n",
    "    - The aspect ratio is getting screwed up during resizing for icons that are not square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "\n",
    "# Directory where the logos are stored\n",
    "logo_dir = os.path.join('..', 'images', 'logos')\n",
    "\n",
    "# Make logos square by adding transparent space equally on both sides\n",
    "for logo_file in os.listdir(logo_dir):\n",
    "    logo_path = os.path.join(logo_dir, logo_file)\n",
    "    \n",
    "    # Check if the path is a file and not a directory\n",
    "    if os.path.isfile(logo_path):\n",
    "        with Image.open(logo_path) as img:\n",
    "            # Ensure the image has an alpha channel (for transparency)\n",
    "            img = img.convert(\"RGBA\")\n",
    "            \n",
    "            width, height = img.size\n",
    "            \n",
    "            # If the image is already square, no changes are needed\n",
    "            if width == height:\n",
    "                continue\n",
    "            \n",
    "            # Calculate padding to add on the shorter side to make the image square\n",
    "            if width > height:\n",
    "                padding = (width - height) // 2\n",
    "                new_img = ImageOps.expand(img, border=(0, padding, 0, padding), fill=(0, 0, 0, 0))\n",
    "            else:\n",
    "                padding = (height - width) // 2\n",
    "                new_img = ImageOps.expand(img, border=(padding, 0, padding, 0), fill=(0, 0, 0, 0))\n",
    "            \n",
    "            # Save the padded square image, overwriting the original\n",
    "            new_img.save(logo_path)\n",
    "\n",
    "print(\"All logos made square by adding transparent space equally to each side.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the coordinates for the rinks in arena_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dependencies\n",
    "# import os\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Path to arena file\n",
    "# arena_file = os.path.join('..','data', 'arena_school_info.csv')\n",
    "# arena_df = pd.read_csv(arena_file)\n",
    "\n",
    "# # Open Roster File To Clean State/Provences Names\n",
    "# roster_file = os.path.join('..','data', 'roster_2024_current_v2.csv')\n",
    "# roster_df = pd.read_csv(roster_file)\n",
    "\n",
    "# roster_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get list of Unique State/Province Names\n",
    "# unique_states = roster_df['State_Province'].unique()\n",
    "# unique_states\n",
    "\n",
    "# ## Dictionary to standardize state/province names\n",
    "\n",
    "# standardized_locations = {\n",
    "#     'Ont.': 'Ontario', 'Mich.': 'Michigan', 'Mass.': 'Massachusetts', 'Minn.': 'Minnesota', \n",
    "#     'Wis.': 'Wisconsin', 'Sweden': 'Sweden', 'Germany': 'Germany', 'B.C.': 'British Columbia',\n",
    "#     'N.Y.': 'New York', 'Wash.': 'Washington', 'Que.': 'Quebec', 'Alb.': 'Alberta', \n",
    "#     'N.J.': 'New Jersey', 'Sask.': 'Saskatchewan', 'Conn.': 'Connecticut', 'Mo.': 'Missouri',\n",
    "#     'Texas': 'Texas', 'Calif.': 'California', 'DC': 'District of Columbia', 'Fla.': 'Florida',\n",
    "#     'Ohio': 'Ohio', 'Ill.': 'Illinois', 'Pa.': 'Pennsylvania', 'Ga.': 'Georgia',\n",
    "#     'Mont.': 'Montana', 'Tenn.': 'Tennessee', 'Colo.': 'Colorado', 'Va.': 'Virginia', \n",
    "#     'Vt.': 'Vermont', 'R.I.': 'Rhode Island', 'Md.': 'Maryland', 'Ariz.': 'Arizona', \n",
    "#     'Wisc.': 'Wisconsin', 'Iowa': 'Iowa', 'Man.': 'Manitoba', 'Slovakia': 'Slovakia', \n",
    "#     'N.D.': 'North Dakota', 'N.C.': 'North Carolina', 'P.E.I.': 'Prince Edward Island',\n",
    "#     'N.H.': 'New Hampshire', 'Alaska': 'Alaska', 'Belarus': 'Belarus', 'MB': 'Manitoba',\n",
    "#     'Russia': 'Russia', 'Finland': 'Finland', 'Newf.': 'Newfoundland and Labrador', \n",
    "#     'Hungary': 'Hungary', 'SUI': 'Switzerland', 'S.C.': 'South Carolina', 'Latvia': 'Latvia',\n",
    "#     'Czech Republic': 'Czech Republic', 'N.B.': 'New Brunswick', 'Great Britain': 'United Kingdom', \n",
    "#     'NB': 'New Brunswick', 'Norway': 'Norway', 'N.S.': 'Nova Scotia', 'Ind.': 'Indiana', \n",
    "#     'NWT': 'Northwest Territories', 'AUT': 'Austria', 'Idaho': 'Idaho', 'S.D.': 'South Dakota', \n",
    "#     'Switzerland': 'Switzerland', 'Ore.': 'Oregon', 'Wyo.': 'Wyoming', 'Utah': 'Utah', \n",
    "#     'ITA': 'Italy', 'Slovenia': 'Slovenia', 'YT': 'Yukon', 'Del.': 'Delaware', 'Maine': 'Maine',\n",
    "#     'Poland': 'Poland', 'Yukon': 'Yukon', 'Ukraine': 'Ukraine', 'Japan': 'Japan', 'Neb.': 'Nebraska'\n",
    "# }\n",
    "\n",
    "# ## Apply the standardization to the State/Province column\n",
    "# roster_df['State_Province'] = roster_df['State_Province'].replace(standardized_locations)\n",
    "\n",
    "# # Check the unique values after standardization\n",
    "# roster_df['State_Province'].unique()\n",
    "# print(roster_df['State_Province'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output the cleaned roster to a new CSV file\n",
    "cleaned_roster_file = os.path.join('..','data', 'roster_cleaned_state_prov_2024.csv')\n",
    "roster_df.to_csv(cleaned_roster_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the function to check the location using Google Places API\n",
    "# def check_location(lat, lng, api_key):\n",
    "#     url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "#     params = {\n",
    "#         'location': f'{lat},{lng}',\n",
    "#         'radius': 500,  # Distance in meters from the provided coordinates\n",
    "#         'type': 'stadium',  # Filter search to stadiums/arenas\n",
    "#         'key': api_key\n",
    "#     }\n",
    "    \n",
    "#     # Debugging: Print the URL and parameters\n",
    "#     print(f\"Requesting URL: {url}\")\n",
    "#     print(f\"Parameters: {params}\")\n",
    "    \n",
    "#     response = requests.get(url, params=params)\n",
    "    \n",
    "#     # Debugging: Print the response code and content\n",
    "#     print(f\"Response status code: {response.status_code}\")\n",
    "#     print(f\"Response content: {response.text}\")\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         results = response.json().get('results')\n",
    "#         if results:\n",
    "#             return results[0].get('name'), results[0].get('vicinity')\n",
    "#         else:\n",
    "#             return None, \"No results found\"\n",
    "#     else:\n",
    "#         return None, f\"API request failed with status {response.status_code}\"\n",
    "\n",
    "# # Define the function to verify coordinates in the DataFrame\n",
    "# def verify_coordinates(df, api_key):\n",
    "#     results = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         lat = row['Latitude']\n",
    "#         lng = row['Longitude']\n",
    "#         arena_name = row['Arena']\n",
    "        \n",
    "#         # Debugging: Print the current coordinates and arena being checked\n",
    "#         print(f\"Checking coordinates for arena: {arena_name}\")\n",
    "#         print(f\"Latitude: {lat}, Longitude: {lng}\")\n",
    "        \n",
    "#         # Get the name and vicinity of the nearest stadium/arena\n",
    "#         name, vicinity = check_location(lat, lng, api_key)\n",
    "        \n",
    "#         # Append the original data and verification results\n",
    "#         results.append({\n",
    "#             'Arena': arena_name,\n",
    "#             'Latitude': lat,\n",
    "#             'Longitude': lng,\n",
    "#             'Google Places Name': name,\n",
    "#             'Vicinity': vicinity\n",
    "#         })\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# # Load your API key\n",
    "\n",
    "\n",
    "# # Assuming arena_df is your DataFrame\n",
    "# verified_df = verify_coordinates(arena_df, api_key)\n",
    "\n",
    "# # Output the results\n",
    "# print(verified_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verified_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Version 2 of Arena Location verifications\n",
    "# ## Returns 5 closest Google Places to coordinates given\n",
    "\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the function to check the 5 closest places using Google Places API\n",
    "# def check_nearby_places(lat, lng, api_key):\n",
    "#     url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "#     params = {\n",
    "#         'location': f'{lat},{lng}',\n",
    "#         'radius': 500,  # Distance in meters from the provided coordinates\n",
    "#         'key': api_key\n",
    "#     }\n",
    "    \n",
    "#     # Debugging: Print the URL and parameters being sent to the API\n",
    "#     print(f\"Requesting places near lat: {lat}, lng: {lng}\")\n",
    "#     print(f\"Request URL: {url}\")\n",
    "#     print(f\"Parameters: {params}\")\n",
    "    \n",
    "#     response = requests.get(url, params=params)\n",
    "    \n",
    "#     # Debugging: Print the response status and content\n",
    "#     print(f\"Response status code: {response.status_code}\")\n",
    "#     print(f\"Response content: {response.text}\\n\")  # This shows the full response from the API\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         results = response.json().get('results')\n",
    "#         if results:\n",
    "#             # Return the top 5 closest places\n",
    "#             return [(result.get('name'), result.get('vicinity')) for result in results[:5]]\n",
    "#         else:\n",
    "#             return [(\"None\", \"No results found\")]\n",
    "#     else:\n",
    "#         return [(\"None\", f\"API request failed with status {response.status_code}\")]\n",
    "\n",
    "# # Define the function to verify coordinates and return the 5 closest places\n",
    "# def verify_coordinates(df, api_key):\n",
    "#     results = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         lat = row['Latitude']\n",
    "#         lng = row['Longitude']\n",
    "#         arena_name = row['Arena']\n",
    "#         school_name = row['School']\n",
    "        \n",
    "#         # Debugging: Print the current arena and coordinates being checked\n",
    "#         print(f\"\\nChecking nearby places for arena: {arena_name} (School: {school_name})\")\n",
    "#         print(f\"Latitude: {lat}, Longitude: {lng}\")\n",
    "        \n",
    "#         # Get the 5 closest places\n",
    "#         nearby_places = check_nearby_places(lat, lng, api_key)\n",
    "        \n",
    "#         # Add each place to the results, along with the original data\n",
    "#         for place in nearby_places:\n",
    "#             results.append({\n",
    "#                 'Arena': arena_name,\n",
    "#                 'School': school_name,\n",
    "#                 'Latitude': lat,\n",
    "#                 'Longitude': lng,\n",
    "#                 'Google Places Name': place[0],\n",
    "#                 'Vicinity': place[1]\n",
    "#             })\n",
    "            \n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# # Load your API key\n",
    "# api_key = ''\n",
    "\n",
    "# # Assuming arena_df is your DataFrame\n",
    "# verified_df = verify_coordinates(arena_df, api_key)\n",
    "\n",
    "# # Output the results\n",
    "# print(verified_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_df.head(10)\n",
    "\n",
    "## OUTPUT TO TEMP FOLDER FOR MANUAL REVIEW\n",
    "# output_file = os.path.join('..','TEMP', 'arena_school_info_place_checkV3.csv')\n",
    "# verified_df.to_csv(output_file, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
