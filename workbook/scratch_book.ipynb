{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH BOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create player_stats_ytd table \n",
    "- already in scraper workbook just needed it for a one off and brought it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore scoring table, tally player goals by situation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "player_stats_ytd\n",
      "master_roster\n",
      "advanced_metrics\n",
      "game_details\n",
      "goalie_stats\n",
      "line_chart\n",
      "linescore\n",
      "penalty_summary\n",
      "player_stats\n",
      "scoring_summary\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Period",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Team",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PP",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Player",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Player_Goals",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Assist1",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Assist2",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "Time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Game_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Away_Team",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Home_Team",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a6446de5-6a34-4a23-b729-71fcce9ab7c4",
       "rows": [
        [
         "0",
         "1st Period",
         "Michigan State",
         "",
         "Isaac Howard",
         "1",
         "Daniel Russell",
         null,
         "13:57",
         "2024-10-04-Michigan State-Lake Superior",
         "Michigan State",
         "Lake Superior"
        ],
        [
         "1",
         "1st Period",
         "Lake Superior",
         "",
         "Connor Milburn",
         "1",
         "Dawson Tritt",
         "Blake Humphrey",
         "16:00",
         "2024-10-04-Michigan State-Lake Superior",
         "Michigan State",
         "Lake Superior"
        ],
        [
         "2",
         "Overtime",
         "Michigan State",
         "3x3",
         "Daniel Russell",
         "1",
         "Isaac Howard",
         "Matt Basgall",
         "0:39",
         "2024-10-04-Michigan State-Lake Superior",
         "Michigan State",
         "Lake Superior"
        ],
        [
         "3",
         "2nd Period",
         "Michigan",
         "PP",
         "William Whitelaw",
         "1",
         "Michael Hage",
         "T.J. Hughes",
         "0:33",
         "2024-10-04-Minnesota State-Michigan",
         "Minnesota State",
         "Michigan"
        ],
        [
         "4",
         "2nd Period",
         "Minnesota State",
         "",
         "Brian Carrabes",
         "1",
         "Campbell Cichosz",
         null,
         "4:25",
         "2024-10-04-Minnesota State-Michigan",
         "Minnesota State",
         "Michigan"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Team</th>\n",
       "      <th>PP</th>\n",
       "      <th>Player</th>\n",
       "      <th>Player_Goals</th>\n",
       "      <th>Assist1</th>\n",
       "      <th>Assist2</th>\n",
       "      <th>Time</th>\n",
       "      <th>Game_ID</th>\n",
       "      <th>Away_Team</th>\n",
       "      <th>Home_Team</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1st Period</td>\n",
       "      <td>Michigan State</td>\n",
       "      <td></td>\n",
       "      <td>Isaac Howard</td>\n",
       "      <td>1</td>\n",
       "      <td>Daniel Russell</td>\n",
       "      <td>None</td>\n",
       "      <td>13:57</td>\n",
       "      <td>2024-10-04-Michigan State-Lake Superior</td>\n",
       "      <td>Michigan State</td>\n",
       "      <td>Lake Superior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st Period</td>\n",
       "      <td>Lake Superior</td>\n",
       "      <td></td>\n",
       "      <td>Connor Milburn</td>\n",
       "      <td>1</td>\n",
       "      <td>Dawson Tritt</td>\n",
       "      <td>Blake Humphrey</td>\n",
       "      <td>16:00</td>\n",
       "      <td>2024-10-04-Michigan State-Lake Superior</td>\n",
       "      <td>Michigan State</td>\n",
       "      <td>Lake Superior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overtime</td>\n",
       "      <td>Michigan State</td>\n",
       "      <td>3x3</td>\n",
       "      <td>Daniel Russell</td>\n",
       "      <td>1</td>\n",
       "      <td>Isaac Howard</td>\n",
       "      <td>Matt Basgall</td>\n",
       "      <td>0:39</td>\n",
       "      <td>2024-10-04-Michigan State-Lake Superior</td>\n",
       "      <td>Michigan State</td>\n",
       "      <td>Lake Superior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2nd Period</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>PP</td>\n",
       "      <td>William Whitelaw</td>\n",
       "      <td>1</td>\n",
       "      <td>Michael Hage</td>\n",
       "      <td>T.J. Hughes</td>\n",
       "      <td>0:33</td>\n",
       "      <td>2024-10-04-Minnesota State-Michigan</td>\n",
       "      <td>Minnesota State</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2nd Period</td>\n",
       "      <td>Minnesota State</td>\n",
       "      <td></td>\n",
       "      <td>Brian Carrabes</td>\n",
       "      <td>1</td>\n",
       "      <td>Campbell Cichosz</td>\n",
       "      <td>None</td>\n",
       "      <td>4:25</td>\n",
       "      <td>2024-10-04-Minnesota State-Michigan</td>\n",
       "      <td>Minnesota State</td>\n",
       "      <td>Michigan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Period             Team   PP            Player  Player_Goals  \\\n",
       "0  1st Period   Michigan State           Isaac Howard             1   \n",
       "1  1st Period    Lake Superior         Connor Milburn             1   \n",
       "2    Overtime   Michigan State  3x3    Daniel Russell             1   \n",
       "3  2nd Period         Michigan   PP  William Whitelaw             1   \n",
       "4  2nd Period  Minnesota State         Brian Carrabes             1   \n",
       "\n",
       "            Assist1         Assist2   Time  \\\n",
       "0    Daniel Russell            None  13:57   \n",
       "1      Dawson Tritt  Blake Humphrey  16:00   \n",
       "2      Isaac Howard    Matt Basgall   0:39   \n",
       "3      Michael Hage     T.J. Hughes   0:33   \n",
       "4  Campbell Cichosz            None   4:25   \n",
       "\n",
       "                                   Game_ID        Away_Team      Home_Team  \n",
       "0  2024-10-04-Michigan State-Lake Superior   Michigan State  Lake Superior  \n",
       "1  2024-10-04-Michigan State-Lake Superior   Michigan State  Lake Superior  \n",
       "2  2024-10-04-Michigan State-Lake Superior   Michigan State  Lake Superior  \n",
       "3      2024-10-04-Minnesota State-Michigan  Minnesota State       Michigan  \n",
       "4      2024-10-04-Minnesota State-Michigan  Minnesota State       Michigan  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "\n",
    "from config import recent_play_by_play, recent_clean_db, last_game_date, tourney_bubble\n",
    "\n",
    "# establish connection to SQLite database\n",
    "conn = sqlite3.connect(recent_clean_db)\n",
    "\n",
    "\n",
    "data_folder = os.path.join('..', 'data') # data folder\n",
    "temp_folder = os.path.join('..', 'TEMP') # temp folder\n",
    "\n",
    "# print list of tables in the database\n",
    "tables = conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\").fetchall()\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])\n",
    "\n",
    "# Load scoring_summary table into a DataFrame\n",
    "scoring_summary_df = pd.read_sql_query(\"SELECT * FROM scoring_summary\", conn)\n",
    "scoring_summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Player             Team  Total_Goals  1st_goals  2nd_goals  \\\n",
      "0      Isaac Howard   Michigan State           24          3          6   \n",
      "1    Connor Milburn    Lake Superior            9          4          2   \n",
      "2    Daniel Russell   Michigan State           12          1          6   \n",
      "3  William Whitelaw         Michigan           11          3          4   \n",
      "4    Brian Carrabes  Minnesota State           10          2          4   \n",
      "\n",
      "   3rd_goals  ot_goals  home_goals  away_goals  5on5_goals  4on4_goals  \\\n",
      "0         14         1           7          17          16           1   \n",
      "1          3         0           5           4           4           0   \n",
      "2          3         2           6           6           4           1   \n",
      "3          4         0           5           6           7           0   \n",
      "4          4         0           5           5           8           0   \n",
      "\n",
      "   3on3_goals  powerplay_goals  shorthanded_goals  empty_net_goals  \\\n",
      "0           1                4                  0                1   \n",
      "1           0                4                  0                2   \n",
      "2           2                2                  1                2   \n",
      "3           0                4                  0                0   \n",
      "4           0                1                  0                1   \n",
      "\n",
      "   extra_attacker_goals  penalty_shots_goals  unassisted  one_assist  \\\n",
      "0                     0                    1           2           6   \n",
      "1                     0                    0           2           1   \n",
      "2                     0                    0           0           1   \n",
      "3                     0                    0           0           2   \n",
      "4                     0                    0           3           1   \n",
      "\n",
      "   two_assist  \n",
      "0          16  \n",
      "1           6  \n",
      "2          11  \n",
      "3           9  \n",
      "4           6  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 12.5   44.44   8.33 ...   0.   100.     0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 25.    22.22  50.   ... 100.     0.   100.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 4.17  0.   16.67 ...  0.    0.    0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[58.33 33.33 25.   ...  0.    0.    0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[4.17 0.   8.33 ... 0.   0.   0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 66.67  44.44  33.33 ... 100.   100.   100.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 70.83  44.44  50.   ...   0.   100.     0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 4.17 22.22 16.67 ...  0.    0.    0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0. 0. 0. ... 0. 0. 0.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 29.17  55.56  50.   ... 100.     0.   100.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[25.   11.11  8.33 ...  0.    0.    0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 4.17  0.   16.67 ...  0.    0.    0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[4.17 0.   0.   ... 0.   0.   0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[16.67 44.44 16.67 ...  0.    0.    0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[0.   0.   8.33 ... 0.   0.   0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 66.67  66.67  91.67 ... 100.   100.   100.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
      "C:\\Users\\jbanc\\AppData\\Local\\Temp\\ipykernel_18208\\3185098601.py:95: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 8.33 22.22  0.   ...  0.    0.    0.  ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n"
     ]
    }
   ],
   "source": [
    "scoring_df = scoring_summary_df.copy()\n",
    "\n",
    "# Clean and preprocess the data\n",
    "scoring_df['PP'] = scoring_df['PP'].fillna('')  # Replace NaN with empty string\n",
    "scoring_df['Assist1'] = scoring_df['Assist1'].fillna('')\n",
    "scoring_df['Assist2'] = scoring_df['Assist2'].fillna('')\n",
    "\n",
    "# Define period mapping\n",
    "period_mapping = {\n",
    "    '1st Period': '1st_goals',\n",
    "    '2nd Period': '2nd_goals',\n",
    "    '3rd Period': '3rd_goals',\n",
    "    'Overtime': 'ot_goals'\n",
    "}\n",
    "\n",
    "# Define game situation mappings\n",
    "game_situation_mapping = {\n",
    "    'PP': 'powerplay_goals',\n",
    "    'SH': 'shorthanded_goals',\n",
    "    '3x5': 'shorthanded_goals',\n",
    "    '4x4': '4on4_goals',\n",
    "    '3x3': '3on3_goals',\n",
    "    '5x3': 'powerplay_goals',\n",
    "    'EA': 'extra_attacker_goals',\n",
    "    'EN': 'empty_net_goals',\n",
    "    'PS': 'penalty_shots_goals'\n",
    "}\n",
    "\n",
    "# Initialize goal tally dictionary\n",
    "goal_tally = {}\n",
    "\n",
    "# Process each row to tally goals\n",
    "for _, row in scoring_df.iterrows():\n",
    "    player = row['Player']\n",
    "    team = row['Team']\n",
    "    \n",
    "    # Unique key for each player-team combination\n",
    "    key = (player, team)\n",
    "    \n",
    "    # Initialize if player not seen before\n",
    "    if key not in goal_tally:\n",
    "        goal_tally[key] = {\n",
    "            'Total_Goals': 0, '1st_goals': 0, '2nd_goals': 0, '3rd_goals': 0, 'ot_goals': 0,\n",
    "            'home_goals': 0, 'away_goals': 0, '5on5_goals': 0, '4on4_goals': 0, '3on3_goals': 0,\n",
    "            'powerplay_goals': 0, 'shorthanded_goals': 0, 'empty_net_goals': 0, \n",
    "            'extra_attacker_goals': 0, 'penalty_shots_goals': 0, 'unassisted_goals': 0,\n",
    "            'one_assist_goals': 0, 'two_assist_goals': 0\n",
    "        }\n",
    "    \n",
    "    # Increment total goals\n",
    "    goal_tally[key]['Total_Goals'] += 1\n",
    "    \n",
    "    # Tally by period\n",
    "    if row['Period'] in period_mapping:\n",
    "        goal_tally[key][period_mapping[row['Period']]] += 1\n",
    "\n",
    "    \n",
    "    # Tally by location - Very Basic home/away\n",
    "    if row['Home_Team'] == team:\n",
    "        goal_tally[key]['home_goals'] += 1\n",
    "    else:\n",
    "        goal_tally[key]['away_goals'] += 1\n",
    "    ### Could be improved to nuetral site games by looking up game details by game id\n",
    "    \n",
    "    # Tally by game situation\n",
    "    game_situations = row['PP'].split(',')\n",
    "    if row['PP'] == '':\n",
    "        goal_tally[key]['5on5_goals'] += 1  # Default 5-on-5 goal\n",
    "    else:\n",
    "        for situation in game_situations:\n",
    "            situation = situation.strip()\n",
    "            if situation in game_situation_mapping:\n",
    "                goal_tally[key][game_situation_mapping[situation]] += 1\n",
    "\n",
    "    # Tally by assist count\n",
    "    if row['Assist1'] == '' and row['Assist2'] == '':\n",
    "        goal_tally[key]['unassisted'] += 1\n",
    "    elif row['Assist1'] != '' and row['Assist2'] == '':\n",
    "        goal_tally[key]['one_assist'] += 1\n",
    "    else:\n",
    "        goal_tally[key]['two_assist'] += 1\n",
    "\n",
    "# Convert goal tally dictionary to DataFrame\n",
    "goal_tally_df = pd.DataFrame.from_dict(goal_tally, orient='index').reset_index()\n",
    "goal_tally_df.columns = ['Player', 'Team'] + list(goal_tally_df.columns[2:])\n",
    "\n",
    "\n",
    "# Convert tally counts to percentages of total goals\n",
    "goal_tally_by_pct_df = goal_tally_df.copy()\n",
    "\n",
    "# Exclude non-numeric columns\n",
    "numeric_cols = goal_tally_by_pct_df.columns[2:]\n",
    "\n",
    "# Convert each column (except 'Total_Goals') to a percentage of total goals\n",
    "goal_tally_by_pct_df.loc[:, numeric_cols.difference(['Total_Goals'])] = (\n",
    "    goal_tally_by_pct_df[numeric_cols.difference(['Total_Goals'])]\n",
    "    .div(goal_tally_by_pct_df[\"Total_Goals\"], axis=0) * 100\n",
    ").round(2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(goal_tally_df.head())\n",
    "\n",
    "# Save the DataFrames to a CSV file\n",
    "goal_tally_df.to_csv(os.path.join(temp_folder, 'individual_goal_tally.csv'), index=False)\n",
    "goal_tally_by_pct_df.to_csv(os.path.join(temp_folder, 'individual_goal_tally_by_pct.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hotfix for problems with punctuation in team names\n",
    "\n",
    "# Replace punctuation in team names (columns Team, Home_Team, Away_Team)\n",
    "scoring_summary_df['Team'] = scoring_summary_df['Team'].str.replace('.', '')\n",
    "scoring_summary_df['Home_Team'] = scoring_summary_df['Home_Team'].str.replace('.', '')\n",
    "scoring_summary_df['Away_Team'] = scoring_summary_df['Away_Team'].str.replace('.', '')\n",
    "\n",
    "scoring_summary_df['Team'] = scoring_summary_df['Team'].str.replace(\"'\", '')\n",
    "scoring_summary_df['Home_Team'] = scoring_summary_df['Home_Team'].str.replace(\"'\", '')\n",
    "scoring_summary_df['Away_Team'] = scoring_summary_df['Away_Team'].str.replace(\"'\", '')\n",
    "\n",
    "scoring_df = scoring_summary_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Game_ID           Team             Player  \\\n",
      "0  2024-10-04-Arizona State-Air Force  Arizona State    Bennett Schimek   \n",
      "1  2024-10-04-Arizona State-Air Force  Arizona State      Dylan Jackson   \n",
      "2  2024-10-04-Arizona State-Air Force  Arizona State    Bennett Schimek   \n",
      "3  2024-10-04-Arizona State-Air Force      Air Force  Clayton Cosentino   \n",
      "4  2024-10-04-Arizona State-Air Force  Arizona State        Kyle Smolen   \n",
      "\n",
      "       Period   Time Score_Before  Game_Situation  \n",
      "0  1st Period  12:30          0-0            Tied  \n",
      "1  1st Period  15:57          1-0    Leading by 1  \n",
      "2  2nd Period   5:26          2-0   Leading by 2+  \n",
      "3  2nd Period   8:16          0-3  Trailing by 2+  \n",
      "4  2nd Period  15:20          3-1   Leading by 2+  \n"
     ]
    }
   ],
   "source": [
    "# Convert time to a sortable format (MM:SS to total seconds)\n",
    "def time_to_seconds(time_str):\n",
    "    \"\"\"Convert MM:SS time format to total seconds.\"\"\"\n",
    "    if isinstance(time_str, str):\n",
    "        minutes, seconds = map(int, time_str.split(\":\"))\n",
    "        return minutes * 60 + seconds\n",
    "    return 0\n",
    "\n",
    "# Add a sortable time column\n",
    "scoring_df[\"Time_Seconds\"] = scoring_df[\"Time\"].apply(time_to_seconds)\n",
    "\n",
    "# Sort data by game, period, and time\n",
    "period_order = {\"1st Period\": 1, \"2nd Period\": 2, \"3rd Period\": 3, \"Overtime\": 4}\n",
    "scoring_df[\"Period_Order\"] = scoring_df[\"Period\"].map(period_order)\n",
    "\n",
    "scoring_df = scoring_df.sort_values(by=[\"Game_ID\", \"Period_Order\", \"Time_Seconds\"]).reset_index(drop=True)\n",
    "\n",
    "# Initialize tracking variables\n",
    "game_scores = {}  # {Game_ID: {Team_A: score, Team_B: score}}\n",
    "score_records = []\n",
    "\n",
    "# Process each game sequentially\n",
    "for _, row in scoring_df.iterrows():\n",
    "    game_id = row[\"Game_ID\"]\n",
    "    team_scoring = row[\"Team\"]\n",
    "    home_team = row[\"Home_Team\"]\n",
    "    away_team = row[\"Away_Team\"]\n",
    "\n",
    "    # Initialize game score if first goal in game\n",
    "    if game_id not in game_scores:\n",
    "        game_scores[game_id] = {home_team: 0, away_team: 0}\n",
    "\n",
    "    # Get the score before the goal\n",
    "    score_before = game_scores[game_id].copy()\n",
    "\n",
    "    # Update score for the scoring team\n",
    "    game_scores[game_id][team_scoring] += 1\n",
    "\n",
    "    # Determine score difference before goal\n",
    "    team_score_before = score_before[team_scoring]\n",
    "    opponent_team = home_team if team_scoring == away_team else away_team\n",
    "    opponent_score_before = score_before[opponent_team]\n",
    "\n",
    "    score_diff = team_score_before - opponent_score_before\n",
    "\n",
    "    # Classify goal situation\n",
    "    if score_diff >= 2:\n",
    "        situation = \"Leading by 2+\"\n",
    "    elif score_diff == 1:\n",
    "        situation = \"Leading by 1\"\n",
    "    elif score_diff == 0:\n",
    "        situation = \"Tied\"\n",
    "    elif score_diff == -1:\n",
    "        situation = \"Trailing by 1\"\n",
    "    else:\n",
    "        situation = \"Trailing by 2+\"\n",
    "\n",
    "    # Store result\n",
    "    score_records.append({\n",
    "        \"Game_ID\": game_id,\n",
    "        \"Team\": team_scoring,\n",
    "        \"Player\": row[\"Player\"],\n",
    "        \"Period\": row[\"Period\"],\n",
    "        \"Time\": row[\"Time\"],\n",
    "        \"Score_Before\": f\"{team_score_before}-{opponent_score_before}\",\n",
    "        \"Game_Situation\": situation\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "score_summary_df = pd.DataFrame(score_records)\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(score_summary_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Player",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Team",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Goals_Leading_by_2+",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Goals_Leading_by_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Goals_Tied",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Goals_Trailing_by_1",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Goals_Trailing_by_2+",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total_Goals",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "f587d188-a4ec-44eb-9a82-f872ec7aa899",
       "rows": [
        [
         "0",
         "A.J. Hodges",
         "Bentley",
         "1",
         "1",
         "3",
         "2",
         "1",
         "8"
        ],
        [
         "1",
         "Aaron Bohlinger",
         "Quinnipiac",
         "1",
         "0",
         "1",
         "1",
         "0",
         "3"
        ],
        [
         "2",
         "Aaron Huglen",
         "Minnesota",
         "0",
         "4",
         "1",
         "0",
         "0",
         "5"
        ],
        [
         "3",
         "Aaron Pionk",
         "Minnesota Duluth",
         "0",
         "0",
         "3",
         "0",
         "0",
         "3"
        ],
        [
         "4",
         "Aaron Schwartz",
         "Quinnipiac",
         "2",
         "6",
         "1",
         "2",
         "1",
         "12"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Goals_Leading_by_2+</th>\n",
       "      <th>Goals_Leading_by_1</th>\n",
       "      <th>Goals_Tied</th>\n",
       "      <th>Goals_Trailing_by_1</th>\n",
       "      <th>Goals_Trailing_by_2+</th>\n",
       "      <th>Total_Goals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A.J. Hodges</td>\n",
       "      <td>Bentley</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aaron Bohlinger</td>\n",
       "      <td>Quinnipiac</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aaron Huglen</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaron Pionk</td>\n",
       "      <td>Minnesota Duluth</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aaron Schwartz</td>\n",
       "      <td>Quinnipiac</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Player              Team  Goals_Leading_by_2+  Goals_Leading_by_1  \\\n",
       "0      A.J. Hodges           Bentley                    1                   1   \n",
       "1  Aaron Bohlinger        Quinnipiac                    1                   0   \n",
       "2     Aaron Huglen         Minnesota                    0                   4   \n",
       "3      Aaron Pionk  Minnesota Duluth                    0                   0   \n",
       "4   Aaron Schwartz        Quinnipiac                    2                   6   \n",
       "\n",
       "   Goals_Tied  Goals_Trailing_by_1  Goals_Trailing_by_2+  Total_Goals  \n",
       "0           3                    2                     1            8  \n",
       "1           1                    1                     0            3  \n",
       "2           1                    0                     0            5  \n",
       "3           3                    0                     0            3  \n",
       "4           1                    2                     1           12  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by Player and Team, then tally goals by game situation\n",
    "goal_situation_tally = score_summary_df.groupby([\"Player\", \"Team\", \"Game_Situation\"]).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "goal_situation_tally.columns = [\n",
    "    \"Player\", \"Team\", \"Goals_Leading_by_2+\", \"Goals_Leading_by_1\",\n",
    "    \"Goals_Tied\", \"Goals_Trailing_by_1\", \"Goals_Trailing_by_2+\"\n",
    "]\n",
    "\n",
    "# Add a total goals column for reference\n",
    "goal_situation_tally[\"Total_Goals\"] = goal_situation_tally.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "# Display the processed DataFrame\n",
    "goal_situation_tally.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Player              Team  Total_Assists  Assists_Leading by 1  \\\n",
      "0      A.J. Hodges           Bentley              9                     3   \n",
      "1    A.J. Macaulay     Bemidji State              2                     0   \n",
      "2  Aaron Bohlinger        Quinnipiac             11                     2   \n",
      "3     Aaron Huglen         Minnesota             14                     4   \n",
      "4      Aaron Pionk  Minnesota Duluth             23                     3   \n",
      "\n",
      "   Assists_Leading by 2+  Assists_Tied  Assists_Trailing by 1  \\\n",
      "0                      1             1                      3   \n",
      "1                      0             0                      2   \n",
      "2                      3             5                      1   \n",
      "3                      2             4                      3   \n",
      "4                      2             8                      4   \n",
      "\n",
      "   Assists_Trailing by 2+  Assists_1st Period  Assists_2nd Period  \\\n",
      "0                       1                   0                   5   \n",
      "1                       0                   0                   2   \n",
      "2                       0                   6                   1   \n",
      "3                       1                   6                   3   \n",
      "4                       6                   9                   5   \n",
      "\n",
      "   Assists_3rd Period  Assists_Overtime  First_Assists  Second_Assists  \n",
      "0                   4                 0              4               5  \n",
      "1                   0                 0              2               0  \n",
      "2                   2                 2              5               6  \n",
      "3                   5                 0              2              12  \n",
      "4                   9                 0             11              12  \n"
     ]
    }
   ],
   "source": [
    "# Recalculate the total assists directly from the raw scoring summary\n",
    "\n",
    "# Initialize a list to store assist records\n",
    "assist_records = []\n",
    "\n",
    "# Process each goal event for assists\n",
    "for _, row in score_summary_df.iterrows():\n",
    "    game_id = row[\"Game_ID\"]\n",
    "    period = row[\"Period\"]\n",
    "    time = row[\"Time\"]\n",
    "    game_situation = row[\"Game_Situation\"]\n",
    "    team = row[\"Team\"]\n",
    "    \n",
    "    # Get assists from original scoring data\n",
    "    original_row = scoring_df[\n",
    "        (scoring_df[\"Game_ID\"] == game_id) & \n",
    "        (scoring_df[\"Team\"] == team) & \n",
    "        (scoring_df[\"Period\"] == period) & \n",
    "        (scoring_df[\"Time\"] == time)\n",
    "    ].iloc[0]\n",
    "\n",
    "    assist1 = original_row[\"Assist1\"] if pd.notna(original_row[\"Assist1\"]) else None\n",
    "    assist2 = original_row[\"Assist2\"] if pd.notna(original_row[\"Assist2\"]) else None\n",
    "\n",
    "    # Record first assist if exists\n",
    "    if assist1:\n",
    "        assist_records.append({\n",
    "            \"Player\": assist1,\n",
    "            \"Team\": team,\n",
    "            \"Assist_Type\": \"First_Assist\",\n",
    "            \"Game_Situation\": game_situation,\n",
    "            \"Period\": period\n",
    "        })\n",
    "\n",
    "    # Record second assist if exists\n",
    "    if assist2:\n",
    "        assist_records.append({\n",
    "            \"Player\": assist2,\n",
    "            \"Team\": team,\n",
    "            \"Assist_Type\": \"Second_Assist\",\n",
    "            \"Game_Situation\": game_situation,\n",
    "            \"Period\": period\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "assist_df = pd.DataFrame(assist_records)\n",
    "\n",
    "# Directly count total assists from the raw data\n",
    "total_assists_df = assist_df.groupby([\"Player\", \"Team\"]).size().reset_index(name=\"Total_Assists\")\n",
    "\n",
    "# Tally assists by game situation\n",
    "assist_by_situation = assist_df.groupby([\"Player\", \"Team\", \"Game_Situation\"]).size().unstack(fill_value=0).reset_index()\n",
    "assist_by_situation.columns = [\"Player\", \"Team\"] + [f\"Assists_{col}\" for col in assist_by_situation.columns[2:]]\n",
    "\n",
    "# Tally assists by period\n",
    "assist_by_period = assist_df.groupby([\"Player\", \"Team\", \"Period\"]).size().unstack(fill_value=0).reset_index()\n",
    "assist_by_period.columns = [\"Player\", \"Team\"] + [f\"Assists_{col}\" for col in assist_by_period.columns[2:]]\n",
    "\n",
    "# Tally assists by type (first vs. second assist)\n",
    "assist_by_type = assist_df.groupby([\"Player\", \"Team\", \"Assist_Type\"]).size().unstack(fill_value=0).reset_index()\n",
    "assist_by_type.columns = [\"Player\", \"Team\", \"First_Assists\", \"Second_Assists\"]\n",
    "\n",
    "# Merge all assist tallies into a single DataFrame\n",
    "assist_tally_df = total_assists_df.merge(assist_by_situation, on=[\"Player\", \"Team\"], how=\"outer\")\n",
    "assist_tally_df = assist_tally_df.merge(assist_by_period, on=[\"Player\", \"Team\"], how=\"outer\")\n",
    "assist_tally_df = assist_tally_df.merge(assist_by_type, on=[\"Player\", \"Team\"], how=\"outer\")\n",
    "\n",
    "# Fill NaN values with 0 (for players who didn't have assists in certain situations)\n",
    "assist_tally_df = assist_tally_df.fillna(0)\n",
    "\n",
    "# Display the processed DataFrame\n",
    "print(assist_tally_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Player              Team  Total_Goals  1st_goals  2nd_goals  \\\n",
      "0      A.J. Hodges           Bentley            8          2          1   \n",
      "1    A.J. Macaulay     Bemidji State            0          0          0   \n",
      "2  Aaron Bohlinger        Quinnipiac            3          2          0   \n",
      "3     Aaron Huglen         Minnesota            5          1          3   \n",
      "4      Aaron Pionk  Minnesota Duluth            3          1          1   \n",
      "\n",
      "   3rd_goals  ot_goals  home_goals  away_goals  5on5_goals  ...  \\\n",
      "0          4         1           3           5           3  ...   \n",
      "1          0         0           0           0           0  ...   \n",
      "2          1         0           0           3           0  ...   \n",
      "3          1         0           2           3           4  ...   \n",
      "4          0         1           3           0           1  ...   \n",
      "\n",
      "   Assists_Leading by 2+  Assists_Tied  Assists_Trailing by 1  \\\n",
      "0                      1             1                      3   \n",
      "1                      0             0                      2   \n",
      "2                      3             5                      1   \n",
      "3                      2             4                      3   \n",
      "4                      2             8                      4   \n",
      "\n",
      "   Assists_Trailing by 2+  Assists_1st Period  Assists_2nd Period  \\\n",
      "0                       1                   0                   5   \n",
      "1                       0                   0                   2   \n",
      "2                       0                   6                   1   \n",
      "3                       1                   6                   3   \n",
      "4                       6                   9                   5   \n",
      "\n",
      "   Assists_3rd Period  Assists_Overtime  First_Assists  Second_Assists  \n",
      "0                   4                 0              4               5  \n",
      "1                   0                 0              2               0  \n",
      "2                   2                 2              5               6  \n",
      "3                   5                 0              2              12  \n",
      "4                   9                 0             11              12  \n",
      "\n",
      "[5 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "### Merge all the dataframes together\n",
    "\n",
    "# Merge goal tally and assist tally DataFrames\n",
    "player_stats_df = goal_tally_df.merge(assist_tally_df, on=[\"Player\", \"Team\"], how=\"outer\")\n",
    "\n",
    "# Fill NaN values with 0 (for players who didn't have goals or assists)\n",
    "player_stats_df = player_stats_df.fillna(0)\n",
    "\n",
    "# Convert all columns except Player and Team to integer type\n",
    "numeric_cols = player_stats_df.columns[2:]\n",
    "player_stats_df[numeric_cols] = player_stats_df[numeric_cols].astype(int)\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(player_stats_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1473 entries, 0 to 1472\n",
      "Data columns (total 32 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Player                  1473 non-null   object \n",
      " 1   Team                    1473 non-null   object \n",
      " 2   Total_Goals             1473 non-null   float64\n",
      " 3   1st_goals               1473 non-null   float64\n",
      " 4   2nd_goals               1473 non-null   float64\n",
      " 5   3rd_goals               1473 non-null   float64\n",
      " 6   ot_goals                1473 non-null   float64\n",
      " 7   home_goals              1473 non-null   float64\n",
      " 8   away_goals              1473 non-null   float64\n",
      " 9   5on5_goals              1473 non-null   float64\n",
      " 10  4on4_goals              1473 non-null   float64\n",
      " 11  3on3_goals              1473 non-null   float64\n",
      " 12  powerplay_goals         1473 non-null   float64\n",
      " 13  shorthanded_goals       1473 non-null   float64\n",
      " 14  empty_net_goals         1473 non-null   float64\n",
      " 15  extra_attacker_goals    1473 non-null   float64\n",
      " 16  penalty_shots_goals     1473 non-null   float64\n",
      " 17  unassisted              1473 non-null   float64\n",
      " 18  one_assist              1473 non-null   float64\n",
      " 19  two_assist              1473 non-null   float64\n",
      " 20  Total_Assists           1473 non-null   float64\n",
      " 21  Assists_Leading by 1    1473 non-null   float64\n",
      " 22  Assists_Leading by 2+   1473 non-null   float64\n",
      " 23  Assists_Tied            1473 non-null   float64\n",
      " 24  Assists_Trailing by 1   1473 non-null   float64\n",
      " 25  Assists_Trailing by 2+  1473 non-null   float64\n",
      " 26  Assists_1st Period      1473 non-null   float64\n",
      " 27  Assists_2nd Period      1473 non-null   float64\n",
      " 28  Assists_3rd Period      1473 non-null   float64\n",
      " 29  Assists_Overtime        1473 non-null   float64\n",
      " 30  First_Assists           1473 non-null   float64\n",
      " 31  Second_Assists          1473 non-null   float64\n",
      "dtypes: float64(30), object(2)\n",
      "memory usage: 368.4+ KB\n"
     ]
    }
   ],
   "source": [
    "player_stats_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score_summary_df = score_summary_new_df\n",
    "\n",
    "# # Initialize a list to store assist records\n",
    "# assist_records = []\n",
    "\n",
    "# # Process each goal event for assists\n",
    "# for _, row in score_summary_df.iterrows():\n",
    "#     game_id = row[\"Game_ID\"]\n",
    "#     period = row[\"Period\"]\n",
    "#     time = row[\"Time\"]\n",
    "#     game_situation = row[\"Game_Situation\"]\n",
    "#     team = row[\"Team\"]\n",
    "    \n",
    "#     # Get assists from original scoring data\n",
    "#     original_row = scoring_df[\n",
    "#         (scoring_df[\"Game_ID\"] == game_id) & \n",
    "#         (scoring_df[\"Team\"] == team) & \n",
    "#         (scoring_df[\"Period\"] == period) & \n",
    "#         (scoring_df[\"Time\"] == time)\n",
    "#     ].iloc[0]\n",
    "\n",
    "#     assist1 = original_row[\"Assist1\"] if pd.notna(original_row[\"Assist1\"]) else None\n",
    "#     assist2 = original_row[\"Assist2\"] if pd.notna(original_row[\"Assist2\"]) else None\n",
    "\n",
    "#     # Record first assist if exists\n",
    "#     if assist1:\n",
    "#         assist_records.append({\n",
    "#             \"Player\": assist1,\n",
    "#             \"Team\": team,\n",
    "#             \"Assist_Type\": \"First_Assist\",\n",
    "#             \"Game_Situation\": game_situation,\n",
    "#             \"Period\": period\n",
    "#         })\n",
    "\n",
    "#     # Record second assist if exists\n",
    "#     if assist2:\n",
    "#         assist_records.append({\n",
    "#             \"Player\": assist2,\n",
    "#             \"Team\": team,\n",
    "#             \"Assist_Type\": \"Second_Assist\",\n",
    "#             \"Game_Situation\": game_situation,\n",
    "#             \"Period\": period\n",
    "#         })\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# assist_df = pd.DataFrame(assist_records)\n",
    "\n",
    "# # Tally assists by game situation\n",
    "# assist_by_situation = assist_df.groupby([\"Player\", \"Team\", \"Game_Situation\"]).size().unstack(fill_value=0).reset_index()\n",
    "# assist_by_situation.columns = [\"Player\", \"Team\"] + [f\"Assists_{col}\" for col in assist_by_situation.columns[2:]]\n",
    "\n",
    "# # Tally assists by period\n",
    "# assist_by_period = assist_df.groupby([\"Player\", \"Team\", \"Period\"]).size().unstack(fill_value=0).reset_index()\n",
    "# assist_by_period.columns = [\"Player\", \"Team\"] + [f\"Assists_{col}\" for col in assist_by_period.columns[2:]]\n",
    "\n",
    "# # Tally assists by type (first vs. second assist)\n",
    "# assist_by_type = assist_df.groupby([\"Player\", \"Team\", \"Assist_Type\"]).size().unstack(fill_value=0).reset_index()\n",
    "# assist_by_type.columns = [\"Player\", \"Team\", \"First_Assists\", \"Second_Assists\"]\n",
    "\n",
    "# # Merge all assist tallies into a single DataFrame\n",
    "# assist_tally_df = assist_by_situation.merge(assist_by_period, on=[\"Player\", \"Team\"], how=\"outer\")\n",
    "# assist_tally_df = assist_tally_df.merge(assist_by_type, on=[\"Player\", \"Team\"], how=\"outer\")\n",
    "\n",
    "# # Fill NaN values with 0 (for players who didn't have assists in certain situations)\n",
    "# assist_tally_df = assist_tally_df.fillna(0)\n",
    "\n",
    "# # Add total assists column\n",
    "# assist_tally_df[\"Total_Assists\"] = assist_tally_df.iloc[:, 2:].sum(axis=1)\n",
    "\n",
    "# # Display the processed DataFrame\n",
    "# print(assist_tally_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score_summary_new_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscore_summary_new_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'score_summary_new_df' is not defined"
     ]
    }
   ],
   "source": [
    "score_summary_new_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dictionary of abrvs - in PP column\n",
    "# SH - shorthanded\n",
    "# PP - power play\n",
    "# 3x5 - 3 on 5 (shorthanded)\n",
    "# 4x4 - 4 on 4\n",
    "# 3x3 - 3 on 3\n",
    "# 5x3 - 5 on 3\n",
    "# EA - extra attacker\n",
    "# EN - empty net\n",
    "# PS - penalty shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import sqlite3\n",
    "\n",
    "# # Connect to the SQLite database\n",
    "# filename = 'nhl.db'\n",
    "# # Clean up the name format in player_stats for easier matching\n",
    "# # Replace the non-breaking space with a regular space\n",
    "# df_player_stats['Clean_Player'] = df_player_stats['Player'].apply(lambda x: x.replace('\\xa0', ' '))\n",
    "\n",
    "# # Remove rows where Player is the team name (e.g., \"Michigan State\")\n",
    "# df_player_stats_cleaned = df_player_stats[df_player_stats['Player'] != df_player_stats['Team']]\n",
    "\n",
    "# # Convert relevant columns to integers for correct aggregation\n",
    "# cols_to_convert = ['G', 'A', 'Pts', 'plus_minus', 'Sh', 'PIM', 'FOW', 'FOL']\n",
    "# for col in cols_to_convert:\n",
    "#     df_player_stats_cleaned[col] = pd.to_numeric(df_player_stats_cleaned[col], errors='coerce')\n",
    "# # Make sure TOI recognized as time in MM:SS format\n",
    "# df_player_stats_cleaned['TOI'] = pd.to_datetime(df_player_stats_cleaned['TOI'], format='%M:%S', errors='coerce').dt.time\n",
    "# # Convert to seconds for easier aggregation\n",
    "# df_player_stats_cleaned['TOI_sec'] = df_player_stats_cleaned['TOI'].apply(lambda x: x.minute * 60 + x.second)\n",
    "\n",
    "\n",
    "\n",
    "# # Aggregate the data for year-to-date stats\n",
    "# # Add a column for counting the number of games each player has played\n",
    "# agg_player_stats_corrected_with_games = df_player_stats_cleaned.groupby(['Clean_Player', 'Team']).agg({\n",
    "#     'G': 'sum',\n",
    "#     'A': 'sum',\n",
    "#     'Pts': 'sum',\n",
    "#     'plus_minus': 'sum',\n",
    "#     'Sh': 'sum',\n",
    "#     'TOI_sec': 'sum',\n",
    "#     'PIM': 'sum',\n",
    "#     'FOW': 'sum',\n",
    "#     'FOL': 'sum',\n",
    "#     'Game_ID': 'count'  # Counting the number of unique Game_IDs for each player\n",
    "# }).reset_index()\n",
    "\n",
    "# # Rename the Game_ID column to Games_Played\n",
    "# agg_player_stats_corrected_with_games.rename(columns={'Game_ID': 'Games_Played'}, inplace=True)\n",
    "# #  Calculate Face off Percentage\n",
    "# agg_player_stats_corrected_with_games['FO%'] = (agg_player_stats_corrected_with_games['FOW'] / (agg_player_stats_corrected_with_games['FOW'] + agg_player_stats_corrected_with_games['FOL'])) * 100\n",
    "\n",
    "# # Conver TOI_sec back to HH:MM:SS format\n",
    "# agg_player_stats_corrected_with_games['TOI'] = pd.to_datetime(agg_player_stats_corrected_with_games['TOI_sec'], unit='s').dt.strftime('%H:%M:%S')\n",
    "\n",
    "\n",
    "# # Save the updated aggregated data back to the database, replacing the existing table\n",
    "# agg_player_stats_corrected_with_games.to_sql('player_stats_ytd', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def fetch_play_by_play_table(url):\n",
    "    \"\"\"\n",
    "    Scrape the play-by-play table from the given NCAA game URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the NCAA game play-by-play page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the play-by-play data.\n",
    "    \"\"\"\n",
    "    # Set up Selenium WebDriver (update path to your ChromeDriver if needed)\n",
    "    # service = Service(executable_path='chromedriver')  # Adjust 'chromedriver' path\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Load the page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load (adjust timeout if necessary)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'play-by-play-period'))\n",
    "        )\n",
    "\n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Locate all periods\n",
    "        period_divs = soup.find_all('div', {'class': 'play-by-play-period'})\n",
    "\n",
    "        # Initialize an empty list to store all data\n",
    "        all_data = []\n",
    "\n",
    "        # Iterate through each period\n",
    "        for period_div in period_divs:\n",
    "            # Extract period header (e.g., \"Start of 1st\")\n",
    "            period_header = period_div.find('h3').get_text(strip=True)\n",
    "\n",
    "            # Find the table inside the period div\n",
    "            pbp_table = period_div.find('div', {'class': 'play-by-play-period-table'}).find('table')\n",
    "\n",
    "            # Extract rows from the table\n",
    "            for row in pbp_table.find('tbody').find_all('tr'):\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 4:  # Ensure the row has all expected columns\n",
    "                    time = cells[0].get_text(strip=True)\n",
    "                    team = cells[1].get_text(strip=True)\n",
    "                    description = cells[2].get_text(strip=True)\n",
    "                    score = cells[3].get_text(strip=True)\n",
    "                    all_data.append([period_header, time, team, description, score])\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(all_data, columns=['Period', 'Time', 'Team', 'Description', 'Score'])\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n",
    "    finally:\n",
    "        # Close the Selenium WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.ncaa.com/game/6344953/play-by-play\"\n",
    "    play_by_play_df = fetch_play_by_play_table(url)\n",
    "    if not play_by_play_df.empty:\n",
    "        print(play_by_play_df.head())\n",
    "    else:\n",
    "        print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play_by_play_df.head(10)\n",
    "# play_by_play_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning of the Raw PbP table\n",
    "-replace values in Period column to simple identifiers (1,2,3,OT)\n",
    "- Convert Time column into Continuous_Time like in other workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename to df for simplicity\n",
    "df = play_by_play_df\n",
    "\n",
    "#TO DO: replace values in Period column to simple identifiers (1,2,3,OT)\n",
    "df['Period'] = df['Period'].replace({\n",
    "    'Start of 1st': '1',\n",
    "    'Start of 2nd': '2',\n",
    "    'Start of 3rd': '3',\n",
    "    'Start of OT': 'OT'\n",
    "})\n",
    "\n",
    "# To DO : Replace the values in the Time column - convert to seconds since the beginning of the game\n",
    "# If period is 1, 20:00 is now 0 and 19:59 is 1, so on and so forth\n",
    "# If period is 2, 20:00 is now 1200 and 19:59 is 1201, so on and so forth\n",
    "\n",
    "# Convert string time to continuous time\n",
    "def convert_to_continuous_time(row):\n",
    "    \"\"\"\n",
    "    Converts period-based time to a continuous format (0-65 minutes).\n",
    "    \"\"\"\n",
    "    period_offsets = {'1': 0, '2': 20, '3': 40, 'OT': 60}\n",
    "    minutes, seconds = map(int, row['Time'].split(':'))\n",
    "    # Subtract the minutes : seconds from 20:00 to get time elapsed\n",
    "    elapsed_time = (20 - minutes) * 60 + - seconds\n",
    "    offset = period_offsets.get(row['Period'], 0) * 60\n",
    "    return offset + elapsed_time\n",
    "\n",
    "df['Time'] = df.apply(convert_to_continuous_time, axis=1)\n",
    "\n",
    "\n",
    "# def convert_time_to_seconds(time_str, period):\n",
    "#     if period == '1' or period == '2':\n",
    "#         minutes, seconds = map(int, time_str.split(':'))\n",
    "#         total_seconds = (20 - minutes) * 60 + seconds\n",
    "#     elif period == '3':\n",
    "#         minutes, seconds = map(int, time_str.split(':'))\n",
    "#         total_seconds = (20 - minutes) * 60 + seconds\n",
    "#     elif period == 'OT':\n",
    "#         minutes, seconds = map(int, time_str.split(':'))\n",
    "#         total_seconds = (5 - minutes) * 60 + seconds\n",
    "#     else:\n",
    "#         total_seconds = None  # Handle unexpected periods\n",
    "#     return total_seconds\n",
    "# df['Time'] = df.apply(lambda x: convert_time_to_seconds(x['Time'], x['Period']), axis=1)\n",
    "\n",
    "df.tail()\n",
    "\n",
    "# def convert_time_to_seconds(time_str, period):\n",
    "#     if period == '1' or period == '2':\n",
    "#         minutes, seconds = map(int, time_str.split(':'))\n",
    "#         total_seconds = (20 - minutes) * 60 + seconds\n",
    "#     elif period == '3':\n",
    "#         minutes, seconds = map(int, time_str.split(':'))\n",
    "#         total_seconds = (20 - minutes) * 60 + seconds\n",
    "#     elif period == 'OT':\n",
    "#         minutes, seconds = map(int, time_str.split(':'))\n",
    "#         total_seconds = (5 - minutes) * 60 + seconds\n",
    "#     else:\n",
    "#         total_seconds = None  # Handle unexpected periods\n",
    "#     return total_seconds\n",
    "# df['Time'] = df.apply(lambda x: convert_time_to_seconds(x['Time'], x['Period']), axis=1)\n",
    "\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse the Description Column into structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "# import unicodedata\n",
    "\n",
    "# # Function to convert period and time to continuous time\n",
    "# def convert_to_continuous_time(period, time):\n",
    "#     period_offsets = {'1': 0, '2': 20, '3': 40, 'OT': 60}\n",
    "#     minutes, seconds = map(int, time.split(':'))\n",
    "#     elapsed_time = (20 - minutes) * 60 + -seconds\n",
    "#     offset = period_offsets.get(period, 0) * 60\n",
    "#     return offset + elapsed_time\n",
    "\n",
    "# # Function to normalize names to handle accents and special characters\n",
    "# def normalize_name(name):\n",
    "#     if not name:\n",
    "#         return None\n",
    "#     # Normalize Unicode accents and remove non-ASCII characters\n",
    "#     normalized = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "#     return normalized\n",
    "\n",
    "# # Enhanced player name formatting function\n",
    "# def clean_player_name(name):\n",
    "#     \"\"\"\n",
    "#     Converts a name from \"Last, First\" to \"First Last\", handling punctuation and normalization.\n",
    "#     Example: \"Hughes, T.J.\" -> \"T.J. Hughes\"\n",
    "#     \"\"\"\n",
    "#     if not name:\n",
    "#         return None\n",
    "#     name = normalize_name(name)\n",
    "#     parts = [p.strip() for p in name.split(',')]\n",
    "#     if len(parts) == 2:\n",
    "#         last, first = parts\n",
    "#         return f\"{first} {last}\".strip()\n",
    "#     return name\n",
    "\n",
    "# # Function to parse play-by-play descriptions\n",
    "# # Improved to handle team abbreviations and player names with issues\n",
    "# def parse_description(description):\n",
    "#     \"\"\"\n",
    "#     Parse a single play-by-play description into structured fields.\n",
    "#     \"\"\"\n",
    "#     desc_lower = description.lower().strip()\n",
    "#     parsed = {\n",
    "#         \"Event_type\": \"Other\",\n",
    "#         \"Primary_player\": None,\n",
    "#         \"Primary_team\": None,\n",
    "#         \"Secondary_player\": None,\n",
    "#         \"Secondary_team\": None,\n",
    "#         \"Outcome\": None,\n",
    "#     }\n",
    "\n",
    "#     # Normalize known team abbreviations\n",
    "#     team_map = {\n",
    "#         'michst': 'MICHST',\n",
    "#         'lake sup': 'LK SUP',\n",
    "#         'lk sup': 'LK SUP',\n",
    "#         'michigan state': 'MICHST',\n",
    "#         'lake superior': 'LK SUP'\n",
    "#     }\n",
    "\n",
    "#     for key, value in team_map.items():\n",
    "#         desc_lower = desc_lower.replace(key, value.lower())\n",
    "\n",
    "#     # --- Faceoff ---\n",
    "#     if \"faceoff\" in desc_lower:\n",
    "#         parsed[\"Event_type\"] = \"Faceoff\"\n",
    "#         faceoff_pattern = (\n",
    "#             r\"Faceoff\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+\"\n",
    "#             r\"vs\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+\"\n",
    "#             r\"won by\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\.\"\n",
    "#         )\n",
    "#         match = re.search(faceoff_pattern, description, re.IGNORECASE)\n",
    "#         if match:\n",
    "#             parsed[\"Primary_player\"] = clean_player_name(match.group(1))\n",
    "#             parsed[\"Secondary_player\"] = clean_player_name(match.group(2))\n",
    "#             parsed[\"Primary_team\"] = match.group(3).strip()\n",
    "#             parsed[\"Outcome\"] = \"won\"\n",
    "#         return parsed\n",
    "\n",
    "#     # --- Goal ---\n",
    "#     if \"goal by\" in desc_lower:\n",
    "#         parsed[\"Event_type\"] = \"Goal\"\n",
    "#         goal_scorer_pattern = r\"Goal by\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\"\n",
    "#         match = re.search(goal_scorer_pattern, description, re.IGNORECASE)\n",
    "#         if match:\n",
    "#             parsed[\"Primary_player\"] = clean_player_name(match.group(1))\n",
    "#         return parsed\n",
    "\n",
    "#     # --- Penalty ---\n",
    "#     if desc_lower.startswith(\"penalty on\"):\n",
    "#         parsed[\"Event_type\"] = \"Penalty\"\n",
    "#         penalty_pattern = (\n",
    "#             r\"Penalty on\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+, [A-Za-zÀ-ÖØ-ÿ'\\.\\- ]+)\\s+\"\n",
    "#             r\"([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+(\\d+) minutes for (.+)\"\n",
    "#         )\n",
    "#         match = re.search(penalty_pattern, description, re.IGNORECASE)\n",
    "#         if match:\n",
    "#             parsed[\"Primary_player\"] = clean_player_name(match.group(1))\n",
    "#             parsed[\"Primary_team\"] = match.group(2).strip()\n",
    "#             parsed[\"Penalty_duration\"] = match.group(3).strip()\n",
    "#             parsed[\"Penalty_type\"] = match.group(4).strip()\n",
    "#         return parsed\n",
    "\n",
    "#     # --- Shot ---\n",
    "#     if \"shot by\" in desc_lower:\n",
    "#         parsed[\"Event_type\"] = \"Shot\"\n",
    "#         shot_pattern = r\"Shot by\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+(.+)\"\n",
    "#         match = re.search(shot_pattern, description, re.IGNORECASE)\n",
    "#         if match:\n",
    "#             parsed[\"Primary_player\"] = clean_player_name(match.group(1))\n",
    "#             parsed[\"Primary_team\"] = match.group(2).strip()\n",
    "#         return parsed\n",
    "\n",
    "#     return parsed\n",
    "\n",
    "# # Function to transform a single game's JSON data into a dataframe\n",
    "# def transform_single_game(json_data, game_id):\n",
    "#     rows = []\n",
    "\n",
    "#     for period in json_data['periods']:\n",
    "#         period_number = period['periodNumber']\n",
    "#         for play in period['playStats']:\n",
    "#             row = {\n",
    "#                 'Game_ID': game_id,\n",
    "#                 'Period': period_number,\n",
    "#                 'Time': play['time'],\n",
    "#                 'Description': play['visitorText'] or play['homeText'],\n",
    "#                 'Score': play['score']\n",
    "#             }\n",
    "#             rows.append(row)\n",
    "\n",
    "#     game_df = pd.DataFrame(rows)\n",
    "\n",
    "#     # Convert period and time to continuous time\n",
    "#     game_df['Period'] = game_df['Period'].replace({'1st': '1', '2nd': '2', '3rd': '3', 'OT': 'OT'})\n",
    "#     game_df['Time'] = game_df.apply(lambda row: convert_to_continuous_time(row['Period'], row['Time']), axis=1)\n",
    "\n",
    "#     # Parse descriptions\n",
    "#     parsed_descriptions = game_df['Description'].apply(parse_description)\n",
    "#     parsed_df = pd.DataFrame(parsed_descriptions.tolist())\n",
    "\n",
    "#     # Combine with original game_df\n",
    "#     return pd.concat([game_df, parsed_df], axis=1)\n",
    "\n",
    "# # Function to process all games and combine into a single dataframe\n",
    "# def process_all_games(schedule_df):\n",
    "#     all_games = []\n",
    "\n",
    "#     for _, row in schedule_df.iterrows():\n",
    "#         game_id = row['Game_ID']\n",
    "#         json_data = row['Play_By_Play_JSON']\n",
    "\n",
    "#         if json_data:\n",
    "#             game_df = transform_single_game(json_data, game_id)\n",
    "#             all_games.append(game_df)\n",
    "\n",
    "#     return pd.concat(all_games, ignore_index=True)\n",
    "\n",
    "# # Example usage\n",
    "# # Assuming `updated_schedule_df` is the dataframe containing the JSON play-by-play data\n",
    "# final_pbp_df = process_all_games(updated_schedule_df)\n",
    "\n",
    "# # Display the resulting dataframe\n",
    "# final_pbp_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## V 2.2\n",
    "\n",
    "# import re\n",
    "# import pandas as pd\n",
    "\n",
    "# def parse_description(description):\n",
    "#     \"\"\"\n",
    "#     Parse a single play-by-play description into structured fields.\n",
    "\n",
    "#     Args:\n",
    "#         description (str): The play-by-play description.\n",
    "\n",
    "#     Returns:\n",
    "#         dict: Parsed fields capturing the important pieces of data.\n",
    "#     \"\"\"\n",
    "#     # Normalize for easier detection\n",
    "#     desc_lower = description.lower().strip()\n",
    "\n",
    "#     # Initialize the parse dictionary with a default fallback\n",
    "#     parsed = {\n",
    "#         \"Event_type\": \"Other\",\n",
    "#         \"Primary_player\": None,\n",
    "#         \"Primary_team\": None,\n",
    "#         \"Secondary_player\": None,\n",
    "#         \"Secondary_team\": None,\n",
    "#         \"Outcome\": None,            # e.g. \"won\", \"blocked\", \"missed\", \"wide\", \"saved\", etc.\n",
    "#         \"Assist_players\": None,     # for goals with multiple assists\n",
    "#         \"Penalty_type\": None,       # e.g., \"Slashing,\" \"Roughing\"\n",
    "#         \"Penalty_duration\": None,   # e.g., 2,4,5,10\n",
    "#         \"Goaltender_in\": None,      # e.g. \"Logan Stein\"\n",
    "#         \"Goaltender_team\": None,\n",
    "#     }\n",
    "\n",
    "#     # --- 1) Faceoff ---\n",
    "#     if \"faceoff\" in desc_lower:\n",
    "#         parsed[\"Event_type\"] = \"Faceoff\"\n",
    "#         # Example: \"Faceoff Shoudy, Tiernan vs Draper, Kienan won by MICHST.\"\n",
    "#         faceoff_pattern = (\n",
    "#             r\"Faceoff\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+)\"  # group(1): winner\n",
    "#             r\"\\s+vs\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+)\"    # group(2): loser\n",
    "#             r\"\\s+won by\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+)\\.?\"                       # group(3): winning team\n",
    "#         )\n",
    "#         match = re.search(faceoff_pattern, description)\n",
    "#         if match:\n",
    "#             parsed[\"Primary_player\"] = match.group(1).strip()\n",
    "#             parsed[\"Secondary_player\"] = match.group(2).strip()\n",
    "#             parsed[\"Primary_team\"] = match.group(3).strip()\n",
    "#             parsed[\"Outcome\"] = \"won\"\n",
    "#         return parsed\n",
    "\n",
    "#     # --- 2) Goal ---\n",
    "#     #    We only interpret lines containing \"Goal by\" (to avoid confusion with \"goalie\")\n",
    "#     if re.search(r\"\\bgoal by\\b\", desc_lower):\n",
    "#         parsed[\"Event_type\"] = \"Goal\"\n",
    "#         # Find \"Goal by <player>\"\n",
    "#         goal_scorer_pattern = (\n",
    "#             r\"Goal by\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+)\"\n",
    "#         )\n",
    "#         gm = re.search(goal_scorer_pattern, description)\n",
    "#         if gm:\n",
    "#             parsed[\"Primary_player\"] = gm.group(1).strip()\n",
    "\n",
    "#         # Extract the assist section if present: \"Assist by: <player1, player2...>\"\n",
    "#         assist_pattern = r\"Assist by:\\s*(.*?)(?=On ice for|$)\"\n",
    "#         am = re.search(assist_pattern, description)\n",
    "#         if am:\n",
    "#             assist_text = am.group(1).strip()\n",
    "#             assist_names = re.findall(\n",
    "#                 r\"[A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+\", assist_text\n",
    "#             )\n",
    "#             if assist_names:\n",
    "#                 parsed[\"Assist_players\"] = [an.strip() for an in assist_names]\n",
    "\n",
    "#         return parsed\n",
    "\n",
    "#     # --- 3) Penalty ---\n",
    "#     #    Enhanced penalty parsing to handle lines like:\n",
    "#     #      \"Penalty on Calafiore, Anthony MAINE 2 minutes for Slashing.\"\n",
    "#     #      \"Penalty on Mitton, Ross MAINE minutes for Duration:10\"\n",
    "#     #    etc.\n",
    "#     if desc_lower.startswith(\"penalty on\"):\n",
    "#         parsed[\"Event_type\"] = \"Penalty\"\n",
    "\n",
    "#         # Try to capture the normal pattern, e.g.\n",
    "#         # \"Penalty on Last, First TEAM 2 minutes for Slashing.\"\n",
    "#         # group(1) = \"Last, First\"\n",
    "#         # group(2) = \"TEAM\"\n",
    "#         # group(3) = numeric duration\n",
    "#         # group(4) = penalty type\n",
    "#         penalty_pattern = re.compile(\n",
    "#             r\"Penalty on\\s+\"\n",
    "#             r\"([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+,\\s*[A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+\"   # (1) \"Last, First\"\n",
    "#             r\"([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+\"                                # (2) \"TEAM\"\n",
    "#             r\"(\\d+)\\s+minutes?\\s+for\\s+(.+)\",                               # (3) \"2\", (4) \"Slashing.\"\n",
    "#             flags=re.IGNORECASE\n",
    "#         )\n",
    "#         pm = penalty_pattern.search(description)\n",
    "#         if pm:\n",
    "#             parsed[\"Primary_player\"] = pm.group(1).strip()\n",
    "#             parsed[\"Primary_team\"] = pm.group(2).strip()\n",
    "#             parsed[\"Penalty_duration\"] = pm.group(3).strip()\n",
    "#             # Clean up trailing punctuation from penalty type\n",
    "#             ptype = pm.group(4).rstrip(\" .\").strip()\n",
    "#             parsed[\"Penalty_type\"] = ptype\n",
    "#             return parsed\n",
    "\n",
    "#         # If the above pattern fails, check for a fallback pattern, e.g.\n",
    "#         # \"Penalty on Mitton, Ross MAINE minutes for Duration:10\" \n",
    "#         # (No explicit numeric before 'minutes', but the duration is embedded.)\n",
    "#         fallback_pattern = re.compile(\n",
    "#             r\"Penalty on\\s+\"\n",
    "#             r\"([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+,\\s*[A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+\"  # (1) \"Last, First\"\n",
    "#             r\"([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+\"                               # (2) \"TEAM\"\n",
    "#             r\"minutes?\\s+for\\s+(.*)\",                                      # (3) \"Duration:10\" or \"some penalty\"\n",
    "#             flags=re.IGNORECASE\n",
    "#         )\n",
    "#         pm2 = fallback_pattern.search(description)\n",
    "#         if pm2:\n",
    "#             parsed[\"Primary_player\"] = pm2.group(1).strip()\n",
    "#             parsed[\"Primary_team\"] = pm2.group(2).strip()\n",
    "#             remainder = pm2.group(3).strip()\n",
    "\n",
    "#             # Check if there's \"Duration:XX\"\n",
    "#             dur_match = re.search(r\"Duration:(\\d+)\", remainder, flags=re.IGNORECASE)\n",
    "#             if dur_match:\n",
    "#                 parsed[\"Penalty_duration\"] = dur_match.group(1).strip()\n",
    "#                 # The penalty type could be after that text,\n",
    "#                 # e.g. \"Duration:10 for Misconduct\"\n",
    "#                 # so let's isolate that if it exists:\n",
    "#                 split_text = remainder.split(dur_match.group(0), 1)\n",
    "#                 if len(split_text) > 1:\n",
    "#                     # anything after \"Duration:XX\" is presumably penalty type\n",
    "#                     ptype = split_text[1].strip(\" .-:\")\n",
    "#                     parsed[\"Penalty_type\"] = ptype\n",
    "#             else:\n",
    "#                 # No numeric duration found, so just treat all of remainder as penalty_type\n",
    "#                 parsed[\"Penalty_type\"] = remainder\n",
    "#             return parsed\n",
    "\n",
    "#         # If we still haven't matched, we leave the penalty info mostly blank\n",
    "#         # but keep Event_type as \"Penalty\"\n",
    "#         return parsed\n",
    "\n",
    "#     # --- 4) Goaltender Change ---\n",
    "#     if \"at goalie for\" in desc_lower:\n",
    "#         parsed[\"Event_type\"] = \"Goaltender Change\"\n",
    "#         goalie_pattern = r\"(.+?)\\s+at goalie for\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+)\\.?\"\n",
    "#         gm = re.search(goalie_pattern, description, flags=re.IGNORECASE)\n",
    "#         if gm:\n",
    "#             parsed[\"Goaltender_in\"] = gm.group(1).strip()\n",
    "#             parsed[\"Goaltender_team\"] = gm.group(2).strip()\n",
    "#         return parsed\n",
    "\n",
    "#     # --- 5) Shot ---\n",
    "#     if desc_lower.startswith(\"shot by\"):\n",
    "#         parsed[\"Event_type\"] = \"Shot\"\n",
    "#         shot_team_pattern = r\"^Shot by\\s+([A-Za-z0-9.\\-]+)\\s+(.*)$\"\n",
    "#         m1 = re.search(shot_team_pattern, description)\n",
    "#         if m1:\n",
    "#             possible_team = m1.group(1).strip()\n",
    "#             remainder = m1.group(2).strip()\n",
    "#             parsed[\"Primary_team\"] = possible_team\n",
    "\n",
    "#             block_re = re.search(r\"\\bBLOCKED by\\b\\s+(.+)\", remainder, flags=re.IGNORECASE)\n",
    "#             miss_re  = re.search(r\"\\bMISSED\\b(.*)\", remainder, flags=re.IGNORECASE)\n",
    "#             wide_re  = re.search(r\"\\bWIDE\\b(.*)\", remainder, flags=re.IGNORECASE)\n",
    "#             save_re  = re.search(r\"\\bsave\\b\\s+(.+)\", remainder, flags=re.IGNORECASE)\n",
    "\n",
    "#             if block_re:\n",
    "#                 parsed[\"Outcome\"] = \"Blocked\"\n",
    "#                 parsed[\"Secondary_player\"] = block_re.group(1).strip()\n",
    "#                 shooter_part = remainder.split(\"BLOCKED by\", 1)[0].strip()\n",
    "#             elif miss_re:\n",
    "#                 parsed[\"Outcome\"] = \"Missed\"\n",
    "#                 shooter_part = remainder.split(\"MISSED\", 1)[0].rstrip(\", \").strip()\n",
    "#             elif wide_re:\n",
    "#                 parsed[\"Outcome\"] = \"Wide\"\n",
    "#                 shooter_part = remainder.split(\"WIDE\", 1)[0].strip()\n",
    "#             elif save_re:\n",
    "#                 parsed[\"Outcome\"] = \"Saved\"\n",
    "#                 parsed[\"Secondary_player\"] = save_re.group(1).strip()\n",
    "#                 shooter_part = remainder.split(\"save\", 1)[0].rstrip(\", \").strip()\n",
    "#             else:\n",
    "#                 # No recognized outcome, store entire remainder as shooter part\n",
    "#                 shooter_part = remainder\n",
    "\n",
    "#             # Try to pick out \"Last, First\" from shooter_part\n",
    "#             name_match = re.search(r\"([A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+)$\", shooter_part)\n",
    "#             if name_match:\n",
    "#                 parsed[\"Primary_player\"] = name_match.group(1).strip()\n",
    "#             else:\n",
    "#                 parsed[\"Primary_player\"] = shooter_part.strip()\n",
    "\n",
    "#         return parsed\n",
    "\n",
    "#     # --- 6) Start/End Power Play ---\n",
    "#     if \"start power play\" in desc_lower:\n",
    "#         parsed[\"Event_type\"] = \"Start Power Play\"\n",
    "#         sp = re.search(r\"Start power play for\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+)\", description, re.IGNORECASE)\n",
    "#         if sp:\n",
    "#             parsed[\"Primary_team\"] = sp.group(1).strip()\n",
    "#         return parsed\n",
    "\n",
    "#     if \"end power play\" in desc_lower:\n",
    "#         parsed[\"Event_type\"] = \"End Power Play\"\n",
    "#         ep = re.search(r\"End power play for\\s+([A-Za-zÀ-ÖØ-öø-ÿ\\'\\.\\- ]+)\", description, re.IGNORECASE)\n",
    "#         if ep:\n",
    "#             parsed[\"Primary_team\"] = ep.group(1).strip()\n",
    "#         return parsed\n",
    "\n",
    "#     # If none of the above matched, we leave it as \"Other\"\n",
    "#     return parsed\n",
    "\n",
    "\n",
    "# def transform_pbp_descriptions(descriptions):\n",
    "#     \"\"\"\n",
    "#     Transform a list of play-by-play descriptions into a structured DataFrame.\n",
    "\n",
    "#     Args:\n",
    "#         descriptions (list): List of play-by-play descriptions.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: Transformed DataFrame with parsed fields.\n",
    "#     \"\"\"\n",
    "#     parsed_data = [parse_description(desc) for desc in descriptions]\n",
    "#     return pd.DataFrame(parsed_data)\n",
    "\n",
    "\n",
    "# def clean_player_name(name):\n",
    "#     \"\"\"\n",
    "#     Converts a name from \"Last, First\" to \"First Last\", handling punctuation.\n",
    "#     Example: \"Hughes, T.J.\" -> \"T.J. Hughes\"\n",
    "#     \"\"\"\n",
    "#     if not name:\n",
    "#         return None\n",
    "#     parts = [p.strip() for p in name.split(',')]\n",
    "#     if len(parts) == 2:\n",
    "#         last, first = parts\n",
    "#         return f\"{first} {last}\".strip()\n",
    "#     return name\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage / integration:\n",
    "# if __name__ == \"__main__\":\n",
    "#     # # Suppose your df has a \"Description\" column\n",
    "#     # df = pd.DataFrame({\n",
    "#     #     \"Description\": [\n",
    "#     #         \"Faceoff Shoudy, Tiernan vs Draper, Kienan won by MICHST.\",\n",
    "#     #         \"Trey Augustine at goalie for Michigan St.\",\n",
    "#     #         \"Shot by MICHST Å trbãk, Maxim BLOCKED by Estapa, Mark.\",\n",
    "#     #         \"Penalty on Draper, Kienan MICH for Slashing.\",\n",
    "#     #         \"Goal by Dorwart, Karsen (EVEN STRENGTH) Assist by: Larson, Joey, On ice for MICH: ...\",\n",
    "#     #         \"Start power play for MICHST.\",\n",
    "#     #         \"End power play for MICHST.\"\n",
    "#     #     ]\n",
    "#     # })\n",
    "\n",
    "#     # Parse\n",
    "#     parsed_df = transform_pbp_descriptions(df[\"Description\"])\n",
    "\n",
    "#     # Combine with original\n",
    "#     df = pd.concat([df, parsed_df], axis=1)\n",
    "\n",
    "#     # Clean up name fields\n",
    "#     name_cols = [\"Primary_player\", \"Secondary_player\", \"Goaltender_in\"]\n",
    "#     for col in name_cols:\n",
    "#         df[col] = df[col].apply(clean_player_name)\n",
    "\n",
    "#     # If you want to clean up lists of assists:\n",
    "#     def clean_assist_list(assist_list):\n",
    "#         if not assist_list:\n",
    "#             return None\n",
    "#         return [clean_player_name(x) for x in assist_list]\n",
    "\n",
    "#     df[\"Assist_players\"] = df[\"Assist_players\"].apply(clean_assist_list)\n",
    "\n",
    "#     print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)\n",
    "\n",
    "## Check out Event_type Goal\n",
    "# df[df['Event_type'] == 'Penalty'].head(20)\n",
    "# # Check out Event_type Shot\n",
    "# df[df['Event_type'] == 'Shot'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.tail()\n",
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "# Step 1: Normalize data - Treat \"Goal\" events as successful \"Shot\" events\n",
    "data['Event_type'] = data['Event_type'].replace('Goal', 'Shot')\n",
    "\n",
    "# Step 2: Group data into time intervals (e.g., every 60 seconds = 1 minute)\n",
    "data['Time Interval (min)'] = (data['Time'] // 60)  # Group by minute\n",
    "\n",
    "# Step 3: Count shots for each team over time\n",
    "shot_counts = data[data['Event_type'] == 'Shot'].groupby(['Time Interval (min)', 'Primary_team']).size().reset_index(name='Shots')\n",
    "\n",
    "# Step 4: Cumulative shots over time\n",
    "shot_counts['Cumulative Shots'] = shot_counts.groupby('Primary_team')['Shots'].cumsum()\n",
    "\n",
    "# Step 5: Pivot data for visualization\n",
    "cumulative_shots_pivot = shot_counts.pivot(index='Time Interval (min)', columns='Primary_team', values='Cumulative Shots').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative shots for each team over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "for team in cumulative_shots_pivot.columns:\n",
    "    plt.plot(cumulative_shots_pivot.index, cumulative_shots_pivot[team], label=team, marker='o')\n",
    "\n",
    "plt.title('Cumulative Shots Over Time')\n",
    "plt.xlabel('Time Interval (Minutes)')\n",
    "plt.ylabel('Cumulative Shots')\n",
    "plt.legend(title='Team')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data for bar chart\n",
    "shot_frequency = data[data['Event_type'] == 'Shot'].groupby(['Time Interval (min)', 'Primary_team']).size().reset_index(name='Shots')\n",
    "\n",
    "# Bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Time Interval (min)', y='Shots', hue='Primary_team', data=shot_frequency)\n",
    "plt.title('Shot Frequency Per Time Interval')\n",
    "plt.xlabel('Time Interval (Minutes)')\n",
    "plt.ylabel('Number of Shots')\n",
    "plt.legend(title='Team')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a list of every value in the description column\n",
    "raw_list = df['Description'].unique()\n",
    "raw_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = 'https://www.ncaa.com/game/6344804/play-by-play'\n",
    "\n",
    "# # Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Make sure you have the ChromeDriver installed\n",
    "# # driver.get(url)\n",
    "\n",
    "def fetch_play_by_play_table(url):\n",
    "    \"\"\"\n",
    "    Scrape the play-by-play table from the given NCAA game URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the NCAA game play-by-play page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the play-by-play data.\n",
    "    \"\"\"\n",
    "    # Set up Selenium WebDriver (adjust the path to your ChromeDriver)\n",
    "    # service = Service(executable_path='chromedriver')  # Update 'chromedriver' path if necessary\n",
    "    # driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    try:\n",
    "        # Load the page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the table to load (adjust timeout if needed)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'gamecenter-tab-play-by-play'))\n",
    "        )\n",
    "\n",
    "        # Get page source and parse it with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the play-by-play table\n",
    "        pbp_table = soup.find('table', {'class': 'play-by-play-period-table'})\n",
    "\n",
    "        # Check if the table was found\n",
    "        if pbp_table is None:\n",
    "            raise ValueError(\"Play-by-play table not found on the page.\")\n",
    "\n",
    "        # Extract rows from the table\n",
    "        data = []\n",
    "        for row in pbp_table.find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) > 1:  # Skip header rows\n",
    "                time = cells[0].get_text(strip=True)\n",
    "                team = cells[1].get_text(strip=True)\n",
    "                description = cells[2].get_text(strip=True)\n",
    "                data.append([time, team, description])\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(data, columns=['Time', 'Team', 'Description'])\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n",
    "    finally:\n",
    "        # Close the Selenium WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.ncaa.com/game/6344804/play-by-play\"\n",
    "    play_by_play_df = fetch_play_by_play_table(url)\n",
    "    if not play_by_play_df.empty:\n",
    "        print(play_by_play_df.head())\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.ncaa.com/game/6344804/play-by-play'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    page_content = response.text\n",
    "else:\n",
    "    raise Exception(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "# print the soup to debug\n",
    "print(soup.prettify())\n",
    "\n",
    "pbp_table = soup.find('table', {'class': 'gamecenter-tab-play-by-play'})\n",
    "\n",
    "data = []\n",
    "for row in pbp_table.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:  # Ensure it's not a header row\n",
    "        time = cells[0].get_text(strip=True)\n",
    "        team = cells[1].get_text(strip=True)\n",
    "        description = cells[2].get_text(strip=True)\n",
    "        data.append([time, team, description])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Time', 'Team', 'Description'])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Make sure you have the ChromeDriver installed\n",
    "driver.get(url)\n",
    "\n",
    "# Let the page load completely\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "driver.quit()\n",
    "\n",
    "# Continue processing the soup object as before\n",
    "pbp_table = soup.find('table', {'class': 'gamecenter-tab-play-by-play'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pbp_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# async def get_url_response(url):\n",
    "#     \"\"\"\n",
    "#     Fetch and render the HTML content from the given URL.\n",
    "#     \"\"\"\n",
    "#     session = AsyncHTMLSession()\n",
    "#     response = await session.get(url)\n",
    "#     await response.html.arender()  # Asynchronously render JavaScript\n",
    "#     return response.html.html\n",
    "\n",
    "# def parse_html_to_table(html_content):\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     rows = []\n",
    "#     current_period = None\n",
    "\n",
    "#     # Debug: Check if play-by-play periods exist\n",
    "#     periods = soup.find_all('div', class_='play-by-play-period')\n",
    "#     print(f\"Periods found: {len(periods)}\")\n",
    "    \n",
    "#     for period in periods:\n",
    "#         # Get the current period from the header\n",
    "#         header = period.find('h3')\n",
    "#         if header:\n",
    "#             current_period = header.text.strip()\n",
    "#             print(f\"Current Period: {current_period}\")\n",
    "\n",
    "#         # Extract play rows\n",
    "#         play_rows = period.find_all('tr')\n",
    "#         print(f\"Rows found in this period: {len(play_rows)}\")\n",
    "#         for play_row in play_rows:\n",
    "#             columns = play_row.find_all('td')\n",
    "#             if len(columns) < 3:  # Skip invalid rows\n",
    "#                 continue\n",
    "\n",
    "#             # Parse columns\n",
    "#             time = columns[0].text.strip()\n",
    "#             team = columns[1].find('img')['alt'].strip() if columns[1].find('img') else None\n",
    "#             description = columns[2].text.strip()\n",
    "#             score = columns[3].text.strip() if len(columns) > 3 else None\n",
    "\n",
    "#             # Convert time to seconds\n",
    "#             if ':' in time:\n",
    "#                 minutes, seconds = map(int, time.split(':'))\n",
    "#                 game_time_seconds = minutes * 60 + seconds\n",
    "#             else:\n",
    "#                 game_time_seconds = None\n",
    "\n",
    "#             # Append the data\n",
    "#             rows.append({\n",
    "#                 'Game Time (Seconds)': game_time_seconds,\n",
    "#                 'Period': current_period,\n",
    "#                 'Team': team,\n",
    "#                 'Action Description': description,\n",
    "#                 'Score': score,\n",
    "#             })\n",
    "\n",
    "#     # Convert to DataFrame\n",
    "#     return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# url = 'https://www.ncaa.com/game/6344804/play-by-play'\n",
    "\n",
    "# html_content = get_url_response(url)\n",
    "# df = parse_html_to_table(html_content)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Time On Ice\n",
    "- Still not in it's own notebook 0 1-23-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "# Basics\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File Paths\n",
    "folder_prefix = ''\n",
    "# folder_prefix = '..'\n",
    "data_folder = os.path.join(folder_prefix, '..', 'data/') # Data Folder Path\n",
    "temp_folder = os.path.join(folder_prefix,'..', 'TEMP/',) # Temp Folder Path\n",
    "TEMP_FOLDER = temp_folder # Temp Folder Path as used in legacy code\n",
    "output_folder = os.path.join(temp_folder, 'team_comp_output/') # Output Folder Path\n",
    "# data\\db\\2024_Dec_10_CLEANED_OLD_METHOD.db\n",
    "db_path = os.path.join(data_folder, 'db', '2025_Jan_19_CLEAN.db') # Database Path\n",
    "\n",
    "image_folder = os.path.join(folder_prefix, '..', 'images/') # Image Folder Path\n",
    "logo_folder = os.path.join(folder_prefix, image_folder, 'logos/') # Logo Folder Path\n",
    "conference_logo_folder = os.path.join(folder_prefix, logo_folder, 'conference') # Conference Logo Folder Path\n",
    "export_folder = os.path.join(folder_prefix, image_folder, 'export/') # Export Folder Path\n",
    "background_folder = os.path.join(folder_prefix, image_folder, 'background/') # Background Folder Path\n",
    "\n",
    "# Other paths\n",
    "school_info_path = os.path.join(data_folder, 'arena_school_info.csv') # School Info Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database connection and extract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the database\n",
    "conn = sqlite3.connect(db_path, isolation_level=None)\n",
    "\n",
    "## Extract player_stats and convert TOI into seconds for easier calculations\n",
    "player_stats = pd.read_sql_query(\"SELECT * FROM player_stats\", conn)\n",
    "\n",
    "### TOI to seconds - From MM:SS string to seconds integer\n",
    "def convert_toi_to_seconds(toi_str):\n",
    "    if pd.isna(toi_str):\n",
    "        return None\n",
    "    try:\n",
    "        minutes, seconds = map(int, toi_str.split(':'))\n",
    "        return minutes * 60 + seconds\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "player_stats['TOI'] = player_stats['TOI'].apply(convert_toi_to_seconds)\n",
    "\n",
    "# Check Data\n",
    "print(player_stats.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a date column for each row based on the Game_ID to track change in ice time over the season\n",
    "def get_date_from_game_id(game_id):\n",
    "    if pd.isna(game_id):\n",
    "        return None\n",
    "    try:\n",
    "        return str(game_id)[:10]\n",
    "\n",
    "        # Convert to datetime object\n",
    "        date_obj = datetime.strptime(game_id, '%Y-%m-%d')\n",
    "        # Convert to desired format\n",
    "        \n",
    "\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Create a new column 'Date' in the DataFrame\n",
    "player_stats['Date'] = player_stats['Game_ID'].apply(get_date_from_game_id)\n",
    "\n",
    "# Check Data\n",
    "print(player_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time On Ice Exploration\n",
    "\n",
    "- WANT TO DO\n",
    "    - Get Line and position for each player for each game - link by game_id can also pull in shot and whatever other data might want (blocked shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call the line_chart db table to the a dataframe\n",
    "line_chart = pd.read_sql_query(\"SELECT * FROM line_chart\", conn)\n",
    "\n",
    "# Group into F / D / G lines based on Position column into new pos_1 / pos_2 columns\n",
    "# # pos_1 = F / D / G \n",
    "# Center, Left Wing, Right Wing = F\n",
    "# left D, Right D = D\n",
    "# Goalie = G\n",
    "\n",
    "# pos_2 = C / L / R / D / G\n",
    "\n",
    "\n",
    "## Assign positions in the new columns\n",
    "def assign_positions(row):\n",
    "    if row['Position'] in ['Center', 'Left Wing', 'Right Wing']:\n",
    "        return pd.Series(['F', row['Position'][0]])\n",
    "    elif row['Position'] in ['Left D', 'Right D']:\n",
    "        return pd.Series(['D', 'D'])\n",
    "    elif row['Position'] == 'Goalie':\n",
    "        return pd.Series(['G', 'G'])\n",
    "    else:\n",
    "        return pd.Series([None, None])\n",
    "line_chart[['pos_1', 'pos_2']] = line_chart.apply(assign_positions, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check Data\n",
    "print(line_chart.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge Position Data in with the player_stats dataframe based on Team, Player, and Game_ID\n",
    "\n",
    "merged_df = pd.merge(player_stats, line_chart[['Game_ID', 'Team', 'Player', 'Line', 'pos_1', 'pos_2']], on=['Game_ID', 'Team', 'Player'], how='left')\n",
    "\n",
    "# Relabel the rows of extra skaters - they have NaN in Line pos1 / pos_2 columns - Replace with 'E'\n",
    "# merged_df.loc[merged_df['pos_1'].isna(), 'pos_1'] = 'E'\n",
    "# merged_df.loc[merged_df['pos_2'].isna(), 'pos_2'] = 'E'\n",
    "merged_df['pos_1'].fillna('E', inplace=True)\n",
    "merged_df['pos_2'].fillna('E', inplace=True)\n",
    "merged_df['Line'].fillna('E', inplace=True)\n",
    "\n",
    "# Remove any rows with TOI = 0\n",
    "merged_df = merged_df[merged_df['TOI'] != 0]\n",
    "\n",
    "\n",
    "# Check Data\n",
    "merged_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some Basic statitical exploration\n",
    "\n",
    "# ### Calculate mean and median TOI for forwards and defense by team\n",
    "def calculate_mean_median_toi(player_stats):\n",
    "    # Group by team and position\n",
    "    grouped = player_stats.groupby(['Team', 'pos_1'])\n",
    "\n",
    "    # Calculate mean and median TOI\n",
    "    mean_toi = grouped['TOI'].mean()\n",
    "    median_toi = grouped['TOI'].median()\n",
    "\n",
    "    return pd.DataFrame({'Mean_TOI': mean_toi, 'Median_TOI': median_toi}).reset_index()\n",
    "\n",
    "mean_median_df = calculate_mean_median_toi(merged_df)\n",
    "\n",
    "# Drop G rows\n",
    "mean_median_df = mean_median_df[mean_median_df['pos_1'] != 'G']\n",
    "\n",
    "\n",
    "# Check Data\n",
    "mean_median_df.head()\n",
    "mean_median_df.tail()\n",
    "# mean_median_df.info()\n",
    "# mean_median_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean, median based on Line and Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the mean and median TOI for each team grouped by line and pos_1\n",
    "\n",
    "def calculate_mean_median_toi_by_line(player_stats):\n",
    "    # Group by team, line, and position\n",
    "    grouped = player_stats.groupby(['Team', 'Line', 'pos_1'])\n",
    "\n",
    "    # Calculate mean and median TOI\n",
    "    mean_toi = grouped['TOI'].mean()\n",
    "    median_toi = grouped['TOI'].median()\n",
    "\n",
    "    return pd.DataFrame({'Mean_TOI': mean_toi, 'Median_TOI': median_toi}).reset_index()\n",
    "\n",
    "mean_median_by_line_df = calculate_mean_median_toi_by_line(merged_df)\n",
    "\n",
    "# Drop G rows\n",
    "mean_median_by_line_df = mean_median_by_line_df[mean_median_by_line_df['pos_1'] != 'G']\n",
    "\n",
    "# Check Data\n",
    "mean_median_by_line_df.head()\n",
    "\n",
    "# Info\n",
    "# mean_median_by_line_df.info()\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Histogram off TOI by F / D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Eaxamine the data with hisotgrams of TOI by position\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Create a histogram for TOI by position\n",
    "sns.histplot(data=merged_df, x='TOI', hue='pos_1', kde=True, stat='density', common_norm=False)\n",
    "# Set the title and labels\n",
    "plt.title('TOI by Position')\n",
    "plt.xlabel('TOI (seconds)')\n",
    "plt.ylabel('Density')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Hosoptgram of TOI Forwards Only on Line assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same thing by line - only forwards\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Create a histogram for TOI by position\n",
    "sns.histplot(data=merged_df[merged_df['pos_1'] == 'F'], x='TOI', hue='Line', kde=True, stat='density', common_norm=False)\n",
    "# Set the title and labels\n",
    "plt.title('TOI by Line')\n",
    "plt.xlabel('TOI (seconds)')\n",
    "plt.ylabel('Density')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Set the figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a plot of TOI for Michigan State fowards over time grouped by line #\n",
    "set_team = 'Michigan State'\n",
    "set_team = 'Michigan'\n",
    "set_team = 'Minnesota'\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Convert TOI column from seconds to minutes\n",
    "merged_df['TOI'] = merged_df['TOI'] / 60\n",
    "\n",
    "# Create a line plot for TOI over time for Michigan State forwards\n",
    "for line in merged_df[(merged_df['pos_1'] == 'F') & (merged_df['Team'] == set_team)]['Line'].unique():\n",
    "    sns.lineplot(data=merged_df[(merged_df['pos_1'] == 'F') & (merged_df['Team'] == set_team) & (merged_df['Line'] == line)], x='Date', y='TOI', label=line)\n",
    "# Set the title and labels\n",
    "plt.title(f'Time on Ice by Line for {set_team} Forwards')\n",
    "plt.xlabel('Date')\n",
    "# Convert the tick marks on the y axis into minutes (divide by 60)\n",
    "plt.ylabel('TOI (minutes)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.ylabel('TOI (seconds)')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Set the figure size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defensive pairs time on ice Plot\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Convert TOI column from seconds to minutes\n",
    "# merged_df['TOI'] = merged_df['TOI'] / 60\n",
    "# Create a line plot for TOI over time for Michigan State defensmen\n",
    "for line in merged_df[(merged_df['pos_1'] == 'D') & (merged_df['Team'] == set_team)]['Line'].unique():\n",
    "    sns.lineplot(data=merged_df[(merged_df['pos_1'] == 'D') & (merged_df['Team'] == set_team) & (merged_df['Line'] == line)], x='Date', y='TOI', label=line)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title(f'Time on Ice by Line for {set_team} Defensmen')\n",
    "plt.xlabel('Date')\n",
    "# Convert the tick marks on the y axis into minutes (divide by 60)\n",
    "plt.ylabel('TOI (minutes)')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the database\n",
    "conn = sqlite3.connect(db_path, isolation_level=None)\n",
    "\n",
    "# convert string time to continuous time\n",
    "## SQL query to fetch\n",
    "def extract_goal_summary(conn):\n",
    "    \"\"\"\n",
    "    Extracts and preprocesses the goal summary data from the database.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        WITH UniqueGoals AS (\n",
    "        SELECT DISTINCT Game_ID, Team, Period, Time, PP\n",
    "        FROM scoring_summary\n",
    "    )\n",
    "    SELECT * FROM UniqueGoals;\n",
    "    \"\"\"\n",
    "    goal_df = pd.read_sql(query, conn)\n",
    "    return goal_df\n",
    "\n",
    "# Convert string time to continuous time\n",
    "def convert_to_continuous_time(row):\n",
    "    \"\"\"\n",
    "    Converts period-based time to a continuous format (0-65 minutes).\n",
    "    \"\"\"\n",
    "    period_offsets = {'1st Period': 0, '2nd Period': 20, '3rd Period': 40, 'Overtime': 60}\n",
    "    minutes, seconds = map(int, row['Time'].split(':'))\n",
    "    offset = period_offsets.get(row['Period'], 0)\n",
    "    return offset + minutes + seconds / 60.0\n",
    "\n",
    "## Load the data\n",
    "goal_data = extract_goal_summary(conn)\n",
    "# Create a continuous time column\n",
    "goal_data['Cont_Time'] = goal_data.apply(convert_to_continuous_time, axis=1)\n",
    "\n",
    "goal_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the scoring table calculate each team's average goals scored and allowed in each period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor code to address issues with games played and overtime calculations\n",
    "\n",
    "# Step 1: Ensure games played are counted correctly by counting unique Game_ID values\n",
    "games_played = goal_data.groupby('Team')['Game_ID'].nunique().reset_index()\n",
    "games_played.rename(columns={'Game_ID': 'games_played'}, inplace=True)\n",
    "\n",
    "# Step 2: Identify games that went to overtime\n",
    "# A game goes to overtime if the goals scored by both teams across the first three periods are equal\n",
    "goal_data['Period'] = goal_data['Period'].str.strip()  # Standardize 'Period' column\n",
    "regulation_periods = ['1st Period', '2nd Period', '3rd Period']\n",
    "\n",
    "# Sum goals by team and game across regulation periods\n",
    "regulation_goals = goal_data[goal_data['Period'].isin(regulation_periods)]\n",
    "regulation_totals = regulation_goals.groupby(['Game_ID', 'Team']).size().unstack(fill_value=0)\n",
    "\n",
    "# Identify games tied after regulation\n",
    "overtime_games = regulation_totals[regulation_totals.apply(lambda row: row.sum() == 0, axis=1)].index\n",
    "\n",
    "# Count overtime games for each team\n",
    "ot_games_played = goal_data[goal_data['Game_ID'].isin(overtime_games)].groupby('Team')['Game_ID'].nunique()\n",
    "ot_games_played = ot_games_played.reset_index()\n",
    "ot_games_played.rename(columns={'Game_ID': 'OT_games_played'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate period-by-period stats\n",
    "team_period_stats = []\n",
    "for team in games_played['Team']:\n",
    "    team_data = goal_data[goal_data['Team'] == team]\n",
    "    opponent_data = goal_data[goal_data['Team'] != team]\n",
    "    team_games = team_data['Game_ID'].unique()\n",
    "    row = {'Team': team, 'games_played': games_played.loc[games_played['Team'] == team, 'games_played'].values[0]}\n",
    "\n",
    "    for i, period in enumerate(regulation_periods, start=1):  # Process 1st, 2nd, 3rd periods\n",
    "        scored = team_data[team_data['Period'] == period].shape[0]\n",
    "        allowed = opponent_data[(opponent_data['Game_ID'].isin(team_games)) & (opponent_data['Period'] == period)].shape[0]\n",
    "        avg_scored = scored / row['games_played']\n",
    "        avg_allowed = allowed / row['games_played']\n",
    "        diff = scored - allowed\n",
    "\n",
    "        period_label = ['1st', '2nd', '3rd'][i - 1]\n",
    "        row[f'{period_label}_scored'] = scored\n",
    "        row[f'{period_label}_allowed'] = allowed\n",
    "        row[f'{period_label}_scored_avg'] = avg_scored\n",
    "        row[f'{period_label}_allowed_avg'] = avg_allowed\n",
    "        row[f'{period_label}_diff'] = diff\n",
    "\n",
    "\n",
    "\n",
    "    team_period_stats.append(row)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "results = pd.DataFrame(team_period_stats)\n",
    "\n",
    "# Display the refactored results\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Calculate the AVG differential for each period\n",
    "results['1st_diff_avg'] = results['1st_diff'] / results['games_played']\n",
    "results['2nd_diff_avg'] = results['2nd_diff'] / results['games_played']\n",
    "results['3rd_diff_avg'] = results['3rd_diff'] / results['games_played']\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working and moved to it's own book for visualizations\n",
    "- High_Impact_Goals_with_viz\n",
    "### Load scoring table and make table of goals scored first and last minute of periods as well as total goals scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "# Basics\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from matplotlib import font_manager\n",
    "\n",
    "\n",
    "## Load The Exo 2 font in case of problems with the font\n",
    "font_path = 'C:/Windows/Fonts/Exo 2.tff'\n",
    "locations =['../data/Exo_2'] # Font Location\n",
    "\n",
    "font_files = font_manager.findSystemFonts(fontpaths=locations)\n",
    "\n",
    "for file in font_files:\n",
    "    font_manager.fontManager.addfont(file)\n",
    "\n",
    "    ## Set the date to appear on the source area of plots (the last date of the data)\n",
    "last_game_date = '2025-01-05'\n",
    "\n",
    "## File Paths\n",
    "folder_prefix = ''\n",
    "# folder_prefix = '..'\n",
    "data_folder = os.path.join(folder_prefix, '..', 'data/') # Data Folder Path\n",
    "temp_folder = os.path.join(folder_prefix,'..', 'TEMP/',) # Temp Folder Path\n",
    "TEMP_FOLDER = temp_folder # Temp Folder Path as used in legacy code\n",
    "output_folder = os.path.join(temp_folder, 'team_comp_output/') # Output Folder Path\n",
    "# data\\db\\2024_Dec_10_CLEANED_OLD_METHOD.db\n",
    "db_path = os.path.join(data_folder, 'db', '2025_Jan_07_test2_ROUGH.db') # Database Path\n",
    "# db_path = os.path.join(data_folder, 'db', '2024_Dec_03_v4_ROUGH.db') # Database Path\n",
    "# db_path = os.path.join(temp_folder, '2024_Dec_03_v3_ROUGH.db') # Database Path\n",
    "image_folder = os.path.join(folder_prefix, '..', 'images/') # Image Folder Path\n",
    "logo_folder = os.path.join(folder_prefix, image_folder, 'logos/') # Logo Folder Path\n",
    "conference_logo_folder = os.path.join(folder_prefix, logo_folder, 'conference') # Conference Logo Folder Path\n",
    "export_folder = os.path.join(folder_prefix, image_folder, 'export/') # Export Folder Path\n",
    "background_folder = os.path.join(folder_prefix, image_folder, 'background/') # Background Folder Path\n",
    "\n",
    "# Other paths\n",
    "school_info_path = os.path.join(data_folder, 'arena_school_info.csv') # School Info Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the database\n",
    "conn = sqlite3.connect(db_path, isolation_level=None)\n",
    "\n",
    "# convert string time to continuous time\n",
    "## SQL query to fetch\n",
    "def extract_goal_summary(conn):\n",
    "    \"\"\"\n",
    "    Extracts and preprocesses the goal summary data from the database.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        WITH UniqueGoals AS (\n",
    "        SELECT DISTINCT Game_ID, Team, Period, Time, PP\n",
    "        FROM scoring_summary\n",
    "    )\n",
    "    SELECT * FROM UniqueGoals;\n",
    "    \"\"\"\n",
    "    goal_df = pd.read_sql(query, conn)\n",
    "    return goal_df\n",
    "\n",
    "# Convert string time to continuous time\n",
    "def convert_to_continuous_time(row):\n",
    "    \"\"\"\n",
    "    Converts period-based time to a continuous format (0-65 minutes).\n",
    "    \"\"\"\n",
    "    period_offsets = {'1st Period': 0, '2nd Period': 20, '3rd Period': 40, 'Overtime': 60}\n",
    "    minutes, seconds = map(int, row['Time'].split(':'))\n",
    "    offset = period_offsets.get(row['Period'], 0)\n",
    "    return offset + minutes + seconds / 60.0\n",
    "\n",
    "## Load the data\n",
    "goal_data = extract_goal_summary(conn)\n",
    "# Create a continuous time column\n",
    "goal_data['Cont_Time'] = goal_data.apply(convert_to_continuous_time, axis=1)\n",
    "\n",
    "goal_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify Empty Net Goals (EN in the PP column) and flag in a new column\n",
    "goal_data['EN'] = goal_data['PP'].apply(lambda x: 'EN' in x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First minute of frame time ranges\n",
    "# 0-1, 20-21, 40-41, 60-61\n",
    "\n",
    "# Last minute of frame time ranges\n",
    "# 19-20, 39-40, 59-60, 64-65\n",
    "\n",
    "# Define the function for categorizing goal types\n",
    "def goal_type_first_last(row):\n",
    "    \"\"\"\n",
    "    Categorizes goals as first minute, last minute, or other.\n",
    "    \"\"\"\n",
    "    # Check if the goal is in the first minute\n",
    "    if (row['Cont_Time'] < 1 or  # Before the end of the first minute of the game\n",
    "        (row['Cont_Time'] > 20 and row['Cont_Time'] < 21) or  # Between 20:00 and 21:00 (1st period)\n",
    "        (row['Cont_Time'] > 40 and row['Cont_Time'] < 41) or  # Between 40:00 and 41:00 (2nd period)\n",
    "        (row['Cont_Time'] > 60 and row['Cont_Time'] < 61)):  # Between 60:00 and 61:00 (3rd period)\n",
    "        return 'First Minute'\n",
    "\n",
    "    # Check if the goal is in the last minute of a period\n",
    "    elif (row['Cont_Time'] > 19 and row['Cont_Time'] < 20 or  # Between 19:00 and 20:00 (1st period)\n",
    "          (row['Cont_Time'] > 39 and row['Cont_Time'] < 40) or  # Between 39:00 and 40:00 (2nd period)\n",
    "          (row['Cont_Time'] > 59 and row['Cont_Time'] < 60) or  # Between 59:00 and 60:00 (3rd period)\n",
    "          (row['Cont_Time'] > 64 and row['Cont_Time'] < 65)):  # Between 64:00 and 65:00 (overtime)\n",
    "        return 'Last Minute'\n",
    "\n",
    "    # Otherwise, categorize as 'Other'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "\n",
    "# Create a new column for the goal type\n",
    "goal_data['Goal_Type'] = goal_data.apply(goal_type_first_last, axis=1)\n",
    "\n",
    "\n",
    "# Check distribution of goal types\n",
    "# goal_data['Goal_Type'].value_counts()\n",
    "\n",
    "goal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do the Same thing to flag the goals that happened in the first and last 2 minutes of the period\n",
    "\n",
    "def goal_type_first2_last_2(row):\n",
    "    \"\"\"\n",
    "    Flag the goals that happened in the first and last 2 minutes of the period\n",
    "    \"\"\"\n",
    "\n",
    "    if (row['Cont_Time'] < 2 or  # Before the end of the first minute of the game\n",
    "        (row['Cont_Time'] > 20 and row['Cont_Time'] < 22) or  # Between 20:00 and 21:00 (1st period)\n",
    "        (row['Cont_Time'] > 40 and row['Cont_Time'] < 42) or  # Between 40:00 and 41:00 (2nd period)\n",
    "        (row['Cont_Time'] > 60 and row['Cont_Time'] < 62)):  # Between 60:00 and 61:00 (3rd period)\n",
    "        return 'First 2 Minutes'\n",
    "\n",
    "    # Check if the goal is in the last 2 minutes of a period\n",
    "    elif (row['Cont_Time'] > 18 and row['Cont_Time'] < 20 or  # Between 18:00 and 20:00 (1st period)\n",
    "          (row['Cont_Time'] > 38 and row['Cont_Time'] < 40) or  # Between 38:00 and 40:00 (2nd period)\n",
    "          (row['Cont_Time'] > 58 and row['Cont_Time'] < 60) or  # Between 58:00 and 60:00 (3rd period)\n",
    "          (row['Cont_Time'] > 63 and row['Cont_Time'] < 65)):  # Between 63:00 and 65:00 (overtime)\n",
    "        return 'Last 2 Minutes'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Create a new column to flag the goals that happened in the first and last 2 minutes of the period\n",
    "goal_data['Goal_Type_2'] = goal_data.apply(goal_type_first2_last_2, axis=1)\n",
    "\n",
    "goal_data.head()\n",
    "# Check distribution of goal types\n",
    "goal_data['Goal_Type_2'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if The EN goals are in the last minute and flag them as such in new column\n",
    "goal_data['EN_Last_Minute'] = (goal_data['EN'] & (goal_data['Goal_Type'] == 'Last Minute'))\n",
    "# DO the same for the last 2 minutes\n",
    "goal_data['EN_Last_2_Minutes'] = (goal_data['EN'] & (goal_data['Goal_Type_2'] == 'Last 2 Minutes'))\n",
    "\n",
    "goal_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count High Impact Goals (Goals scored within a minute or 2 of another goal being scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by Game_ID and Cont_Time for chronological processing\n",
    "goal_data_sorted = goal_data.sort_values(by=['Game_ID', 'Cont_Time']).reset_index(drop=True)\n",
    "\n",
    "# Display the sorted data to verify\n",
    "goal_data_sorted.head()\n",
    "\n",
    "# Group data by Game_ID to analyze each game separately\n",
    "grouped = goal_data_sorted.groupby('Game_ID')\n",
    "\n",
    "\n",
    "# Debugging the logic and fixing the issue\n",
    "# Reinitialize dictionaries to track counts\n",
    "team_quick_responses = defaultdict(int)\n",
    "opponent_quick_responses = defaultdict(int)\n",
    "\n",
    "# Process each game individually again\n",
    "for game_id, game_data in grouped:\n",
    "    # Reset index for easier row iteration\n",
    "    game_data = game_data.reset_index(drop=True)\n",
    "    \n",
    "    # Iterate through goals in this game\n",
    "    for i in range(len(game_data)):\n",
    "        current_team = game_data.loc[i, 'Team']\n",
    "        current_time = game_data.loc[i, 'Cont_Time']\n",
    "        \n",
    "        # Compare with subsequent goals in the same game\n",
    "        for j in range(i + 1, len(game_data)):\n",
    "            next_team = game_data.loc[j, 'Team']\n",
    "            next_time = game_data.loc[j, 'Cont_Time']\n",
    "            \n",
    "            # If the time difference is more than 1 minute, stop checking\n",
    "            if next_time - current_time > 1:\n",
    "                break\n",
    "            \n",
    "            # If the same team scores again within 1 minute\n",
    "            if next_team == current_team:\n",
    "                team_quick_responses[current_team] += 1\n",
    "            \n",
    "            # If the opposing team scores within 1 minute\n",
    "            elif next_team != current_team:\n",
    "                opponent_quick_responses[current_team] += 1\n",
    "\n",
    "# Create the results DataFrame with updated column labels\n",
    "quick_responses_df = pd.DataFrame({\n",
    "    'Team': list(set(goal_data_sorted['Team'])),\n",
    "    'HI_Back_to_Back': [team_quick_responses[team] for team in set(goal_data_sorted['Team'])],\n",
    "    'HI_Quick_Response': [opponent_quick_responses[team] for team in set(goal_data_sorted['Team'])]\n",
    "})\n",
    "\n",
    "quick_responses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize dictionaries to track the new counts\n",
    "team_quick_responses_2 = defaultdict(int)\n",
    "opponent_quick_responses_2 = defaultdict(int)\n",
    "\n",
    "# Process each game again for the new 2-minute interval logic\n",
    "for game_id, game_data in grouped:\n",
    "    # Reset index for easier row iteration\n",
    "    game_data = game_data.reset_index(drop=True)\n",
    "    \n",
    "    # Iterate through goals in this game\n",
    "    for i in range(len(game_data)):\n",
    "        current_team = game_data.loc[i, 'Team']\n",
    "        current_time = game_data.loc[i, 'Cont_Time']\n",
    "        \n",
    "        # Compare with subsequent goals in the same game\n",
    "        for j in range(i + 1, len(game_data)):\n",
    "            next_team = game_data.loc[j, 'Team']\n",
    "            next_time = game_data.loc[j, 'Cont_Time']\n",
    "            \n",
    "            # If the time difference is more than 2 minutes, stop checking for 2-minute responses\n",
    "            if next_time - current_time > 2:\n",
    "                break\n",
    "            \n",
    "            # If the same team scores again within 2 minutes\n",
    "            if next_team == current_team:\n",
    "                team_quick_responses_2[current_team] += 1\n",
    "            \n",
    "            # If the opposing team scores within 2 minutes\n",
    "            elif next_team != current_team:\n",
    "                opponent_quick_responses_2[current_team] += 1\n",
    "\n",
    "# Add the new columns to the results DataFrame\n",
    "quick_responses_df['HI_Back_to_Back_2'] = [\n",
    "    team_quick_responses_2[team] for team in set(goal_data_sorted['Team'])\n",
    "]\n",
    "quick_responses_df['HI_Quick_Response_2'] = [\n",
    "    opponent_quick_responses_2[team] for team in set(goal_data_sorted['Team'])\n",
    "]\n",
    "\n",
    "# Display the updated results\n",
    "quick_responses_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_column_presence(goal_tally, columns):\n",
    "    \"\"\"\n",
    "    Ensure that all required columns are present in the goal tally DataFrame.\n",
    "    If a column is missing, add it and fill with zeros.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in goal_tally.columns:\n",
    "            goal_tally[col] = 0\n",
    "    return goal_tally\n",
    "\n",
    "def robust_final_tally(goal_data):\n",
    "    \"\"\"\n",
    "    Tally the number of goals by team, ensuring all expected columns are enforced explicitly.\n",
    "    \"\"\"\n",
    "    # Define expected columns for each group type\n",
    "    expected_columns_type = ['First Minute', 'Last Minute', 'Other']\n",
    "    expected_columns_type2 = ['First 2 Minutes', 'Last 2 Minutes']\n",
    "\n",
    "    # Group and tally goals by Goal_Type\n",
    "    goal_tally_type = goal_data.groupby(['Team', 'Goal_Type']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # Ensure all expected columns are present for Goal_Type\n",
    "    goal_tally_type = enforce_column_presence(goal_tally_type, expected_columns_type)\n",
    "\n",
    "    # Group and tally goals by Goal_Type_2\n",
    "    goal_tally_type2 = goal_data.groupby(['Team', 'Goal_Type_2']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # Ensure all expected columns are present for Goal_Type_2\n",
    "    goal_tally_type2 = enforce_column_presence(goal_tally_type2, expected_columns_type2)\n",
    "\n",
    "    # Merge both grouped DataFrames\n",
    "    goal_tally = pd.merge(goal_tally_type, goal_tally_type2, on='Team', how='outer').fillna(0)\n",
    "\n",
    "    # Ensure all columns in the final DataFrame\n",
    "    all_expected_columns = ['Team'] + expected_columns_type + expected_columns_type2\n",
    "    goal_tally = enforce_column_presence(goal_tally, all_expected_columns)\n",
    "\n",
    "    # Calculate total goals\n",
    "    goal_tally['Total Goals'] = (goal_tally['First Minute'] +\n",
    "                                 \n",
    "                                 goal_tally['Last Minute'] +\n",
    "                                 \n",
    "                                 goal_tally['Other_x'])\n",
    "\n",
    "    # Calculate percentages\n",
    "    goal_tally['Pct First Minute'] = goal_tally['First Minute'] / goal_tally['Total Goals'].replace(0, 1)\n",
    "    goal_tally['Pct First 2 Minutes'] = goal_tally['First 2 Minutes'] / goal_tally['Total Goals'].replace(0, 1)\n",
    "    goal_tally['Pct Last Minute'] = goal_tally['Last Minute'] / goal_tally['Total Goals'].replace(0, 1)\n",
    "    goal_tally['Pct Last 2 Minutes'] = goal_tally['Last 2 Minutes'] / goal_tally['Total Goals'].replace(0, 1)\n",
    "\n",
    "    # Tally EN Last Minute and EN Last 2 Minutes\n",
    "    en_last_minute_tally = goal_data[goal_data['EN_Last_Minute']].groupby('Team').size()\n",
    "    en_last_2_minute_tally = goal_data[goal_data['EN_Last_2_Minutes']].groupby('Team').size()\n",
    "\n",
    "    # Add EN tallies to the final DataFrame\n",
    "    goal_tally['EN Last Minute'] = goal_tally['Team'].map(en_last_minute_tally).fillna(0).astype(int)\n",
    "    goal_tally['EN Last 2 Minutes'] = goal_tally['Team'].map(en_last_2_minute_tally).fillna(0).astype(int)\n",
    "\n",
    "    return goal_tally\n",
    "\n",
    "# Apply the function to the data\n",
    "team_goal_tally = robust_final_tally(goal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the High Impact goal data with the Team Goal Tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the High Impact Goal data with the goal tally data\n",
    "team_goal_tally = pd.merge(team_goal_tally, quick_responses_df, on='Team', how='outer')\n",
    "\n",
    "# Reorganize the table into the following order\n",
    "# Team, Total Goals, First minute, Pct First Minute, First 2 Minutes, Pct First 2 Minutes, \n",
    "# Last Minute, EN Last Minute, Pct Last Minute, Last 2 Minutes, EN Last 2 Minutes, Pct Last 2 Minutes, HI Back-to-Back, \n",
    "# HI Quick Response, HI Back-to-Back 2, HI Quick Response 2\n",
    "\n",
    "# Define the column order\n",
    "column_order = ['Team', 'Total Goals', 'First Minute', 'Pct First Minute', 'First 2 Minutes', 'Pct First 2 Minutes',\n",
    "                'Last Minute', 'EN Last Minute', 'Pct Last Minute', 'Last 2 Minutes', 'EN Last 2 Minutes', 'Pct Last 2 Minutes',\n",
    "                'HI_Back_to_Back', 'HI_Quick_Response', 'HI_Back_to_Back_2', 'HI_Quick_Response_2']\n",
    "\n",
    "# Reorder the columns\n",
    "team_goal_tally = team_goal_tally[column_order]\n",
    "\n",
    "\n",
    "# Display the final DataFrame\n",
    "team_goal_tally.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ORIGINAL CODE #########\n",
    "## Tally the number of goals by team and goal type\n",
    "# Output a Table with the following columns:\n",
    "# Team, First Minute Goals, Last Minute Goals, Other Goals, Total Goals, Pct First Minute, Pct Last Minute\n",
    "# def tally_goals_by_team(goal_data):\n",
    "#     \"\"\"\n",
    "#     Tally the number of goals by team and goal type.\n",
    "#     \"\"\"\n",
    "#     # Group by team and goal type\n",
    "#     goal_tally = goal_data.groupby(['Team', 'Goal_Type']).size().unstack().reset_index()\n",
    "\n",
    "#     # Fill in missing columns\n",
    "#     goal_tally = goal_tally.fillna(0)\n",
    "\n",
    "#     # Calculate total goals\n",
    "#     goal_tally['Total Goals'] = goal_tally['First Minute'] + goal_tally['Last Minute'] + goal_tally['Other']\n",
    "\n",
    "#     # Calculate percentages\n",
    "#     goal_tally['Pct First Minute'] = goal_tally['First Minute'] / goal_tally['Total Goals']\n",
    "#     goal_tally['Pct Last Minute'] = goal_tally['Last Minute'] / goal_tally['Total Goals']\n",
    "#     goal_tally['Pct Other'] = goal_tally['Other'] / goal_tally['Total Goals']\n",
    "\n",
    "#     # Tally EN Last Minute Goals\n",
    "#     goal_tally['EN Last Minute'] = goal_data[goal_data['EN_Last_Minute']].groupby('Team').size()\n",
    "\n",
    "#     return goal_tally\n",
    "\n",
    "# # Tally the goals by team\n",
    "# team_goal_tally = tally_goals_by_team(goal_data)\n",
    "\n",
    "# team_goal_tally.head()\n",
    "\n",
    "# Value Counts for EN Last Minute Goals\n",
    "# goal_data['EN_Last_Minute'].value_counts()\n",
    "\n",
    "# team_goal_tally['EN Last Minute'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by First Minute Goals\n",
    "team_goal_tally = team_goal_tally.sort_values('First Minute', ascending=False)\n",
    "\n",
    "# Sort by Last Minute Goals\n",
    "team_goal_tally = team_goal_tally.sort_values('Last Minute', ascending=False)\n",
    "\n",
    "# sort by percentage of other goals (decending)\n",
    "\n",
    "\n",
    "# sort by total goals\n",
    "team_goal_tally = team_goal_tally.sort_values('Total Goals', ascending=False)\n",
    "\n",
    "team_goal_tally.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LONG ISLAND UNIVERSITY #### 1-8-24 IS MISSING\n",
    "### PROVIDENCE HAS ONE MORE GOAL THAN CHN TABLES #### 1-8-24\n",
    "######## SOMETHING MIGHT BE UP WITH OHIO STATE ########\n",
    "#### DOES NOT MATCH CHN TABLES #### 1-8-24\n",
    "\n",
    "#### 1-8-24 - DISCOVERED THAT CHNS TEAM STATS TABLE IS NOT ACCURATE - OHIO STATE LISTED AS ONLY HAVING 58 GOALS, WHEN YOU CLICK THROUGH TO PLAYER BREAKDOWN THEY TALLY TO 64, JUST LIKE MY DATA HAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scorigami - Annimated Gif Code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "image_folder = os.path.join('..', 'TEMP', 'IMAGES', 'stich_folder')\n",
    "\n",
    "\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import re  # For extracting numbers from filenames\n",
    "\n",
    "# def create_animated_gif(image_folder, output_gif, total_duration=5, transition_frames=10):\n",
    "#     \"\"\"\n",
    "#     Create an animated GIF from a sequence of images with fade transitions.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - image_folder: Path to the folder containing images (named with leading numbers, e.g., 1_*.png, 2_*.png).\n",
    "#     - output_gif: Path for the output GIF.\n",
    "#     - total_duration: Total duration of the animation in seconds.\n",
    "#     - transition_frames: Number of intermediate frames for transitions between images.\n",
    "#     \"\"\"\n",
    "#     # Load images sorted by the leading number in filenames\n",
    "#     images = sorted(\n",
    "#         [Image.open(os.path.join(image_folder, img)) for img in os.listdir(image_folder) if img.endswith(\".png\")],\n",
    "#         key=lambda x: int(re.match(r\"(\\d+)\", os.path.basename(x.filename)).group(1))  # Extract leading numbers\n",
    "#     )\n",
    "\n",
    "#     # Resize images to a suitable size while maintaining the aspect ratio\n",
    "#     # Set max width and height\n",
    "#     max_width = 800\n",
    "#     max_height = 1080\n",
    "\n",
    "#     for i, img in enumerate(images):\n",
    "#         width, height = img.size\n",
    "#         if width > max_width or height > max_height:\n",
    "#             # Resize the image\n",
    "#             ratio = min(max_width / width, max_height / height)\n",
    "#             new_size = (int(width * ratio), int(height * ratio))\n",
    "#             images[i] = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     # Calculate total frames and duration per frame\n",
    "#     num_images = len(images)\n",
    "#     frames_per_image = total_duration * 1000 // (num_images + (num_images - 1) * transition_frames)\n",
    "#     frame_duration = int(frames_per_image)  # Duration of each frame in milliseconds\n",
    "\n",
    "#     all_frames = []\n",
    "#     for i in range(num_images - 1):\n",
    "#         # Add the current image\n",
    "#         all_frames.append(images[i])\n",
    "        \n",
    "#         # Create transition frames (fade to next image)\n",
    "#         for t in range(1, transition_frames + 1):\n",
    "#             alpha = t / (transition_frames + 1)\n",
    "#             blend_frame = Image.blend(images[i], images[i + 1], alpha)\n",
    "#             all_frames.append(blend_frame)\n",
    "\n",
    "#     # Add the final image\n",
    "#     all_frames.append(images[-1])\n",
    "\n",
    "#     # Save all frames as a GIF\n",
    "#     all_frames[0].save(\n",
    "#         output_gif,\n",
    "#         save_all=True,\n",
    "#         append_images=all_frames[1:],\n",
    "#         duration=frame_duration,\n",
    "#         loop=1\n",
    "#     )\n",
    "\n",
    "# def create_animated_gif(image_folder, output_gif, total_duration=5, transition_frames=10, reverse_order=False):\n",
    "#     \"\"\"\n",
    "#     Create an animated GIF from a sequence of images with fade transitions.\n",
    "\n",
    "#     Parameters:\n",
    "#     - image_folder: Path to the folder containing images (named with leading numbers, e.g., 1_*.png, 2_*.png).\n",
    "#     - output_gif: Path for the output GIF.\n",
    "#     - total_duration: Total duration of the animation in seconds.\n",
    "#     - transition_frames: Number of intermediate frames for transitions between images.\n",
    "#     - reverse_order: If True, creates the GIF in reverse order.\n",
    "#     \"\"\"\n",
    "#     # Load images sorted by the leading number in filenames\n",
    "#     images = sorted(\n",
    "#         [Image.open(os.path.join(image_folder, img)) for img in os.listdir(image_folder) if img.endswith(\".png\")],\n",
    "#         key=lambda x: int(re.match(r\"(\\d+)\", os.path.basename(x.filename)).group(1))  # Extract leading numbers\n",
    "#     )\n",
    "\n",
    "#     # Reverse the order if requested\n",
    "#     if reverse_order:\n",
    "#         images.reverse()\n",
    "\n",
    "#     # Resize images to a suitable size while maintaining the aspect ratio\n",
    "#     max_width = 800\n",
    "#     max_height = 1080\n",
    "#     for i, img in enumerate(images):\n",
    "#         width, height = img.size\n",
    "#         if width > max_width or height > max_height:\n",
    "#             ratio = min(max_width / width, max_height / height)\n",
    "#             new_size = (int(width * ratio), int(height * ratio))\n",
    "#             images[i] = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "#     # Calculate total frames and duration per frame\n",
    "#     num_images = len(images)\n",
    "#     frames_per_image = total_duration * 1000 // (num_images + (num_images - 1) * transition_frames)\n",
    "#     frame_duration = int(frames_per_image)\n",
    "\n",
    "#     all_frames = []\n",
    "#     for i in range(num_images - 1):\n",
    "#         all_frames.append(images[i])\n",
    "#         for t in range(1, transition_frames + 1):\n",
    "#             alpha = t / (transition_frames + 1)\n",
    "#             blend_frame = Image.blend(images[i], images[i + 1], alpha)\n",
    "#             all_frames.append(blend_frame)\n",
    "\n",
    "#     all_frames.append(images[-1])\n",
    "\n",
    "#     # Save all frames as a GIF\n",
    "#     all_frames[0].save(\n",
    "#         output_gif,\n",
    "#         save_all=True,\n",
    "#         append_images=all_frames[1:],\n",
    "#         duration=frame_duration,\n",
    "#         loop=0\n",
    "#     )\n",
    "\n",
    "def create_animated_gif(image_folder, output_gif, total_duration=5, transition_frames=10, reverse_order=False):\n",
    "    \"\"\"\n",
    "    Create an animated GIF with smooth transitions and precise duration control.\n",
    "\n",
    "    Parameters:\n",
    "    - image_folder: Path to the folder containing images (named with leading numbers, e.g., 1_*.png, 2_*.png).\n",
    "    - output_gif: Path for the output GIF.\n",
    "    - total_duration: Total duration of the animation in seconds.\n",
    "    - transition_frames: Number of intermediate frames for transitions between images.\n",
    "    - reverse_order: If True, creates the GIF in reverse order.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from PIL import Image\n",
    "    import re\n",
    "\n",
    "    # Load images sorted by the leading number in filenames\n",
    "    images = sorted(\n",
    "        [Image.open(os.path.join(image_folder, img)) for img in os.listdir(image_folder) if img.endswith(\".png\")],\n",
    "        key=lambda x: int(re.match(r\"(\\d+)\", os.path.basename(x.filename)).group(1))\n",
    "    )\n",
    "\n",
    "    # Reverse the order if requested\n",
    "    if reverse_order:\n",
    "        images.reverse()\n",
    "\n",
    "    # Resize images to maintain aspect ratio\n",
    "    max_width, max_height = 800, 1080\n",
    "    for i, img in enumerate(images):\n",
    "        width, height = img.size\n",
    "        if width > max_width or height > max_height:\n",
    "            ratio = min(max_width / width, max_height / height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            images[i] = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Total frames calculation\n",
    "    num_images = len(images)\n",
    "    total_frames = num_images + (num_images - 1) * transition_frames\n",
    "\n",
    "    # Frame duration in milliseconds\n",
    "    frame_duration = (total_duration * 1000) // total_frames\n",
    "\n",
    "    all_frames = []\n",
    "    durations = []\n",
    "\n",
    "    for i in range(num_images - 1):\n",
    "        # Add the current image as a static frame\n",
    "        all_frames.append(images[i])\n",
    "        durations.append(frame_duration)\n",
    "\n",
    "        # Create transition frames\n",
    "        for t in range(1, transition_frames + 1):\n",
    "            alpha = t / (transition_frames + 1)\n",
    "            blend_frame = Image.blend(images[i], images[i + 1], alpha)\n",
    "            all_frames.append(blend_frame)\n",
    "            durations.append(frame_duration)\n",
    "\n",
    "    # Add the final image\n",
    "    all_frames.append(images[-1])\n",
    "    durations.append(frame_duration)\n",
    "\n",
    "    # Save all frames as a GIF\n",
    "    all_frames[0].save(\n",
    "        output_gif,\n",
    "        save_all=True,\n",
    "        append_images=all_frames[1:],\n",
    "        duration=durations,\n",
    "        # loop=0  # Infinite loop\n",
    "        loop=1  # Loop once\n",
    "    )\n",
    "\n",
    "\n",
    "# # image_folder = \"/path/to/your/image/folder\"\n",
    "output_gif_reverse = os.path.join('..', 'TEMP', 'IMAGES', 'stich_folder', 'scorigami_all_time_reverse_animated.gif')\n",
    "output_gif = os.path.join('..', 'TEMP', 'IMAGES', 'stich_folder', 'scorigami_all_time_animated.gif')\n",
    "# total_duration = 20  # Total duration of the animation in seconds\n",
    "# transition_frames = 15  # Number of fade frames per transition\n",
    "\n",
    "\n",
    "\n",
    "## Reverse Order\n",
    "create_animated_gif(image_folder, output_gif_reverse, total_duration=10, transition_frames=0, reverse_order=False)\n",
    "\n",
    "## Reverse Order\n",
    "create_animated_gif(image_folder, output_gif, total_duration=10, transition_frames=0, reverse_order=True)\n",
    "\n",
    "\n",
    "# # Example Usage\n",
    "# # Set the folder containing images and output GIF path\n",
    "\n",
    "\n",
    "# create_animated_gif(image_folder, output_gif, total_duration, transition_frames)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load Schedult/results data and compare conferences\n",
    "path = os.path.join('..', 'data', 'schedule', 'Week 1 Scores.csv')\n",
    "\n",
    "# Load the data\n",
    "schedule_df = pd.read_csv(path)\n",
    "\n",
    "# filter out exhibition games\n",
    "schedule_df = schedule_df[schedule_df['Conference'] != 'Exhibition']\n",
    "# Clean up Team names (remove ' and periods)\n",
    "schedule_df['Away_Team'] = schedule_df['Away_Team'].str.replace(\"'\", \"\").str.replace(\".\", \"\")\n",
    "schedule_df['Home_Team'] = schedule_df['Home_Team'].str.replace(\"'\", \"\").str.replace(\".\", \"\")\n",
    "# strip leading and trailing spaces\n",
    "schedule_df['Away_Team'] = schedule_df['Away_Team'].str.strip()\n",
    "schedule_df['Home_Team'] = schedule_df['Home_Team'].str.strip()\n",
    "# Drop any rows containing a / or TBD\n",
    "schedule_df = schedule_df[~schedule_df['Away_Team'].str.contains('/')]\n",
    "schedule_df = schedule_df[~schedule_df['Home_Team'].str.contains('/')]\n",
    "schedule_df = schedule_df[~schedule_df['Away_Team'].str.contains('TBD')]\n",
    "schedule_df = schedule_df[~schedule_df['Home_Team'].str.contains('TBD')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the conferences\n",
    "conference_teams = {\n",
    "    'Atlantic': ['Air Force', \"American Intl\", 'Army', 'Bentley', 'Canisius', 'Holy Cross', 'Mercyhurst', \n",
    "                 'Niagara', 'RIT', 'Robert Morris', 'Sacred Heart'],\n",
    "    'Big Ten': ['Michigan', 'Michigan State', 'Minnesota', 'Notre Dame', 'Ohio State', 'Penn State', 'Wisconsin'],\n",
    "    'CCHA': ['Augustana', 'Bemidji State', 'Bowling Green', 'Ferris State', 'Lake Superior', 'Michigan Tech', \n",
    "             'Minnesota State', 'Northern Michigan', 'St Thomas'],\n",
    "    'ECAC': ['Brown', 'Clarkson', 'Colgate', 'Cornell', 'Dartmouth', 'Harvard', 'Princeton', 'Quinnipiac',\n",
    "             'Rensselaer', 'St Lawrence', 'Union', 'Yale'],\n",
    "    'Hockey East': ['Boston College', 'Boston University', 'Connecticut', 'Maine', 'Massachusetts', 'Mass Lowell',\n",
    "                    'Merrimack', 'New Hampshire', 'Northeastern', 'Providence', 'Vermont'],\n",
    "    'NCHC': ['Arizona State', 'Colorado College', 'Denver', 'Miami', 'Minnesota Duluth', 'North Dakota', 'Omaha', \n",
    "             'St Cloud State', 'Western Michigan'],\n",
    "    'Independents': ['Alaska Anchorage', 'Alaska', 'Lindenwood', 'Long Island', 'Stonehill']\n",
    "}\n",
    "\n",
    "# Function to get the conference of a team\n",
    "def get_conference(team):\n",
    "    for conference, teams in conference_teams.items():\n",
    "        if team in teams:\n",
    "            return conference\n",
    "    return 'Unknown'  # For teams not in the provided lists\n",
    "\n",
    "# Add columns for conference of both the away and home teams\n",
    "schedule_df['Away_Conference'] = schedule_df['Away_Team'].apply(get_conference)\n",
    "schedule_df['Home_Conference'] = schedule_df['Home_Team'].apply(get_conference)\n",
    "\n",
    "# Drop rows with Unknown conferences - Stonehill and Long Island annonmaly\n",
    "schedule_df = schedule_df[schedule_df['Away_Conference'] != 'Unknown']\n",
    "schedule_df = schedule_df[schedule_df['Home_Conference'] != 'Unknown']\n",
    "\n",
    "# Rename to completed_games_df\n",
    "completed_games_df = schedule_df\n",
    "\n",
    "# Matrix for away team wins\n",
    "away_wins_matrix = pd.crosstab(index=completed_games_df['Away_Conference'],\n",
    "                               columns=completed_games_df['Home_Conference'],\n",
    "                               values=(completed_games_df['Away_Score'] > completed_games_df['Home_Score']).astype(int),\n",
    "                               aggfunc='sum', dropna=False)\n",
    "\n",
    "# Matrix for home team wins\n",
    "home_wins_matrix = pd.crosstab(index=completed_games_df['Home_Conference'],\n",
    "                               columns=completed_games_df['Away_Conference'],\n",
    "                               values=(completed_games_df['Home_Score'] > completed_games_df['Away_Score']).astype(int),\n",
    "                               aggfunc='sum', dropna=False)\n",
    "\n",
    "# Transpose the home wins matrix so that it aligns with the away wins matrix for summation\n",
    "home_wins_matrix = home_wins_matrix.T\n",
    "\n",
    "# Sum both matrices to get the total wins\n",
    "total_wins_matrix = away_wins_matrix.add(home_wins_matrix, fill_value=0)\n",
    "# total_wins_matrix = total_wins_matrix.astype(int) # Convert to integers\n",
    "\n",
    "\n",
    "# Display the results matrix\n",
    "print(total_wins_matrix)\n",
    "# calculate and print the total number of games played\n",
    "total_games = total_wins_matrix.sum().sum()\n",
    "print(f'Total games played: {total_games}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the team without a conference\n",
    "# # Display rows with 'Unknown' in either column\n",
    "# unknown_teams = schedule_df[(schedule_df['Away_Conference'] == 'Unknown') | (schedule_df['Home_Conference'] == 'Unknown')]\n",
    "# print(len(unknown_teams))  # Number of rows with unknown teams\n",
    "\n",
    "# # value count of unknown teams\n",
    "# print(unknown_teams['Away_Team'].value_counts())\n",
    "# print(unknown_teams['Home_Team'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join 2023 Player Stats to 2024 Rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "## Path to the data\n",
    "roster_path = os.path.join(\"..\", \"data\", \"roster_2024_current_v3.csv\")\n",
    "stat_path = os.path.join(\"..\", \"data\", \"player_stats_2023_v1.csv\")\n",
    "\n",
    "# Load the data\n",
    "roster_df = pd.read_csv(roster_path)\n",
    "stat_df = pd.read_csv(stat_path)\n",
    "\n",
    "# Check the data\n",
    "roster_df.head()\n",
    "# stat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split stats Clean_Player into first and last name\n",
    "stat_df['First_Name'] = stat_df['Clean_Player'].str.split(\" \").str[0]\n",
    "stat_df['Last_Name'] = stat_df['Clean_Player'].str.split(\" \").str[1:]\n",
    "\n",
    "\n",
    "stat_df['Last_Name'] = stat_df['Last_Name'].str[0].str.replace('[','').str.replace(']','') # Remove the brackets from the last name\n",
    "# Remove periods dashes ect from both names\n",
    "stat_df['First_Name'] = stat_df['First_Name'].str.replace('.','').str.replace('-',' ')\n",
    "stat_df['Last_Name'] = stat_df['Last_Name'].str.replace('.','').str.replace('-',' ')\n",
    "roster_df['First_Name'] = roster_df['First_Name'].str.replace('.','').str.replace('-',' ')\n",
    "roster_df['Last_Name'] = roster_df['Last_Name'].str.replace('.','').str.replace('-',' ')\n",
    "# strip white space\n",
    "stat_df['First_Name'] = stat_df['First_Name'].str.strip()\n",
    "stat_df['Last_Name'] = stat_df['Last_Name'].str.strip()\n",
    "roster_df['First_Name'] = roster_df['First_Name'].str.strip()\n",
    "roster_df['Last_Name'] = roster_df['Last_Name'].str.strip()\n",
    "\n",
    "# Rename Team to Team_2023 for clarity\n",
    "stat_df.rename(columns={'Team':'Team_2023'}, inplace=True)\n",
    "# Rename Current_Team to Team_2024 for clarity\n",
    "roster_df.rename(columns={'Current Team':'Team_2024'}, inplace=True)\n",
    "\n",
    "stat_df.head()\n",
    "# OUTPUT THE DATA TO TEMP CSVs\n",
    "roster_df.to_csv(os.path.join(\"..\", \"TEMP\", \"TEST_roster_2024_current_v4.csv\"), index=False)\n",
    "stat_df.to_csv(os.path.join(\"..\", \"TEMP\", \"TEST_player_stats_2023_v2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try a quick merge\n",
    "merged_df = pd.merge(roster_df, stat_df, left_on=['First_Name', 'Last_Name'], right_on=['First_Name', 'Last_Name'], how='outer', suffixes=('_2024', '_2023'))\n",
    "merged_df.head()\n",
    "\n",
    "# Print report of the merge\n",
    "print(f\"Number of players in the roster: {len(roster_df)}\")\n",
    "print(f\"Number of players in the stats: {len(stat_df)}\")\n",
    "print(f\"Number of players in the merged data: {len(merged_df)}\")\n",
    "\n",
    "\n",
    "# Find Number Number of players whos Team_2023 does not match Team_2024\n",
    "mismatched_teams = merged_df[merged_df['Team_2023'] != merged_df['Team_2024']]\n",
    "print(f\"Number of players with mismatched teams: {len(mismatched_teams)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(merged_df.info())\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop players who aren't playing this year (No Team_2024)\n",
    "merged_df = merged_df.dropna(subset=['Team_2024'])\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert all number columns to int\n",
    "int_columns = ['No', 'Height_Inches', 'Wt', 'Draft_Year', 'D_Round', \n",
    "               'G', 'A', 'Pts', 'plus_minus', 'Sh', 'PIM', 'Games_Played']\n",
    "\n",
    "for col in int_columns:\n",
    "    merged_df[col] = merged_df[col].astype('Int64')\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUT CSV TO TEMP FOR INSPECTION\n",
    "merged_df.to_csv(os.path.join(\"..\", \"data\", \"roster_2024_with_2023_stats.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform congressional demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import geopandas as gpd\n",
    "\n",
    "\n",
    "# ## PATHS ##\n",
    "# # ## 118 Congress Shapefile\n",
    "# # shape_path = os.path.join('..', 'data', 'vault', '118th_congress', 'USA_118th_Congressional_Districts.shp')\n",
    "# # ## Load Shapefile\n",
    "# # gdf = gpd.read_file(shape_path)\n",
    "\n",
    "# # Income data table - 5 Year Average 2022\n",
    "# income_path = os.path.join('..', 'data', 'vault', '118th_congress', 'income_data', 'ACSST5Y2022.S1903-Data.csv')\n",
    "# income_df = pd.read_csv(income_path, skiprows=1) # Load Income Data\n",
    "\n",
    "# # Summary table with Populations and Representative Names\n",
    "# summary_path = os.path.join('..', 'data', 'vault', 'USA_118th_Congressional_Districts_info_table.csv')\n",
    "# summary_df = pd.read_csv(summary_path)\n",
    "\n",
    "# # Check \n",
    "# # gdf.head()\n",
    "# # income_df.head()\n",
    "# # summary_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check \n",
    "# gdf.head()\n",
    "# income_df.head()\n",
    "# # summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate Image icons so they are all 300 x 300 px squares\n",
    "- making sure they are all squares will make resizing issues easier later on\n",
    "    - The aspect ratio is getting screwed up during resizing for icons that are not square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "\n",
    "# Directory where the logos are stored\n",
    "logo_dir = os.path.join('..', 'images', 'logos')\n",
    "\n",
    "# Make logos square by adding transparent space equally on both sides\n",
    "for logo_file in os.listdir(logo_dir):\n",
    "    logo_path = os.path.join(logo_dir, logo_file)\n",
    "    \n",
    "    # Check if the path is a file and not a directory\n",
    "    if os.path.isfile(logo_path):\n",
    "        with Image.open(logo_path) as img:\n",
    "            # Ensure the image has an alpha channel (for transparency)\n",
    "            img = img.convert(\"RGBA\")\n",
    "            \n",
    "            width, height = img.size\n",
    "            \n",
    "            # If the image is already square, no changes are needed\n",
    "            if width == height:\n",
    "                continue\n",
    "            \n",
    "            # Calculate padding to add on the shorter side to make the image square\n",
    "            if width > height:\n",
    "                padding = (width - height) // 2\n",
    "                new_img = ImageOps.expand(img, border=(0, padding, 0, padding), fill=(0, 0, 0, 0))\n",
    "            else:\n",
    "                padding = (height - width) // 2\n",
    "                new_img = ImageOps.expand(img, border=(padding, 0, padding, 0), fill=(0, 0, 0, 0))\n",
    "            \n",
    "            # Save the padded square image, overwriting the original\n",
    "            new_img.save(logo_path)\n",
    "\n",
    "print(\"All logos made square by adding transparent space equally to each side.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the coordinates for the rinks in arena_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dependencies\n",
    "# import os\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Path to arena file\n",
    "# arena_file = os.path.join('..','data', 'arena_school_info.csv')\n",
    "# arena_df = pd.read_csv(arena_file)\n",
    "\n",
    "# # Open Roster File To Clean State/Provences Names\n",
    "# roster_file = os.path.join('..','data', 'roster_2024_current_v2.csv')\n",
    "# roster_df = pd.read_csv(roster_file)\n",
    "\n",
    "# roster_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get list of Unique State/Province Names\n",
    "# unique_states = roster_df['State_Province'].unique()\n",
    "# unique_states\n",
    "\n",
    "# ## Dictionary to standardize state/province names\n",
    "\n",
    "# standardized_locations = {\n",
    "#     'Ont.': 'Ontario', 'Mich.': 'Michigan', 'Mass.': 'Massachusetts', 'Minn.': 'Minnesota', \n",
    "#     'Wis.': 'Wisconsin', 'Sweden': 'Sweden', 'Germany': 'Germany', 'B.C.': 'British Columbia',\n",
    "#     'N.Y.': 'New York', 'Wash.': 'Washington', 'Que.': 'Quebec', 'Alb.': 'Alberta', \n",
    "#     'N.J.': 'New Jersey', 'Sask.': 'Saskatchewan', 'Conn.': 'Connecticut', 'Mo.': 'Missouri',\n",
    "#     'Texas': 'Texas', 'Calif.': 'California', 'DC': 'District of Columbia', 'Fla.': 'Florida',\n",
    "#     'Ohio': 'Ohio', 'Ill.': 'Illinois', 'Pa.': 'Pennsylvania', 'Ga.': 'Georgia',\n",
    "#     'Mont.': 'Montana', 'Tenn.': 'Tennessee', 'Colo.': 'Colorado', 'Va.': 'Virginia', \n",
    "#     'Vt.': 'Vermont', 'R.I.': 'Rhode Island', 'Md.': 'Maryland', 'Ariz.': 'Arizona', \n",
    "#     'Wisc.': 'Wisconsin', 'Iowa': 'Iowa', 'Man.': 'Manitoba', 'Slovakia': 'Slovakia', \n",
    "#     'N.D.': 'North Dakota', 'N.C.': 'North Carolina', 'P.E.I.': 'Prince Edward Island',\n",
    "#     'N.H.': 'New Hampshire', 'Alaska': 'Alaska', 'Belarus': 'Belarus', 'MB': 'Manitoba',\n",
    "#     'Russia': 'Russia', 'Finland': 'Finland', 'Newf.': 'Newfoundland and Labrador', \n",
    "#     'Hungary': 'Hungary', 'SUI': 'Switzerland', 'S.C.': 'South Carolina', 'Latvia': 'Latvia',\n",
    "#     'Czech Republic': 'Czech Republic', 'N.B.': 'New Brunswick', 'Great Britain': 'United Kingdom', \n",
    "#     'NB': 'New Brunswick', 'Norway': 'Norway', 'N.S.': 'Nova Scotia', 'Ind.': 'Indiana', \n",
    "#     'NWT': 'Northwest Territories', 'AUT': 'Austria', 'Idaho': 'Idaho', 'S.D.': 'South Dakota', \n",
    "#     'Switzerland': 'Switzerland', 'Ore.': 'Oregon', 'Wyo.': 'Wyoming', 'Utah': 'Utah', \n",
    "#     'ITA': 'Italy', 'Slovenia': 'Slovenia', 'YT': 'Yukon', 'Del.': 'Delaware', 'Maine': 'Maine',\n",
    "#     'Poland': 'Poland', 'Yukon': 'Yukon', 'Ukraine': 'Ukraine', 'Japan': 'Japan', 'Neb.': 'Nebraska'\n",
    "# }\n",
    "\n",
    "# ## Apply the standardization to the State/Province column\n",
    "# roster_df['State_Province'] = roster_df['State_Province'].replace(standardized_locations)\n",
    "\n",
    "# # Check the unique values after standardization\n",
    "# roster_df['State_Province'].unique()\n",
    "# print(roster_df['State_Province'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output the cleaned roster to a new CSV file\n",
    "cleaned_roster_file = os.path.join('..','data', 'roster_cleaned_state_prov_2024.csv')\n",
    "roster_df.to_csv(cleaned_roster_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the function to check the location using Google Places API\n",
    "# def check_location(lat, lng, api_key):\n",
    "#     url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "#     params = {\n",
    "#         'location': f'{lat},{lng}',\n",
    "#         'radius': 500,  # Distance in meters from the provided coordinates\n",
    "#         'type': 'stadium',  # Filter search to stadiums/arenas\n",
    "#         'key': api_key\n",
    "#     }\n",
    "    \n",
    "#     # Debugging: Print the URL and parameters\n",
    "#     print(f\"Requesting URL: {url}\")\n",
    "#     print(f\"Parameters: {params}\")\n",
    "    \n",
    "#     response = requests.get(url, params=params)\n",
    "    \n",
    "#     # Debugging: Print the response code and content\n",
    "#     print(f\"Response status code: {response.status_code}\")\n",
    "#     print(f\"Response content: {response.text}\")\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         results = response.json().get('results')\n",
    "#         if results:\n",
    "#             return results[0].get('name'), results[0].get('vicinity')\n",
    "#         else:\n",
    "#             return None, \"No results found\"\n",
    "#     else:\n",
    "#         return None, f\"API request failed with status {response.status_code}\"\n",
    "\n",
    "# # Define the function to verify coordinates in the DataFrame\n",
    "# def verify_coordinates(df, api_key):\n",
    "#     results = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         lat = row['Latitude']\n",
    "#         lng = row['Longitude']\n",
    "#         arena_name = row['Arena']\n",
    "        \n",
    "#         # Debugging: Print the current coordinates and arena being checked\n",
    "#         print(f\"Checking coordinates for arena: {arena_name}\")\n",
    "#         print(f\"Latitude: {lat}, Longitude: {lng}\")\n",
    "        \n",
    "#         # Get the name and vicinity of the nearest stadium/arena\n",
    "#         name, vicinity = check_location(lat, lng, api_key)\n",
    "        \n",
    "#         # Append the original data and verification results\n",
    "#         results.append({\n",
    "#             'Arena': arena_name,\n",
    "#             'Latitude': lat,\n",
    "#             'Longitude': lng,\n",
    "#             'Google Places Name': name,\n",
    "#             'Vicinity': vicinity\n",
    "#         })\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# # Load your API key\n",
    "\n",
    "\n",
    "# # Assuming arena_df is your DataFrame\n",
    "# verified_df = verify_coordinates(arena_df, api_key)\n",
    "\n",
    "# # Output the results\n",
    "# print(verified_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verified_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Version 2 of Arena Location verifications\n",
    "# ## Returns 5 closest Google Places to coordinates given\n",
    "\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the function to check the 5 closest places using Google Places API\n",
    "# def check_nearby_places(lat, lng, api_key):\n",
    "#     url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "#     params = {\n",
    "#         'location': f'{lat},{lng}',\n",
    "#         'radius': 500,  # Distance in meters from the provided coordinates\n",
    "#         'key': api_key\n",
    "#     }\n",
    "    \n",
    "#     # Debugging: Print the URL and parameters being sent to the API\n",
    "#     print(f\"Requesting places near lat: {lat}, lng: {lng}\")\n",
    "#     print(f\"Request URL: {url}\")\n",
    "#     print(f\"Parameters: {params}\")\n",
    "    \n",
    "#     response = requests.get(url, params=params)\n",
    "    \n",
    "#     # Debugging: Print the response status and content\n",
    "#     print(f\"Response status code: {response.status_code}\")\n",
    "#     print(f\"Response content: {response.text}\\n\")  # This shows the full response from the API\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         results = response.json().get('results')\n",
    "#         if results:\n",
    "#             # Return the top 5 closest places\n",
    "#             return [(result.get('name'), result.get('vicinity')) for result in results[:5]]\n",
    "#         else:\n",
    "#             return [(\"None\", \"No results found\")]\n",
    "#     else:\n",
    "#         return [(\"None\", f\"API request failed with status {response.status_code}\")]\n",
    "\n",
    "# # Define the function to verify coordinates and return the 5 closest places\n",
    "# def verify_coordinates(df, api_key):\n",
    "#     results = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         lat = row['Latitude']\n",
    "#         lng = row['Longitude']\n",
    "#         arena_name = row['Arena']\n",
    "#         school_name = row['School']\n",
    "        \n",
    "#         # Debugging: Print the current arena and coordinates being checked\n",
    "#         print(f\"\\nChecking nearby places for arena: {arena_name} (School: {school_name})\")\n",
    "#         print(f\"Latitude: {lat}, Longitude: {lng}\")\n",
    "        \n",
    "#         # Get the 5 closest places\n",
    "#         nearby_places = check_nearby_places(lat, lng, api_key)\n",
    "        \n",
    "#         # Add each place to the results, along with the original data\n",
    "#         for place in nearby_places:\n",
    "#             results.append({\n",
    "#                 'Arena': arena_name,\n",
    "#                 'School': school_name,\n",
    "#                 'Latitude': lat,\n",
    "#                 'Longitude': lng,\n",
    "#                 'Google Places Name': place[0],\n",
    "#                 'Vicinity': place[1]\n",
    "#             })\n",
    "            \n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# # Load your API key\n",
    "# api_key = ''\n",
    "\n",
    "# # Assuming arena_df is your DataFrame\n",
    "# verified_df = verify_coordinates(arena_df, api_key)\n",
    "\n",
    "# # Output the results\n",
    "# print(verified_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_df.head(10)\n",
    "\n",
    "## OUTPUT TO TEMP FOLDER FOR MANUAL REVIEW\n",
    "# output_file = os.path.join('..','TEMP', 'arena_school_info_place_checkV3.csv')\n",
    "# verified_df.to_csv(output_file, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
