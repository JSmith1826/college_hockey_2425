{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRATCH BOOK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Period   Time Team  \\\n",
      "0  Start of 1st  20:00        \n",
      "1  Start of 1st  20:00        \n",
      "2  Start of 1st  20:00        \n",
      "3  Start of 1st  19:27        \n",
      "4  Start of 1st  17:56        \n",
      "\n",
      "                                         Description Score  \n",
      "0  Faceoff Shoudy, Tiernan vs Draper, Kienan won ...        \n",
      "1                 Logan Stein at goalie for Michigan        \n",
      "2          Trey Augustine at goalie for Michigan St.        \n",
      "3  Shot by MICHST Mãnnistã, Tommi MISSED, save ...        \n",
      "4  Goal by Dorwart, Karsen (EVEN STRENGTH, FIRST ...   1-0  \n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_play_by_play_table(url):\n",
    "    \"\"\"\n",
    "    Scrape the play-by-play table from the given NCAA game URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the NCAA game play-by-play page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the play-by-play data.\n",
    "    \"\"\"\n",
    "    # Set up Selenium WebDriver (update path to your ChromeDriver if needed)\n",
    "    # service = Service(executable_path='chromedriver')  # Adjust 'chromedriver' path\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    try:\n",
    "        # Load the page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the page to load (adjust timeout if necessary)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'play-by-play-period'))\n",
    "        )\n",
    "\n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Locate all periods\n",
    "        period_divs = soup.find_all('div', {'class': 'play-by-play-period'})\n",
    "\n",
    "        # Initialize an empty list to store all data\n",
    "        all_data = []\n",
    "\n",
    "        # Iterate through each period\n",
    "        for period_div in period_divs:\n",
    "            # Extract period header (e.g., \"Start of 1st\")\n",
    "            period_header = period_div.find('h3').get_text(strip=True)\n",
    "\n",
    "            # Find the table inside the period div\n",
    "            pbp_table = period_div.find('div', {'class': 'play-by-play-period-table'}).find('table')\n",
    "\n",
    "            # Extract rows from the table\n",
    "            for row in pbp_table.find('tbody').find_all('tr'):\n",
    "                cells = row.find_all('td')\n",
    "                if len(cells) == 4:  # Ensure the row has all expected columns\n",
    "                    time = cells[0].get_text(strip=True)\n",
    "                    team = cells[1].get_text(strip=True)\n",
    "                    description = cells[2].get_text(strip=True)\n",
    "                    score = cells[3].get_text(strip=True)\n",
    "                    all_data.append([period_header, time, team, description, score])\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(all_data, columns=['Period', 'Time', 'Team', 'Description', 'Score'])\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n",
    "    finally:\n",
    "        # Close the Selenium WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.ncaa.com/game/6348408/play-by-play\"\n",
    "    play_by_play_df = fetch_play_by_play_table(url)\n",
    "    if not play_by_play_df.empty:\n",
    "        print(play_by_play_df.head())\n",
    "    else:\n",
    "        print(\"No data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play_by_play_df.head(10)\n",
    "# play_by_play_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning of the Raw PbP table\n",
    "-TO DO: replace values in Period column to simple identifiers (1,2,3,OT)\n",
    "    -'Start of 1st' replace with '1', ect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Time</th>\n",
       "      <th>Team</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>OT</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>at goalie for Michigan</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>OT</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Trey Augustine at goalie for Michigan St.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>OT</td>\n",
       "      <td>76</td>\n",
       "      <td></td>\n",
       "      <td>Shot by MICH Hage, Michael MISSED, save August...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>OT</td>\n",
       "      <td>76</td>\n",
       "      <td></td>\n",
       "      <td>Faceoff Hughes, T.j. vs Shoudy, Tiernan won by...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>OT</td>\n",
       "      <td>60</td>\n",
       "      <td></td>\n",
       "      <td>Goal by Schifsky, Garrett (EVEN STRENGTH, OVER...</td>\n",
       "      <td>2-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Period  Time Team                                        Description Score\n",
       "206     OT     0                                  at goalie for Michigan      \n",
       "207     OT     0               Trey Augustine at goalie for Michigan St.      \n",
       "208     OT    76       Shot by MICH Hage, Michael MISSED, save August...      \n",
       "209     OT    76       Faceoff Hughes, T.j. vs Shoudy, Tiernan won by...      \n",
       "210     OT    60       Goal by Schifsky, Garrett (EVEN STRENGTH, OVER...   2-3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename to df for simplicity\n",
    "df = play_by_play_df\n",
    "\n",
    "#TO DO: replace values in Period column to simple identifiers (1,2,3,OT)\n",
    "df['Period'] = df['Period'].replace({\n",
    "    'Start of 1st': '1',\n",
    "    'Start of 2nd': '2',\n",
    "    'Start of 3rd': '3',\n",
    "    'Start of OT': 'OT'\n",
    "})\n",
    "\n",
    "# To DO : Replace the values in the Time column - convert to seconds since the beginning of the game\n",
    "# If period is 1, 20:00 is now 0 and 19:59 is 1, so on and so forth\n",
    "# If period is 2, 20:00 is now 1200 and 19:59 is 1201, so on and so forth\n",
    "\n",
    "def convert_time_to_seconds(time_str, period):\n",
    "    if period == '1' or period == '2':\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        total_seconds = (20 - minutes) * 60 + seconds\n",
    "    elif period == '3':\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        total_seconds = (20 - minutes) * 60 + seconds\n",
    "    elif period == 'OT':\n",
    "        minutes, seconds = map(int, time_str.split(':'))\n",
    "        total_seconds = (5 - minutes) * 60 + seconds\n",
    "    else:\n",
    "        total_seconds = None  # Handle unexpected periods\n",
    "    return total_seconds\n",
    "df['Time'] = df.apply(lambda x: convert_time_to_seconds(x['Time'], x['Period']), axis=1)\n",
    "\n",
    "df.tail()\n",
    "\n",
    "# def convert_time_to_seconds(time_str, period):\n",
    "#     if period == '1' or period == '2':\n",
    "#         minutes, seconds = map(int, time_str.split(':'))\n",
    "#         total_seconds = (20 - minutes) * 60 + seconds\n",
    "#     elif period == '3':\n",
    "#         minutes, seconds = map(int, time_str.split(':'))\n",
    "#         total_seconds = (20 - minutes) * 60 + seconds\n",
    "#     elif period == 'OT':\n",
    "#         minutes, seconds = map(int, time_str.split(':'))\n",
    "#         total_seconds = (5 - minutes) * 60 + seconds\n",
    "#     else:\n",
    "#         total_seconds = None  # Handle unexpected periods\n",
    "#     return total_seconds\n",
    "# df['Time'] = df.apply(lambda x: convert_time_to_seconds(x['Time'], x['Period']), axis=1)\n",
    "\n",
    "df.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Time</th>\n",
       "      <th>Team</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Faceoff Shoudy, Tiernan vs Draper, Kienan won ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Logan Stein at goalie for Michigan</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Trey Augustine at goalie for Michigan St.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td></td>\n",
       "      <td>Shot by MICHST Mãnnistã, Tommi MISSED, save ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "      <td></td>\n",
       "      <td>Goal by Dorwart, Karsen (EVEN STRENGTH, FIRST ...</td>\n",
       "      <td>1-0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Period  Time Team                                        Description Score\n",
       "0      1     0       Faceoff Shoudy, Tiernan vs Draper, Kienan won ...      \n",
       "1      1     0                      Logan Stein at goalie for Michigan      \n",
       "2      1     0               Trey Augustine at goalie for Michigan St.      \n",
       "3      1    87       Shot by MICHST Mãnnistã, Tommi MISSED, save ...      \n",
       "4      1   236       Goal by Dorwart, Karsen (EVEN STRENGTH, FIRST ...   1-0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Time</th>\n",
       "      <th>Team</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "      <th>Event_type</th>\n",
       "      <th>Primary_player</th>\n",
       "      <th>Primary_team</th>\n",
       "      <th>Secondary_event</th>\n",
       "      <th>Secondary_player</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Faceoff Shoudy, Tiernan vs Draper, Kienan won ...</td>\n",
       "      <td></td>\n",
       "      <td>Faceoff</td>\n",
       "      <td>Shoudy, Tiernan</td>\n",
       "      <td>MICHST</td>\n",
       "      <td>None</td>\n",
       "      <td>Draper, Kienan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Logan Stein at goalie for Michigan</td>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Trey Augustine at goalie for Michigan St.</td>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td></td>\n",
       "      <td>Shot by MICHST Mãnnistã, Tommi MISSED, save ...</td>\n",
       "      <td></td>\n",
       "      <td>Shot-Missed</td>\n",
       "      <td>Stein, Logan</td>\n",
       "      <td>MICHST</td>\n",
       "      <td>Missed</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>236</td>\n",
       "      <td></td>\n",
       "      <td>Goal by Dorwart, Karsen (EVEN STRENGTH, FIRST ...</td>\n",
       "      <td>1-0</td>\n",
       "      <td>Goal</td>\n",
       "      <td>Dorwart, Karsen</td>\n",
       "      <td>MICH</td>\n",
       "      <td>Assist</td>\n",
       "      <td>Larson, Joey</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Period  Time Team                                        Description Score  \\\n",
       "0      1     0       Faceoff Shoudy, Tiernan vs Draper, Kienan won ...         \n",
       "1      1     0                      Logan Stein at goalie for Michigan         \n",
       "2      1     0               Trey Augustine at goalie for Michigan St.         \n",
       "3      1    87       Shot by MICHST Mãnnistã, Tommi MISSED, save ...         \n",
       "4      1   236       Goal by Dorwart, Karsen (EVEN STRENGTH, FIRST ...   1-0   \n",
       "\n",
       "    Event_type   Primary_player Primary_team Secondary_event Secondary_player  \n",
       "0      Faceoff  Shoudy, Tiernan       MICHST            None   Draper, Kienan  \n",
       "1        Other             None         None            None             None  \n",
       "2        Other             None         None            None             None  \n",
       "3  Shot-Missed     Stein, Logan       MICHST          Missed             None  \n",
       "4         Goal  Dorwart, Karsen         MICH          Assist     Larson, Joey  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_description(description):\n",
    "    \"\"\"\n",
    "    Parse a single play-by-play description into structured fields.\n",
    "\n",
    "    Args:\n",
    "        description (str): The play-by-play description.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed fields (Event_type, Primary_player, Primary_team, Secondary_event, Secondary_player).\n",
    "    \"\"\"\n",
    "    # Initialize default values\n",
    "    parsed = {\n",
    "        \"Event_type\": \"Other\",\n",
    "        \"Primary_player\": None,\n",
    "        \"Primary_team\": None,\n",
    "        \"Secondary_event\": None,\n",
    "        \"Secondary_player\": None,\n",
    "    }\n",
    "\n",
    "    # Detect event type\n",
    "    if \"Faceoff\" in description:\n",
    "        parsed[\"Event_type\"] = \"Faceoff\"\n",
    "    elif \"Shot\" in description:\n",
    "        if \"MISSED\" in description:\n",
    "            parsed[\"Event_type\"] = \"Shot-Missed\"\n",
    "        elif \"BLOCKED\" in description:\n",
    "            parsed[\"Event_type\"] = \"Shot-Blocked\"\n",
    "        elif \"WIDE\" in description:\n",
    "            parsed[\"Event_type\"] = \"Shot-Wide\"\n",
    "    elif \"Goal\" in description:\n",
    "        parsed[\"Event_type\"] = \"Goal\"\n",
    "    elif \"Penalty\" in description:\n",
    "        parsed[\"Event_type\"] = \"Penalty\"\n",
    "    elif \"Start power play\" in description:\n",
    "        parsed[\"Event_type\"] = \"Start Power Play\"\n",
    "    elif \"End power play\" in description:\n",
    "        parsed[\"Event_type\"] = \"End Power Play\"\n",
    "\n",
    "    # Extract primary player\n",
    "    match = re.search(r'([A-Z][a-z]+, [A-Z][a-z]+)', description)\n",
    "    if match:\n",
    "        parsed[\"Primary_player\"] = match.group(1)\n",
    "\n",
    "    # Extract primary team\n",
    "    team_match = re.search(r'\\b(MICH|MICHST)\\b', description)\n",
    "    if team_match:\n",
    "        parsed[\"Primary_team\"] = team_match.group(1)\n",
    "\n",
    "    # Extract secondary event\n",
    "    if \"BLOCKED\" in description:\n",
    "        parsed[\"Secondary_event\"] = \"Blocked\"\n",
    "    elif \"WIDE\" in description:\n",
    "        parsed[\"Secondary_event\"] = \"Wide\"\n",
    "    elif \"MISSED\" in description:\n",
    "        parsed[\"Secondary_event\"] = \"Missed\"\n",
    "    elif \"Assist\" in description:\n",
    "        parsed[\"Secondary_event\"] = \"Assist\"\n",
    "\n",
    "    # Extract secondary player\n",
    "    secondary_match = re.findall(r'([A-Z][a-z]+, [A-Z][a-z]+)', description)\n",
    "    if len(secondary_match) > 1:\n",
    "        parsed[\"Secondary_player\"] = secondary_match[1]\n",
    "\n",
    "    return parsed\n",
    "\n",
    "def transform_pbp_descriptions(descriptions):\n",
    "    \"\"\"\n",
    "    Transform a list of play-by-play descriptions into a structured DataFrame.\n",
    "\n",
    "    Args:\n",
    "        descriptions (list): List of play-by-play descriptions.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Transformed DataFrame with parsed fields.\n",
    "    \"\"\"\n",
    "    # Parse each description\n",
    "    parsed_data = [parse_description(desc) for desc in descriptions]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    return pd.DataFrame(parsed_data)\n",
    "\n",
    "# call the function\n",
    "transformed_df = transform_pbp_descriptions(df['Description'])\n",
    "\n",
    "# add the transformed data to the original DataFrame\n",
    "df = pd.concat([df, transformed_df], axis=1)\n",
    "\n",
    "# transformed_df.head(10)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Time</th>\n",
       "      <th>Team</th>\n",
       "      <th>Description</th>\n",
       "      <th>Score</th>\n",
       "      <th>Event_type</th>\n",
       "      <th>Primary_player</th>\n",
       "      <th>Primary_team</th>\n",
       "      <th>Secondary_event</th>\n",
       "      <th>Secondary_player</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>OT</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>at goalie for Michigan</td>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>OT</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>Trey Augustine at goalie for Michigan St.</td>\n",
       "      <td></td>\n",
       "      <td>Other</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>OT</td>\n",
       "      <td>76</td>\n",
       "      <td></td>\n",
       "      <td>Shot by MICH Hage, Michael MISSED, save August...</td>\n",
       "      <td></td>\n",
       "      <td>Shot-Missed</td>\n",
       "      <td>Hage, Michael</td>\n",
       "      <td>MICH</td>\n",
       "      <td>Missed</td>\n",
       "      <td>Augustine, Trey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>OT</td>\n",
       "      <td>76</td>\n",
       "      <td></td>\n",
       "      <td>Faceoff Hughes, T.j. vs Shoudy, Tiernan won by...</td>\n",
       "      <td></td>\n",
       "      <td>Faceoff</td>\n",
       "      <td>Shoudy, Tiernan</td>\n",
       "      <td>MICH</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>OT</td>\n",
       "      <td>60</td>\n",
       "      <td></td>\n",
       "      <td>Goal by Schifsky, Garrett (EVEN STRENGTH, OVER...</td>\n",
       "      <td>2-3</td>\n",
       "      <td>Goal</td>\n",
       "      <td>Schifsky, Garrett</td>\n",
       "      <td>MICH</td>\n",
       "      <td>Assist</td>\n",
       "      <td>Truscott, Jacob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Period  Time Team                                        Description  \\\n",
       "206     OT     0                                  at goalie for Michigan   \n",
       "207     OT     0               Trey Augustine at goalie for Michigan St.   \n",
       "208     OT    76       Shot by MICH Hage, Michael MISSED, save August...   \n",
       "209     OT    76       Faceoff Hughes, T.j. vs Shoudy, Tiernan won by...   \n",
       "210     OT    60       Goal by Schifsky, Garrett (EVEN STRENGTH, OVER...   \n",
       "\n",
       "    Score   Event_type     Primary_player Primary_team Secondary_event  \\\n",
       "206              Other               None         None            None   \n",
       "207              Other               None         None            None   \n",
       "208        Shot-Missed      Hage, Michael         MICH          Missed   \n",
       "209            Faceoff    Shoudy, Tiernan         MICH            None   \n",
       "210   2-3         Goal  Schifsky, Garrett         MICH          Assist   \n",
       "\n",
       "    Secondary_player  \n",
       "206             None  \n",
       "207             None  \n",
       "208  Augustine, Trey  \n",
       "209             None  \n",
       "210  Truscott, Jacob  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a list of every value in the description column\n",
    "raw_list = df['Description'].unique()\n",
    "raw_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# url = 'https://www.ncaa.com/game/6344804/play-by-play'\n",
    "\n",
    "# # Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Make sure you have the ChromeDriver installed\n",
    "# # driver.get(url)\n",
    "\n",
    "def fetch_play_by_play_table(url):\n",
    "    \"\"\"\n",
    "    Scrape the play-by-play table from the given NCAA game URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): URL of the NCAA game play-by-play page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the play-by-play data.\n",
    "    \"\"\"\n",
    "    # Set up Selenium WebDriver (adjust the path to your ChromeDriver)\n",
    "    # service = Service(executable_path='chromedriver')  # Update 'chromedriver' path if necessary\n",
    "    # driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    try:\n",
    "        # Load the page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the table to load (adjust timeout if needed)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, 'gamecenter-tab-play-by-play'))\n",
    "        )\n",
    "\n",
    "        # Get page source and parse it with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Find the play-by-play table\n",
    "        pbp_table = soup.find('table', {'class': 'play-by-play-period-table'})\n",
    "\n",
    "        # Check if the table was found\n",
    "        if pbp_table is None:\n",
    "            raise ValueError(\"Play-by-play table not found on the page.\")\n",
    "\n",
    "        # Extract rows from the table\n",
    "        data = []\n",
    "        for row in pbp_table.find_all('tr'):\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) > 1:  # Skip header rows\n",
    "                time = cells[0].get_text(strip=True)\n",
    "                team = cells[1].get_text(strip=True)\n",
    "                description = cells[2].get_text(strip=True)\n",
    "                data.append([time, team, description])\n",
    "\n",
    "        # Create a DataFrame\n",
    "        df = pd.DataFrame(data, columns=['Time', 'Team', 'Description'])\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n",
    "    finally:\n",
    "        # Close the Selenium WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.ncaa.com/game/6344804/play-by-play\"\n",
    "    play_by_play_df = fetch_play_by_play_table(url)\n",
    "    if not play_by_play_df.empty:\n",
    "        print(play_by_play_df.head())\n",
    "    else:\n",
    "        print(\"No data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.ncaa.com/game/6344804/play-by-play'\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    page_content = response.text\n",
    "else:\n",
    "    raise Exception(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "# print the soup to debug\n",
    "print(soup.prettify())\n",
    "\n",
    "pbp_table = soup.find('table', {'class': 'gamecenter-tab-play-by-play'})\n",
    "\n",
    "data = []\n",
    "for row in pbp_table.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:  # Ensure it's not a header row\n",
    "        time = cells[0].get_text(strip=True)\n",
    "        team = cells[1].get_text(strip=True)\n",
    "        description = cells[2].get_text(strip=True)\n",
    "        data.append([time, team, description])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Time', 'Team', 'Description'])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Make sure you have the ChromeDriver installed\n",
    "driver.get(url)\n",
    "\n",
    "# Let the page load completely\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "driver.quit()\n",
    "\n",
    "# Continue processing the soup object as before\n",
    "pbp_table = soup.find('table', {'class': 'gamecenter-tab-play-by-play'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pbp_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# async def get_url_response(url):\n",
    "#     \"\"\"\n",
    "#     Fetch and render the HTML content from the given URL.\n",
    "#     \"\"\"\n",
    "#     session = AsyncHTMLSession()\n",
    "#     response = await session.get(url)\n",
    "#     await response.html.arender()  # Asynchronously render JavaScript\n",
    "#     return response.html.html\n",
    "\n",
    "# def parse_html_to_table(html_content):\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "#     rows = []\n",
    "#     current_period = None\n",
    "\n",
    "#     # Debug: Check if play-by-play periods exist\n",
    "#     periods = soup.find_all('div', class_='play-by-play-period')\n",
    "#     print(f\"Periods found: {len(periods)}\")\n",
    "    \n",
    "#     for period in periods:\n",
    "#         # Get the current period from the header\n",
    "#         header = period.find('h3')\n",
    "#         if header:\n",
    "#             current_period = header.text.strip()\n",
    "#             print(f\"Current Period: {current_period}\")\n",
    "\n",
    "#         # Extract play rows\n",
    "#         play_rows = period.find_all('tr')\n",
    "#         print(f\"Rows found in this period: {len(play_rows)}\")\n",
    "#         for play_row in play_rows:\n",
    "#             columns = play_row.find_all('td')\n",
    "#             if len(columns) < 3:  # Skip invalid rows\n",
    "#                 continue\n",
    "\n",
    "#             # Parse columns\n",
    "#             time = columns[0].text.strip()\n",
    "#             team = columns[1].find('img')['alt'].strip() if columns[1].find('img') else None\n",
    "#             description = columns[2].text.strip()\n",
    "#             score = columns[3].text.strip() if len(columns) > 3 else None\n",
    "\n",
    "#             # Convert time to seconds\n",
    "#             if ':' in time:\n",
    "#                 minutes, seconds = map(int, time.split(':'))\n",
    "#                 game_time_seconds = minutes * 60 + seconds\n",
    "#             else:\n",
    "#                 game_time_seconds = None\n",
    "\n",
    "#             # Append the data\n",
    "#             rows.append({\n",
    "#                 'Game Time (Seconds)': game_time_seconds,\n",
    "#                 'Period': current_period,\n",
    "#                 'Team': team,\n",
    "#                 'Action Description': description,\n",
    "#                 'Score': score,\n",
    "#             })\n",
    "\n",
    "#     # Convert to DataFrame\n",
    "#     return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# url = 'https://www.ncaa.com/game/6344804/play-by-play'\n",
    "\n",
    "# html_content = get_url_response(url)\n",
    "# df = parse_html_to_table(html_content)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Time On Ice\n",
    "- Still not in it's own notebook 0 1-23-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "# Basics\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File Paths\n",
    "folder_prefix = ''\n",
    "# folder_prefix = '..'\n",
    "data_folder = os.path.join(folder_prefix, '..', 'data/') # Data Folder Path\n",
    "temp_folder = os.path.join(folder_prefix,'..', 'TEMP/',) # Temp Folder Path\n",
    "TEMP_FOLDER = temp_folder # Temp Folder Path as used in legacy code\n",
    "output_folder = os.path.join(temp_folder, 'team_comp_output/') # Output Folder Path\n",
    "# data\\db\\2024_Dec_10_CLEANED_OLD_METHOD.db\n",
    "db_path = os.path.join(data_folder, 'db', '2025_Jan_19_CLEAN.db') # Database Path\n",
    "\n",
    "image_folder = os.path.join(folder_prefix, '..', 'images/') # Image Folder Path\n",
    "logo_folder = os.path.join(folder_prefix, image_folder, 'logos/') # Logo Folder Path\n",
    "conference_logo_folder = os.path.join(folder_prefix, logo_folder, 'conference') # Conference Logo Folder Path\n",
    "export_folder = os.path.join(folder_prefix, image_folder, 'export/') # Export Folder Path\n",
    "background_folder = os.path.join(folder_prefix, image_folder, 'background/') # Background Folder Path\n",
    "\n",
    "# Other paths\n",
    "school_info_path = os.path.join(data_folder, 'arena_school_info.csv') # School Info Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Database connection and extract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the database\n",
    "conn = sqlite3.connect(db_path, isolation_level=None)\n",
    "\n",
    "## Extract player_stats and convert TOI into seconds for easier calculations\n",
    "player_stats = pd.read_sql_query(\"SELECT * FROM player_stats\", conn)\n",
    "\n",
    "### TOI to seconds - From MM:SS string to seconds integer\n",
    "def convert_toi_to_seconds(toi_str):\n",
    "    if pd.isna(toi_str):\n",
    "        return None\n",
    "    try:\n",
    "        minutes, seconds = map(int, toi_str.split(':'))\n",
    "        return minutes * 60 + seconds\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "player_stats['TOI'] = player_stats['TOI'].apply(convert_toi_to_seconds)\n",
    "\n",
    "# Check Data\n",
    "print(player_stats.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Create a date column for each row based on the Game_ID to track change in ice time over the season\n",
    "def get_date_from_game_id(game_id):\n",
    "    if pd.isna(game_id):\n",
    "        return None\n",
    "    try:\n",
    "        return str(game_id)[:10]\n",
    "\n",
    "        # Convert to datetime object\n",
    "        date_obj = datetime.strptime(game_id, '%Y-%m-%d')\n",
    "        # Convert to desired format\n",
    "        \n",
    "\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "# Create a new column 'Date' in the DataFrame\n",
    "player_stats['Date'] = player_stats['Game_ID'].apply(get_date_from_game_id)\n",
    "\n",
    "# Check Data\n",
    "print(player_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time On Ice Exploration\n",
    "\n",
    "- WANT TO DO\n",
    "    - Get Line and position for each player for each game - link by game_id can also pull in shot and whatever other data might want (blocked shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call the line_chart db table to the a dataframe\n",
    "line_chart = pd.read_sql_query(\"SELECT * FROM line_chart\", conn)\n",
    "\n",
    "# Group into F / D / G lines based on Position column into new pos_1 / pos_2 columns\n",
    "# # pos_1 = F / D / G \n",
    "# Center, Left Wing, Right Wing = F\n",
    "# left D, Right D = D\n",
    "# Goalie = G\n",
    "\n",
    "# pos_2 = C / L / R / D / G\n",
    "\n",
    "\n",
    "## Assign positions in the new columns\n",
    "def assign_positions(row):\n",
    "    if row['Position'] in ['Center', 'Left Wing', 'Right Wing']:\n",
    "        return pd.Series(['F', row['Position'][0]])\n",
    "    elif row['Position'] in ['Left D', 'Right D']:\n",
    "        return pd.Series(['D', 'D'])\n",
    "    elif row['Position'] == 'Goalie':\n",
    "        return pd.Series(['G', 'G'])\n",
    "    else:\n",
    "        return pd.Series([None, None])\n",
    "line_chart[['pos_1', 'pos_2']] = line_chart.apply(assign_positions, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check Data\n",
    "print(line_chart.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Merge Position Data in with the player_stats dataframe based on Team, Player, and Game_ID\n",
    "\n",
    "merged_df = pd.merge(player_stats, line_chart[['Game_ID', 'Team', 'Player', 'Line', 'pos_1', 'pos_2']], on=['Game_ID', 'Team', 'Player'], how='left')\n",
    "\n",
    "# Relabel the rows of extra skaters - they have NaN in Line pos1 / pos_2 columns - Replace with 'E'\n",
    "# merged_df.loc[merged_df['pos_1'].isna(), 'pos_1'] = 'E'\n",
    "# merged_df.loc[merged_df['pos_2'].isna(), 'pos_2'] = 'E'\n",
    "merged_df['pos_1'].fillna('E', inplace=True)\n",
    "merged_df['pos_2'].fillna('E', inplace=True)\n",
    "merged_df['Line'].fillna('E', inplace=True)\n",
    "\n",
    "# Remove any rows with TOI = 0\n",
    "merged_df = merged_df[merged_df['TOI'] != 0]\n",
    "\n",
    "\n",
    "# Check Data\n",
    "merged_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Some Basic statitical exploration\n",
    "\n",
    "# ### Calculate mean and median TOI for forwards and defense by team\n",
    "def calculate_mean_median_toi(player_stats):\n",
    "    # Group by team and position\n",
    "    grouped = player_stats.groupby(['Team', 'pos_1'])\n",
    "\n",
    "    # Calculate mean and median TOI\n",
    "    mean_toi = grouped['TOI'].mean()\n",
    "    median_toi = grouped['TOI'].median()\n",
    "\n",
    "    return pd.DataFrame({'Mean_TOI': mean_toi, 'Median_TOI': median_toi}).reset_index()\n",
    "\n",
    "mean_median_df = calculate_mean_median_toi(merged_df)\n",
    "\n",
    "# Drop G rows\n",
    "mean_median_df = mean_median_df[mean_median_df['pos_1'] != 'G']\n",
    "\n",
    "\n",
    "# Check Data\n",
    "mean_median_df.head()\n",
    "mean_median_df.tail()\n",
    "# mean_median_df.info()\n",
    "# mean_median_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate mean, median based on Line and Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Get the mean and median TOI for each team grouped by line and pos_1\n",
    "\n",
    "def calculate_mean_median_toi_by_line(player_stats):\n",
    "    # Group by team, line, and position\n",
    "    grouped = player_stats.groupby(['Team', 'Line', 'pos_1'])\n",
    "\n",
    "    # Calculate mean and median TOI\n",
    "    mean_toi = grouped['TOI'].mean()\n",
    "    median_toi = grouped['TOI'].median()\n",
    "\n",
    "    return pd.DataFrame({'Mean_TOI': mean_toi, 'Median_TOI': median_toi}).reset_index()\n",
    "\n",
    "mean_median_by_line_df = calculate_mean_median_toi_by_line(merged_df)\n",
    "\n",
    "# Drop G rows\n",
    "mean_median_by_line_df = mean_median_by_line_df[mean_median_by_line_df['pos_1'] != 'G']\n",
    "\n",
    "# Check Data\n",
    "mean_median_by_line_df.head()\n",
    "\n",
    "# Info\n",
    "# mean_median_by_line_df.info()\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Histogram off TOI by F / D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Eaxamine the data with hisotgrams of TOI by position\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Create a histogram for TOI by position\n",
    "sns.histplot(data=merged_df, x='TOI', hue='pos_1', kde=True, stat='density', common_norm=False)\n",
    "# Set the title and labels\n",
    "plt.title('TOI by Position')\n",
    "plt.xlabel('TOI (seconds)')\n",
    "plt.ylabel('Density')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick Hosoptgram of TOI Forwards Only on Line assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Same thing by line - only forwards\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Create a histogram for TOI by position\n",
    "sns.histplot(data=merged_df[merged_df['pos_1'] == 'F'], x='TOI', hue='Line', kde=True, stat='density', common_norm=False)\n",
    "# Set the title and labels\n",
    "plt.title('TOI by Line')\n",
    "plt.xlabel('TOI (seconds)')\n",
    "plt.ylabel('Density')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Set the figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a plot of TOI for Michigan State fowards over time grouped by line #\n",
    "set_team = 'Michigan State'\n",
    "set_team = 'Michigan'\n",
    "set_team = 'Minnesota'\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Convert TOI column from seconds to minutes\n",
    "merged_df['TOI'] = merged_df['TOI'] / 60\n",
    "\n",
    "# Create a line plot for TOI over time for Michigan State forwards\n",
    "for line in merged_df[(merged_df['pos_1'] == 'F') & (merged_df['Team'] == set_team)]['Line'].unique():\n",
    "    sns.lineplot(data=merged_df[(merged_df['pos_1'] == 'F') & (merged_df['Team'] == set_team) & (merged_df['Line'] == line)], x='Date', y='TOI', label=line)\n",
    "# Set the title and labels\n",
    "plt.title(f'Time on Ice by Line for {set_team} Forwards')\n",
    "plt.xlabel('Date')\n",
    "# Convert the tick marks on the y axis into minutes (divide by 60)\n",
    "plt.ylabel('TOI (minutes)')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.ylabel('TOI (seconds)')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# Set the figure size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defensive pairs time on ice Plot\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Convert TOI column from seconds to minutes\n",
    "# merged_df['TOI'] = merged_df['TOI'] / 60\n",
    "# Create a line plot for TOI over time for Michigan State defensmen\n",
    "for line in merged_df[(merged_df['pos_1'] == 'D') & (merged_df['Team'] == set_team)]['Line'].unique():\n",
    "    sns.lineplot(data=merged_df[(merged_df['pos_1'] == 'D') & (merged_df['Team'] == set_team) & (merged_df['Line'] == line)], x='Date', y='TOI', label=line)\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title(f'Time on Ice by Line for {set_team} Defensmen')\n",
    "plt.xlabel('Date')\n",
    "# Convert the tick marks on the y axis into minutes (divide by 60)\n",
    "plt.ylabel('TOI (minutes)')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the database\n",
    "conn = sqlite3.connect(db_path, isolation_level=None)\n",
    "\n",
    "# convert string time to continuous time\n",
    "## SQL query to fetch\n",
    "def extract_goal_summary(conn):\n",
    "    \"\"\"\n",
    "    Extracts and preprocesses the goal summary data from the database.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        WITH UniqueGoals AS (\n",
    "        SELECT DISTINCT Game_ID, Team, Period, Time, PP\n",
    "        FROM scoring_summary\n",
    "    )\n",
    "    SELECT * FROM UniqueGoals;\n",
    "    \"\"\"\n",
    "    goal_df = pd.read_sql(query, conn)\n",
    "    return goal_df\n",
    "\n",
    "# Convert string time to continuous time\n",
    "def convert_to_continuous_time(row):\n",
    "    \"\"\"\n",
    "    Converts period-based time to a continuous format (0-65 minutes).\n",
    "    \"\"\"\n",
    "    period_offsets = {'1st Period': 0, '2nd Period': 20, '3rd Period': 40, 'Overtime': 60}\n",
    "    minutes, seconds = map(int, row['Time'].split(':'))\n",
    "    offset = period_offsets.get(row['Period'], 0)\n",
    "    return offset + minutes + seconds / 60.0\n",
    "\n",
    "## Load the data\n",
    "goal_data = extract_goal_summary(conn)\n",
    "# Create a continuous time column\n",
    "goal_data['Cont_Time'] = goal_data.apply(convert_to_continuous_time, axis=1)\n",
    "\n",
    "goal_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the scoring table calculate each team's average goals scored and allowed in each period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor code to address issues with games played and overtime calculations\n",
    "\n",
    "# Step 1: Ensure games played are counted correctly by counting unique Game_ID values\n",
    "games_played = goal_data.groupby('Team')['Game_ID'].nunique().reset_index()\n",
    "games_played.rename(columns={'Game_ID': 'games_played'}, inplace=True)\n",
    "\n",
    "# Step 2: Identify games that went to overtime\n",
    "# A game goes to overtime if the goals scored by both teams across the first three periods are equal\n",
    "goal_data['Period'] = goal_data['Period'].str.strip()  # Standardize 'Period' column\n",
    "regulation_periods = ['1st Period', '2nd Period', '3rd Period']\n",
    "\n",
    "# Sum goals by team and game across regulation periods\n",
    "regulation_goals = goal_data[goal_data['Period'].isin(regulation_periods)]\n",
    "regulation_totals = regulation_goals.groupby(['Game_ID', 'Team']).size().unstack(fill_value=0)\n",
    "\n",
    "# Identify games tied after regulation\n",
    "overtime_games = regulation_totals[regulation_totals.apply(lambda row: row.sum() == 0, axis=1)].index\n",
    "\n",
    "# Count overtime games for each team\n",
    "ot_games_played = goal_data[goal_data['Game_ID'].isin(overtime_games)].groupby('Team')['Game_ID'].nunique()\n",
    "ot_games_played = ot_games_played.reset_index()\n",
    "ot_games_played.rename(columns={'Game_ID': 'OT_games_played'}, inplace=True)\n",
    "\n",
    "# Step 3: Calculate period-by-period stats\n",
    "team_period_stats = []\n",
    "for team in games_played['Team']:\n",
    "    team_data = goal_data[goal_data['Team'] == team]\n",
    "    opponent_data = goal_data[goal_data['Team'] != team]\n",
    "    team_games = team_data['Game_ID'].unique()\n",
    "    row = {'Team': team, 'games_played': games_played.loc[games_played['Team'] == team, 'games_played'].values[0]}\n",
    "\n",
    "    for i, period in enumerate(regulation_periods, start=1):  # Process 1st, 2nd, 3rd periods\n",
    "        scored = team_data[team_data['Period'] == period].shape[0]\n",
    "        allowed = opponent_data[(opponent_data['Game_ID'].isin(team_games)) & (opponent_data['Period'] == period)].shape[0]\n",
    "        avg_scored = scored / row['games_played']\n",
    "        avg_allowed = allowed / row['games_played']\n",
    "        diff = scored - allowed\n",
    "\n",
    "        period_label = ['1st', '2nd', '3rd'][i - 1]\n",
    "        row[f'{period_label}_scored'] = scored\n",
    "        row[f'{period_label}_allowed'] = allowed\n",
    "        row[f'{period_label}_scored_avg'] = avg_scored\n",
    "        row[f'{period_label}_allowed_avg'] = avg_allowed\n",
    "        row[f'{period_label}_diff'] = diff\n",
    "\n",
    "\n",
    "\n",
    "    team_period_stats.append(row)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "results = pd.DataFrame(team_period_stats)\n",
    "\n",
    "# Display the refactored results\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Calculate the AVG differential for each period\n",
    "results['1st_diff_avg'] = results['1st_diff'] / results['games_played']\n",
    "results['2nd_diff_avg'] = results['2nd_diff'] / results['games_played']\n",
    "results['3rd_diff_avg'] = results['3rd_diff'] / results['games_played']\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working and moved to it's own book for visualizations\n",
    "- High_Impact_Goals_with_viz\n",
    "### Load scoring table and make table of goals scored first and last minute of periods as well as total goals scored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "# Basics\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from matplotlib import font_manager\n",
    "\n",
    "\n",
    "## Load The Exo 2 font in case of problems with the font\n",
    "font_path = 'C:/Windows/Fonts/Exo 2.tff'\n",
    "locations =['../data/Exo_2'] # Font Location\n",
    "\n",
    "font_files = font_manager.findSystemFonts(fontpaths=locations)\n",
    "\n",
    "for file in font_files:\n",
    "    font_manager.fontManager.addfont(file)\n",
    "\n",
    "    ## Set the date to appear on the source area of plots (the last date of the data)\n",
    "last_game_date = '2025-01-05'\n",
    "\n",
    "## File Paths\n",
    "folder_prefix = ''\n",
    "# folder_prefix = '..'\n",
    "data_folder = os.path.join(folder_prefix, '..', 'data/') # Data Folder Path\n",
    "temp_folder = os.path.join(folder_prefix,'..', 'TEMP/',) # Temp Folder Path\n",
    "TEMP_FOLDER = temp_folder # Temp Folder Path as used in legacy code\n",
    "output_folder = os.path.join(temp_folder, 'team_comp_output/') # Output Folder Path\n",
    "# data\\db\\2024_Dec_10_CLEANED_OLD_METHOD.db\n",
    "db_path = os.path.join(data_folder, 'db', '2025_Jan_07_test2_ROUGH.db') # Database Path\n",
    "# db_path = os.path.join(data_folder, 'db', '2024_Dec_03_v4_ROUGH.db') # Database Path\n",
    "# db_path = os.path.join(temp_folder, '2024_Dec_03_v3_ROUGH.db') # Database Path\n",
    "image_folder = os.path.join(folder_prefix, '..', 'images/') # Image Folder Path\n",
    "logo_folder = os.path.join(folder_prefix, image_folder, 'logos/') # Logo Folder Path\n",
    "conference_logo_folder = os.path.join(folder_prefix, logo_folder, 'conference') # Conference Logo Folder Path\n",
    "export_folder = os.path.join(folder_prefix, image_folder, 'export/') # Export Folder Path\n",
    "background_folder = os.path.join(folder_prefix, image_folder, 'background/') # Background Folder Path\n",
    "\n",
    "# Other paths\n",
    "school_info_path = os.path.join(data_folder, 'arena_school_info.csv') # School Info Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the database\n",
    "conn = sqlite3.connect(db_path, isolation_level=None)\n",
    "\n",
    "# convert string time to continuous time\n",
    "## SQL query to fetch\n",
    "def extract_goal_summary(conn):\n",
    "    \"\"\"\n",
    "    Extracts and preprocesses the goal summary data from the database.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "        WITH UniqueGoals AS (\n",
    "        SELECT DISTINCT Game_ID, Team, Period, Time, PP\n",
    "        FROM scoring_summary\n",
    "    )\n",
    "    SELECT * FROM UniqueGoals;\n",
    "    \"\"\"\n",
    "    goal_df = pd.read_sql(query, conn)\n",
    "    return goal_df\n",
    "\n",
    "# Convert string time to continuous time\n",
    "def convert_to_continuous_time(row):\n",
    "    \"\"\"\n",
    "    Converts period-based time to a continuous format (0-65 minutes).\n",
    "    \"\"\"\n",
    "    period_offsets = {'1st Period': 0, '2nd Period': 20, '3rd Period': 40, 'Overtime': 60}\n",
    "    minutes, seconds = map(int, row['Time'].split(':'))\n",
    "    offset = period_offsets.get(row['Period'], 0)\n",
    "    return offset + minutes + seconds / 60.0\n",
    "\n",
    "## Load the data\n",
    "goal_data = extract_goal_summary(conn)\n",
    "# Create a continuous time column\n",
    "goal_data['Cont_Time'] = goal_data.apply(convert_to_continuous_time, axis=1)\n",
    "\n",
    "goal_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify Empty Net Goals (EN in the PP column) and flag in a new column\n",
    "goal_data['EN'] = goal_data['PP'].apply(lambda x: 'EN' in x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First minute of frame time ranges\n",
    "# 0-1, 20-21, 40-41, 60-61\n",
    "\n",
    "# Last minute of frame time ranges\n",
    "# 19-20, 39-40, 59-60, 64-65\n",
    "\n",
    "# Define the function for categorizing goal types\n",
    "def goal_type_first_last(row):\n",
    "    \"\"\"\n",
    "    Categorizes goals as first minute, last minute, or other.\n",
    "    \"\"\"\n",
    "    # Check if the goal is in the first minute\n",
    "    if (row['Cont_Time'] < 1 or  # Before the end of the first minute of the game\n",
    "        (row['Cont_Time'] > 20 and row['Cont_Time'] < 21) or  # Between 20:00 and 21:00 (1st period)\n",
    "        (row['Cont_Time'] > 40 and row['Cont_Time'] < 41) or  # Between 40:00 and 41:00 (2nd period)\n",
    "        (row['Cont_Time'] > 60 and row['Cont_Time'] < 61)):  # Between 60:00 and 61:00 (3rd period)\n",
    "        return 'First Minute'\n",
    "\n",
    "    # Check if the goal is in the last minute of a period\n",
    "    elif (row['Cont_Time'] > 19 and row['Cont_Time'] < 20 or  # Between 19:00 and 20:00 (1st period)\n",
    "          (row['Cont_Time'] > 39 and row['Cont_Time'] < 40) or  # Between 39:00 and 40:00 (2nd period)\n",
    "          (row['Cont_Time'] > 59 and row['Cont_Time'] < 60) or  # Between 59:00 and 60:00 (3rd period)\n",
    "          (row['Cont_Time'] > 64 and row['Cont_Time'] < 65)):  # Between 64:00 and 65:00 (overtime)\n",
    "        return 'Last Minute'\n",
    "\n",
    "    # Otherwise, categorize as 'Other'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "\n",
    "# Create a new column for the goal type\n",
    "goal_data['Goal_Type'] = goal_data.apply(goal_type_first_last, axis=1)\n",
    "\n",
    "\n",
    "# Check distribution of goal types\n",
    "# goal_data['Goal_Type'].value_counts()\n",
    "\n",
    "goal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do the Same thing to flag the goals that happened in the first and last 2 minutes of the period\n",
    "\n",
    "def goal_type_first2_last_2(row):\n",
    "    \"\"\"\n",
    "    Flag the goals that happened in the first and last 2 minutes of the period\n",
    "    \"\"\"\n",
    "\n",
    "    if (row['Cont_Time'] < 2 or  # Before the end of the first minute of the game\n",
    "        (row['Cont_Time'] > 20 and row['Cont_Time'] < 22) or  # Between 20:00 and 21:00 (1st period)\n",
    "        (row['Cont_Time'] > 40 and row['Cont_Time'] < 42) or  # Between 40:00 and 41:00 (2nd period)\n",
    "        (row['Cont_Time'] > 60 and row['Cont_Time'] < 62)):  # Between 60:00 and 61:00 (3rd period)\n",
    "        return 'First 2 Minutes'\n",
    "\n",
    "    # Check if the goal is in the last 2 minutes of a period\n",
    "    elif (row['Cont_Time'] > 18 and row['Cont_Time'] < 20 or  # Between 18:00 and 20:00 (1st period)\n",
    "          (row['Cont_Time'] > 38 and row['Cont_Time'] < 40) or  # Between 38:00 and 40:00 (2nd period)\n",
    "          (row['Cont_Time'] > 58 and row['Cont_Time'] < 60) or  # Between 58:00 and 60:00 (3rd period)\n",
    "          (row['Cont_Time'] > 63 and row['Cont_Time'] < 65)):  # Between 63:00 and 65:00 (overtime)\n",
    "        return 'Last 2 Minutes'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Create a new column to flag the goals that happened in the first and last 2 minutes of the period\n",
    "goal_data['Goal_Type_2'] = goal_data.apply(goal_type_first2_last_2, axis=1)\n",
    "\n",
    "goal_data.head()\n",
    "# Check distribution of goal types\n",
    "goal_data['Goal_Type_2'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if The EN goals are in the last minute and flag them as such in new column\n",
    "goal_data['EN_Last_Minute'] = (goal_data['EN'] & (goal_data['Goal_Type'] == 'Last Minute'))\n",
    "# DO the same for the last 2 minutes\n",
    "goal_data['EN_Last_2_Minutes'] = (goal_data['EN'] & (goal_data['Goal_Type_2'] == 'Last 2 Minutes'))\n",
    "\n",
    "goal_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count High Impact Goals (Goals scored within a minute or 2 of another goal being scored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the data by Game_ID and Cont_Time for chronological processing\n",
    "goal_data_sorted = goal_data.sort_values(by=['Game_ID', 'Cont_Time']).reset_index(drop=True)\n",
    "\n",
    "# Display the sorted data to verify\n",
    "goal_data_sorted.head()\n",
    "\n",
    "# Group data by Game_ID to analyze each game separately\n",
    "grouped = goal_data_sorted.groupby('Game_ID')\n",
    "\n",
    "\n",
    "# Debugging the logic and fixing the issue\n",
    "# Reinitialize dictionaries to track counts\n",
    "team_quick_responses = defaultdict(int)\n",
    "opponent_quick_responses = defaultdict(int)\n",
    "\n",
    "# Process each game individually again\n",
    "for game_id, game_data in grouped:\n",
    "    # Reset index for easier row iteration\n",
    "    game_data = game_data.reset_index(drop=True)\n",
    "    \n",
    "    # Iterate through goals in this game\n",
    "    for i in range(len(game_data)):\n",
    "        current_team = game_data.loc[i, 'Team']\n",
    "        current_time = game_data.loc[i, 'Cont_Time']\n",
    "        \n",
    "        # Compare with subsequent goals in the same game\n",
    "        for j in range(i + 1, len(game_data)):\n",
    "            next_team = game_data.loc[j, 'Team']\n",
    "            next_time = game_data.loc[j, 'Cont_Time']\n",
    "            \n",
    "            # If the time difference is more than 1 minute, stop checking\n",
    "            if next_time - current_time > 1:\n",
    "                break\n",
    "            \n",
    "            # If the same team scores again within 1 minute\n",
    "            if next_team == current_team:\n",
    "                team_quick_responses[current_team] += 1\n",
    "            \n",
    "            # If the opposing team scores within 1 minute\n",
    "            elif next_team != current_team:\n",
    "                opponent_quick_responses[current_team] += 1\n",
    "\n",
    "# Create the results DataFrame with updated column labels\n",
    "quick_responses_df = pd.DataFrame({\n",
    "    'Team': list(set(goal_data_sorted['Team'])),\n",
    "    'HI_Back_to_Back': [team_quick_responses[team] for team in set(goal_data_sorted['Team'])],\n",
    "    'HI_Quick_Response': [opponent_quick_responses[team] for team in set(goal_data_sorted['Team'])]\n",
    "})\n",
    "\n",
    "quick_responses_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize dictionaries to track the new counts\n",
    "team_quick_responses_2 = defaultdict(int)\n",
    "opponent_quick_responses_2 = defaultdict(int)\n",
    "\n",
    "# Process each game again for the new 2-minute interval logic\n",
    "for game_id, game_data in grouped:\n",
    "    # Reset index for easier row iteration\n",
    "    game_data = game_data.reset_index(drop=True)\n",
    "    \n",
    "    # Iterate through goals in this game\n",
    "    for i in range(len(game_data)):\n",
    "        current_team = game_data.loc[i, 'Team']\n",
    "        current_time = game_data.loc[i, 'Cont_Time']\n",
    "        \n",
    "        # Compare with subsequent goals in the same game\n",
    "        for j in range(i + 1, len(game_data)):\n",
    "            next_team = game_data.loc[j, 'Team']\n",
    "            next_time = game_data.loc[j, 'Cont_Time']\n",
    "            \n",
    "            # If the time difference is more than 2 minutes, stop checking for 2-minute responses\n",
    "            if next_time - current_time > 2:\n",
    "                break\n",
    "            \n",
    "            # If the same team scores again within 2 minutes\n",
    "            if next_team == current_team:\n",
    "                team_quick_responses_2[current_team] += 1\n",
    "            \n",
    "            # If the opposing team scores within 2 minutes\n",
    "            elif next_team != current_team:\n",
    "                opponent_quick_responses_2[current_team] += 1\n",
    "\n",
    "# Add the new columns to the results DataFrame\n",
    "quick_responses_df['HI_Back_to_Back_2'] = [\n",
    "    team_quick_responses_2[team] for team in set(goal_data_sorted['Team'])\n",
    "]\n",
    "quick_responses_df['HI_Quick_Response_2'] = [\n",
    "    opponent_quick_responses_2[team] for team in set(goal_data_sorted['Team'])\n",
    "]\n",
    "\n",
    "# Display the updated results\n",
    "quick_responses_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_column_presence(goal_tally, columns):\n",
    "    \"\"\"\n",
    "    Ensure that all required columns are present in the goal tally DataFrame.\n",
    "    If a column is missing, add it and fill with zeros.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col not in goal_tally.columns:\n",
    "            goal_tally[col] = 0\n",
    "    return goal_tally\n",
    "\n",
    "def robust_final_tally(goal_data):\n",
    "    \"\"\"\n",
    "    Tally the number of goals by team, ensuring all expected columns are enforced explicitly.\n",
    "    \"\"\"\n",
    "    # Define expected columns for each group type\n",
    "    expected_columns_type = ['First Minute', 'Last Minute', 'Other']\n",
    "    expected_columns_type2 = ['First 2 Minutes', 'Last 2 Minutes']\n",
    "\n",
    "    # Group and tally goals by Goal_Type\n",
    "    goal_tally_type = goal_data.groupby(['Team', 'Goal_Type']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # Ensure all expected columns are present for Goal_Type\n",
    "    goal_tally_type = enforce_column_presence(goal_tally_type, expected_columns_type)\n",
    "\n",
    "    # Group and tally goals by Goal_Type_2\n",
    "    goal_tally_type2 = goal_data.groupby(['Team', 'Goal_Type_2']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    # Ensure all expected columns are present for Goal_Type_2\n",
    "    goal_tally_type2 = enforce_column_presence(goal_tally_type2, expected_columns_type2)\n",
    "\n",
    "    # Merge both grouped DataFrames\n",
    "    goal_tally = pd.merge(goal_tally_type, goal_tally_type2, on='Team', how='outer').fillna(0)\n",
    "\n",
    "    # Ensure all columns in the final DataFrame\n",
    "    all_expected_columns = ['Team'] + expected_columns_type + expected_columns_type2\n",
    "    goal_tally = enforce_column_presence(goal_tally, all_expected_columns)\n",
    "\n",
    "    # Calculate total goals\n",
    "    goal_tally['Total Goals'] = (goal_tally['First Minute'] +\n",
    "                                 \n",
    "                                 goal_tally['Last Minute'] +\n",
    "                                 \n",
    "                                 goal_tally['Other_x'])\n",
    "\n",
    "    # Calculate percentages\n",
    "    goal_tally['Pct First Minute'] = goal_tally['First Minute'] / goal_tally['Total Goals'].replace(0, 1)\n",
    "    goal_tally['Pct First 2 Minutes'] = goal_tally['First 2 Minutes'] / goal_tally['Total Goals'].replace(0, 1)\n",
    "    goal_tally['Pct Last Minute'] = goal_tally['Last Minute'] / goal_tally['Total Goals'].replace(0, 1)\n",
    "    goal_tally['Pct Last 2 Minutes'] = goal_tally['Last 2 Minutes'] / goal_tally['Total Goals'].replace(0, 1)\n",
    "\n",
    "    # Tally EN Last Minute and EN Last 2 Minutes\n",
    "    en_last_minute_tally = goal_data[goal_data['EN_Last_Minute']].groupby('Team').size()\n",
    "    en_last_2_minute_tally = goal_data[goal_data['EN_Last_2_Minutes']].groupby('Team').size()\n",
    "\n",
    "    # Add EN tallies to the final DataFrame\n",
    "    goal_tally['EN Last Minute'] = goal_tally['Team'].map(en_last_minute_tally).fillna(0).astype(int)\n",
    "    goal_tally['EN Last 2 Minutes'] = goal_tally['Team'].map(en_last_2_minute_tally).fillna(0).astype(int)\n",
    "\n",
    "    return goal_tally\n",
    "\n",
    "# Apply the function to the data\n",
    "team_goal_tally = robust_final_tally(goal_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge the High Impact goal data with the Team Goal Tally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the High Impact Goal data with the goal tally data\n",
    "team_goal_tally = pd.merge(team_goal_tally, quick_responses_df, on='Team', how='outer')\n",
    "\n",
    "# Reorganize the table into the following order\n",
    "# Team, Total Goals, First minute, Pct First Minute, First 2 Minutes, Pct First 2 Minutes, \n",
    "# Last Minute, EN Last Minute, Pct Last Minute, Last 2 Minutes, EN Last 2 Minutes, Pct Last 2 Minutes, HI Back-to-Back, \n",
    "# HI Quick Response, HI Back-to-Back 2, HI Quick Response 2\n",
    "\n",
    "# Define the column order\n",
    "column_order = ['Team', 'Total Goals', 'First Minute', 'Pct First Minute', 'First 2 Minutes', 'Pct First 2 Minutes',\n",
    "                'Last Minute', 'EN Last Minute', 'Pct Last Minute', 'Last 2 Minutes', 'EN Last 2 Minutes', 'Pct Last 2 Minutes',\n",
    "                'HI_Back_to_Back', 'HI_Quick_Response', 'HI_Back_to_Back_2', 'HI_Quick_Response_2']\n",
    "\n",
    "# Reorder the columns\n",
    "team_goal_tally = team_goal_tally[column_order]\n",
    "\n",
    "\n",
    "# Display the final DataFrame\n",
    "team_goal_tally.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### ORIGINAL CODE #########\n",
    "## Tally the number of goals by team and goal type\n",
    "# Output a Table with the following columns:\n",
    "# Team, First Minute Goals, Last Minute Goals, Other Goals, Total Goals, Pct First Minute, Pct Last Minute\n",
    "# def tally_goals_by_team(goal_data):\n",
    "#     \"\"\"\n",
    "#     Tally the number of goals by team and goal type.\n",
    "#     \"\"\"\n",
    "#     # Group by team and goal type\n",
    "#     goal_tally = goal_data.groupby(['Team', 'Goal_Type']).size().unstack().reset_index()\n",
    "\n",
    "#     # Fill in missing columns\n",
    "#     goal_tally = goal_tally.fillna(0)\n",
    "\n",
    "#     # Calculate total goals\n",
    "#     goal_tally['Total Goals'] = goal_tally['First Minute'] + goal_tally['Last Minute'] + goal_tally['Other']\n",
    "\n",
    "#     # Calculate percentages\n",
    "#     goal_tally['Pct First Minute'] = goal_tally['First Minute'] / goal_tally['Total Goals']\n",
    "#     goal_tally['Pct Last Minute'] = goal_tally['Last Minute'] / goal_tally['Total Goals']\n",
    "#     goal_tally['Pct Other'] = goal_tally['Other'] / goal_tally['Total Goals']\n",
    "\n",
    "#     # Tally EN Last Minute Goals\n",
    "#     goal_tally['EN Last Minute'] = goal_data[goal_data['EN_Last_Minute']].groupby('Team').size()\n",
    "\n",
    "#     return goal_tally\n",
    "\n",
    "# # Tally the goals by team\n",
    "# team_goal_tally = tally_goals_by_team(goal_data)\n",
    "\n",
    "# team_goal_tally.head()\n",
    "\n",
    "# Value Counts for EN Last Minute Goals\n",
    "# goal_data['EN_Last_Minute'].value_counts()\n",
    "\n",
    "# team_goal_tally['EN Last Minute'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by First Minute Goals\n",
    "team_goal_tally = team_goal_tally.sort_values('First Minute', ascending=False)\n",
    "\n",
    "# Sort by Last Minute Goals\n",
    "team_goal_tally = team_goal_tally.sort_values('Last Minute', ascending=False)\n",
    "\n",
    "# sort by percentage of other goals (decending)\n",
    "\n",
    "\n",
    "# sort by total goals\n",
    "team_goal_tally = team_goal_tally.sort_values('Total Goals', ascending=False)\n",
    "\n",
    "team_goal_tally.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LONG ISLAND UNIVERSITY #### 1-8-24 IS MISSING\n",
    "### PROVIDENCE HAS ONE MORE GOAL THAN CHN TABLES #### 1-8-24\n",
    "######## SOMETHING MIGHT BE UP WITH OHIO STATE ########\n",
    "#### DOES NOT MATCH CHN TABLES #### 1-8-24\n",
    "\n",
    "#### 1-8-24 - DISCOVERED THAT CHNS TEAM STATS TABLE IS NOT ACCURATE - OHIO STATE LISTED AS ONLY HAVING 58 GOALS, WHEN YOU CLICK THROUGH TO PLAYER BREAKDOWN THEY TALLY TO 64, JUST LIKE MY DATA HAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "breakpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scorigami - Annimated Gif Code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "image_folder = os.path.join('..', 'TEMP', 'IMAGES', 'stich_folder')\n",
    "\n",
    "\n",
    "from PIL import Image, ImageSequence\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import re  # For extracting numbers from filenames\n",
    "\n",
    "# def create_animated_gif(image_folder, output_gif, total_duration=5, transition_frames=10):\n",
    "#     \"\"\"\n",
    "#     Create an animated GIF from a sequence of images with fade transitions.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - image_folder: Path to the folder containing images (named with leading numbers, e.g., 1_*.png, 2_*.png).\n",
    "#     - output_gif: Path for the output GIF.\n",
    "#     - total_duration: Total duration of the animation in seconds.\n",
    "#     - transition_frames: Number of intermediate frames for transitions between images.\n",
    "#     \"\"\"\n",
    "#     # Load images sorted by the leading number in filenames\n",
    "#     images = sorted(\n",
    "#         [Image.open(os.path.join(image_folder, img)) for img in os.listdir(image_folder) if img.endswith(\".png\")],\n",
    "#         key=lambda x: int(re.match(r\"(\\d+)\", os.path.basename(x.filename)).group(1))  # Extract leading numbers\n",
    "#     )\n",
    "\n",
    "#     # Resize images to a suitable size while maintaining the aspect ratio\n",
    "#     # Set max width and height\n",
    "#     max_width = 800\n",
    "#     max_height = 1080\n",
    "\n",
    "#     for i, img in enumerate(images):\n",
    "#         width, height = img.size\n",
    "#         if width > max_width or height > max_height:\n",
    "#             # Resize the image\n",
    "#             ratio = min(max_width / width, max_height / height)\n",
    "#             new_size = (int(width * ratio), int(height * ratio))\n",
    "#             images[i] = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#     # Calculate total frames and duration per frame\n",
    "#     num_images = len(images)\n",
    "#     frames_per_image = total_duration * 1000 // (num_images + (num_images - 1) * transition_frames)\n",
    "#     frame_duration = int(frames_per_image)  # Duration of each frame in milliseconds\n",
    "\n",
    "#     all_frames = []\n",
    "#     for i in range(num_images - 1):\n",
    "#         # Add the current image\n",
    "#         all_frames.append(images[i])\n",
    "        \n",
    "#         # Create transition frames (fade to next image)\n",
    "#         for t in range(1, transition_frames + 1):\n",
    "#             alpha = t / (transition_frames + 1)\n",
    "#             blend_frame = Image.blend(images[i], images[i + 1], alpha)\n",
    "#             all_frames.append(blend_frame)\n",
    "\n",
    "#     # Add the final image\n",
    "#     all_frames.append(images[-1])\n",
    "\n",
    "#     # Save all frames as a GIF\n",
    "#     all_frames[0].save(\n",
    "#         output_gif,\n",
    "#         save_all=True,\n",
    "#         append_images=all_frames[1:],\n",
    "#         duration=frame_duration,\n",
    "#         loop=1\n",
    "#     )\n",
    "\n",
    "# def create_animated_gif(image_folder, output_gif, total_duration=5, transition_frames=10, reverse_order=False):\n",
    "#     \"\"\"\n",
    "#     Create an animated GIF from a sequence of images with fade transitions.\n",
    "\n",
    "#     Parameters:\n",
    "#     - image_folder: Path to the folder containing images (named with leading numbers, e.g., 1_*.png, 2_*.png).\n",
    "#     - output_gif: Path for the output GIF.\n",
    "#     - total_duration: Total duration of the animation in seconds.\n",
    "#     - transition_frames: Number of intermediate frames for transitions between images.\n",
    "#     - reverse_order: If True, creates the GIF in reverse order.\n",
    "#     \"\"\"\n",
    "#     # Load images sorted by the leading number in filenames\n",
    "#     images = sorted(\n",
    "#         [Image.open(os.path.join(image_folder, img)) for img in os.listdir(image_folder) if img.endswith(\".png\")],\n",
    "#         key=lambda x: int(re.match(r\"(\\d+)\", os.path.basename(x.filename)).group(1))  # Extract leading numbers\n",
    "#     )\n",
    "\n",
    "#     # Reverse the order if requested\n",
    "#     if reverse_order:\n",
    "#         images.reverse()\n",
    "\n",
    "#     # Resize images to a suitable size while maintaining the aspect ratio\n",
    "#     max_width = 800\n",
    "#     max_height = 1080\n",
    "#     for i, img in enumerate(images):\n",
    "#         width, height = img.size\n",
    "#         if width > max_width or height > max_height:\n",
    "#             ratio = min(max_width / width, max_height / height)\n",
    "#             new_size = (int(width * ratio), int(height * ratio))\n",
    "#             images[i] = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "#     # Calculate total frames and duration per frame\n",
    "#     num_images = len(images)\n",
    "#     frames_per_image = total_duration * 1000 // (num_images + (num_images - 1) * transition_frames)\n",
    "#     frame_duration = int(frames_per_image)\n",
    "\n",
    "#     all_frames = []\n",
    "#     for i in range(num_images - 1):\n",
    "#         all_frames.append(images[i])\n",
    "#         for t in range(1, transition_frames + 1):\n",
    "#             alpha = t / (transition_frames + 1)\n",
    "#             blend_frame = Image.blend(images[i], images[i + 1], alpha)\n",
    "#             all_frames.append(blend_frame)\n",
    "\n",
    "#     all_frames.append(images[-1])\n",
    "\n",
    "#     # Save all frames as a GIF\n",
    "#     all_frames[0].save(\n",
    "#         output_gif,\n",
    "#         save_all=True,\n",
    "#         append_images=all_frames[1:],\n",
    "#         duration=frame_duration,\n",
    "#         loop=0\n",
    "#     )\n",
    "\n",
    "def create_animated_gif(image_folder, output_gif, total_duration=5, transition_frames=10, reverse_order=False):\n",
    "    \"\"\"\n",
    "    Create an animated GIF with smooth transitions and precise duration control.\n",
    "\n",
    "    Parameters:\n",
    "    - image_folder: Path to the folder containing images (named with leading numbers, e.g., 1_*.png, 2_*.png).\n",
    "    - output_gif: Path for the output GIF.\n",
    "    - total_duration: Total duration of the animation in seconds.\n",
    "    - transition_frames: Number of intermediate frames for transitions between images.\n",
    "    - reverse_order: If True, creates the GIF in reverse order.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    from PIL import Image\n",
    "    import re\n",
    "\n",
    "    # Load images sorted by the leading number in filenames\n",
    "    images = sorted(\n",
    "        [Image.open(os.path.join(image_folder, img)) for img in os.listdir(image_folder) if img.endswith(\".png\")],\n",
    "        key=lambda x: int(re.match(r\"(\\d+)\", os.path.basename(x.filename)).group(1))\n",
    "    )\n",
    "\n",
    "    # Reverse the order if requested\n",
    "    if reverse_order:\n",
    "        images.reverse()\n",
    "\n",
    "    # Resize images to maintain aspect ratio\n",
    "    max_width, max_height = 800, 1080\n",
    "    for i, img in enumerate(images):\n",
    "        width, height = img.size\n",
    "        if width > max_width or height > max_height:\n",
    "            ratio = min(max_width / width, max_height / height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            images[i] = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "    # Total frames calculation\n",
    "    num_images = len(images)\n",
    "    total_frames = num_images + (num_images - 1) * transition_frames\n",
    "\n",
    "    # Frame duration in milliseconds\n",
    "    frame_duration = (total_duration * 1000) // total_frames\n",
    "\n",
    "    all_frames = []\n",
    "    durations = []\n",
    "\n",
    "    for i in range(num_images - 1):\n",
    "        # Add the current image as a static frame\n",
    "        all_frames.append(images[i])\n",
    "        durations.append(frame_duration)\n",
    "\n",
    "        # Create transition frames\n",
    "        for t in range(1, transition_frames + 1):\n",
    "            alpha = t / (transition_frames + 1)\n",
    "            blend_frame = Image.blend(images[i], images[i + 1], alpha)\n",
    "            all_frames.append(blend_frame)\n",
    "            durations.append(frame_duration)\n",
    "\n",
    "    # Add the final image\n",
    "    all_frames.append(images[-1])\n",
    "    durations.append(frame_duration)\n",
    "\n",
    "    # Save all frames as a GIF\n",
    "    all_frames[0].save(\n",
    "        output_gif,\n",
    "        save_all=True,\n",
    "        append_images=all_frames[1:],\n",
    "        duration=durations,\n",
    "        # loop=0  # Infinite loop\n",
    "        loop=1  # Loop once\n",
    "    )\n",
    "\n",
    "\n",
    "# # image_folder = \"/path/to/your/image/folder\"\n",
    "output_gif_reverse = os.path.join('..', 'TEMP', 'IMAGES', 'stich_folder', 'scorigami_all_time_reverse_animated.gif')\n",
    "output_gif = os.path.join('..', 'TEMP', 'IMAGES', 'stich_folder', 'scorigami_all_time_animated.gif')\n",
    "# total_duration = 20  # Total duration of the animation in seconds\n",
    "# transition_frames = 15  # Number of fade frames per transition\n",
    "\n",
    "\n",
    "\n",
    "## Reverse Order\n",
    "create_animated_gif(image_folder, output_gif_reverse, total_duration=10, transition_frames=0, reverse_order=False)\n",
    "\n",
    "## Reverse Order\n",
    "create_animated_gif(image_folder, output_gif, total_duration=10, transition_frames=0, reverse_order=True)\n",
    "\n",
    "\n",
    "# # Example Usage\n",
    "# # Set the folder containing images and output GIF path\n",
    "\n",
    "\n",
    "# create_animated_gif(image_folder, output_gif, total_duration, transition_frames)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load Schedult/results data and compare conferences\n",
    "path = os.path.join('..', 'data', 'schedule', 'Week 1 Scores.csv')\n",
    "\n",
    "# Load the data\n",
    "schedule_df = pd.read_csv(path)\n",
    "\n",
    "# filter out exhibition games\n",
    "schedule_df = schedule_df[schedule_df['Conference'] != 'Exhibition']\n",
    "# Clean up Team names (remove ' and periods)\n",
    "schedule_df['Away_Team'] = schedule_df['Away_Team'].str.replace(\"'\", \"\").str.replace(\".\", \"\")\n",
    "schedule_df['Home_Team'] = schedule_df['Home_Team'].str.replace(\"'\", \"\").str.replace(\".\", \"\")\n",
    "# strip leading and trailing spaces\n",
    "schedule_df['Away_Team'] = schedule_df['Away_Team'].str.strip()\n",
    "schedule_df['Home_Team'] = schedule_df['Home_Team'].str.strip()\n",
    "# Drop any rows containing a / or TBD\n",
    "schedule_df = schedule_df[~schedule_df['Away_Team'].str.contains('/')]\n",
    "schedule_df = schedule_df[~schedule_df['Home_Team'].str.contains('/')]\n",
    "schedule_df = schedule_df[~schedule_df['Away_Team'].str.contains('TBD')]\n",
    "schedule_df = schedule_df[~schedule_df['Home_Team'].str.contains('TBD')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the conferences\n",
    "conference_teams = {\n",
    "    'Atlantic': ['Air Force', \"American Intl\", 'Army', 'Bentley', 'Canisius', 'Holy Cross', 'Mercyhurst', \n",
    "                 'Niagara', 'RIT', 'Robert Morris', 'Sacred Heart'],\n",
    "    'Big Ten': ['Michigan', 'Michigan State', 'Minnesota', 'Notre Dame', 'Ohio State', 'Penn State', 'Wisconsin'],\n",
    "    'CCHA': ['Augustana', 'Bemidji State', 'Bowling Green', 'Ferris State', 'Lake Superior', 'Michigan Tech', \n",
    "             'Minnesota State', 'Northern Michigan', 'St Thomas'],\n",
    "    'ECAC': ['Brown', 'Clarkson', 'Colgate', 'Cornell', 'Dartmouth', 'Harvard', 'Princeton', 'Quinnipiac',\n",
    "             'Rensselaer', 'St Lawrence', 'Union', 'Yale'],\n",
    "    'Hockey East': ['Boston College', 'Boston University', 'Connecticut', 'Maine', 'Massachusetts', 'Mass Lowell',\n",
    "                    'Merrimack', 'New Hampshire', 'Northeastern', 'Providence', 'Vermont'],\n",
    "    'NCHC': ['Arizona State', 'Colorado College', 'Denver', 'Miami', 'Minnesota Duluth', 'North Dakota', 'Omaha', \n",
    "             'St Cloud State', 'Western Michigan'],\n",
    "    'Independents': ['Alaska Anchorage', 'Alaska', 'Lindenwood', 'Long Island', 'Stonehill']\n",
    "}\n",
    "\n",
    "# Function to get the conference of a team\n",
    "def get_conference(team):\n",
    "    for conference, teams in conference_teams.items():\n",
    "        if team in teams:\n",
    "            return conference\n",
    "    return 'Unknown'  # For teams not in the provided lists\n",
    "\n",
    "# Add columns for conference of both the away and home teams\n",
    "schedule_df['Away_Conference'] = schedule_df['Away_Team'].apply(get_conference)\n",
    "schedule_df['Home_Conference'] = schedule_df['Home_Team'].apply(get_conference)\n",
    "\n",
    "# Drop rows with Unknown conferences - Stonehill and Long Island annonmaly\n",
    "schedule_df = schedule_df[schedule_df['Away_Conference'] != 'Unknown']\n",
    "schedule_df = schedule_df[schedule_df['Home_Conference'] != 'Unknown']\n",
    "\n",
    "# Rename to completed_games_df\n",
    "completed_games_df = schedule_df\n",
    "\n",
    "# Matrix for away team wins\n",
    "away_wins_matrix = pd.crosstab(index=completed_games_df['Away_Conference'],\n",
    "                               columns=completed_games_df['Home_Conference'],\n",
    "                               values=(completed_games_df['Away_Score'] > completed_games_df['Home_Score']).astype(int),\n",
    "                               aggfunc='sum', dropna=False)\n",
    "\n",
    "# Matrix for home team wins\n",
    "home_wins_matrix = pd.crosstab(index=completed_games_df['Home_Conference'],\n",
    "                               columns=completed_games_df['Away_Conference'],\n",
    "                               values=(completed_games_df['Home_Score'] > completed_games_df['Away_Score']).astype(int),\n",
    "                               aggfunc='sum', dropna=False)\n",
    "\n",
    "# Transpose the home wins matrix so that it aligns with the away wins matrix for summation\n",
    "home_wins_matrix = home_wins_matrix.T\n",
    "\n",
    "# Sum both matrices to get the total wins\n",
    "total_wins_matrix = away_wins_matrix.add(home_wins_matrix, fill_value=0)\n",
    "# total_wins_matrix = total_wins_matrix.astype(int) # Convert to integers\n",
    "\n",
    "\n",
    "# Display the results matrix\n",
    "print(total_wins_matrix)\n",
    "# calculate and print the total number of games played\n",
    "total_games = total_wins_matrix.sum().sum()\n",
    "print(f'Total games played: {total_games}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find the team without a conference\n",
    "# # Display rows with 'Unknown' in either column\n",
    "# unknown_teams = schedule_df[(schedule_df['Away_Conference'] == 'Unknown') | (schedule_df['Home_Conference'] == 'Unknown')]\n",
    "# print(len(unknown_teams))  # Number of rows with unknown teams\n",
    "\n",
    "# # value count of unknown teams\n",
    "# print(unknown_teams['Away_Team'].value_counts())\n",
    "# print(unknown_teams['Home_Team'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join 2023 Player Stats to 2024 Rosters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "## Path to the data\n",
    "roster_path = os.path.join(\"..\", \"data\", \"roster_2024_current_v3.csv\")\n",
    "stat_path = os.path.join(\"..\", \"data\", \"player_stats_2023_v1.csv\")\n",
    "\n",
    "# Load the data\n",
    "roster_df = pd.read_csv(roster_path)\n",
    "stat_df = pd.read_csv(stat_path)\n",
    "\n",
    "# Check the data\n",
    "roster_df.head()\n",
    "# stat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split stats Clean_Player into first and last name\n",
    "stat_df['First_Name'] = stat_df['Clean_Player'].str.split(\" \").str[0]\n",
    "stat_df['Last_Name'] = stat_df['Clean_Player'].str.split(\" \").str[1:]\n",
    "\n",
    "\n",
    "stat_df['Last_Name'] = stat_df['Last_Name'].str[0].str.replace('[','').str.replace(']','') # Remove the brackets from the last name\n",
    "# Remove periods dashes ect from both names\n",
    "stat_df['First_Name'] = stat_df['First_Name'].str.replace('.','').str.replace('-',' ')\n",
    "stat_df['Last_Name'] = stat_df['Last_Name'].str.replace('.','').str.replace('-',' ')\n",
    "roster_df['First_Name'] = roster_df['First_Name'].str.replace('.','').str.replace('-',' ')\n",
    "roster_df['Last_Name'] = roster_df['Last_Name'].str.replace('.','').str.replace('-',' ')\n",
    "# strip white space\n",
    "stat_df['First_Name'] = stat_df['First_Name'].str.strip()\n",
    "stat_df['Last_Name'] = stat_df['Last_Name'].str.strip()\n",
    "roster_df['First_Name'] = roster_df['First_Name'].str.strip()\n",
    "roster_df['Last_Name'] = roster_df['Last_Name'].str.strip()\n",
    "\n",
    "# Rename Team to Team_2023 for clarity\n",
    "stat_df.rename(columns={'Team':'Team_2023'}, inplace=True)\n",
    "# Rename Current_Team to Team_2024 for clarity\n",
    "roster_df.rename(columns={'Current Team':'Team_2024'}, inplace=True)\n",
    "\n",
    "stat_df.head()\n",
    "# OUTPUT THE DATA TO TEMP CSVs\n",
    "roster_df.to_csv(os.path.join(\"..\", \"TEMP\", \"TEST_roster_2024_current_v4.csv\"), index=False)\n",
    "stat_df.to_csv(os.path.join(\"..\", \"TEMP\", \"TEST_player_stats_2023_v2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try a quick merge\n",
    "merged_df = pd.merge(roster_df, stat_df, left_on=['First_Name', 'Last_Name'], right_on=['First_Name', 'Last_Name'], how='outer', suffixes=('_2024', '_2023'))\n",
    "merged_df.head()\n",
    "\n",
    "# Print report of the merge\n",
    "print(f\"Number of players in the roster: {len(roster_df)}\")\n",
    "print(f\"Number of players in the stats: {len(stat_df)}\")\n",
    "print(f\"Number of players in the merged data: {len(merged_df)}\")\n",
    "\n",
    "\n",
    "# Find Number Number of players whos Team_2023 does not match Team_2024\n",
    "mismatched_teams = merged_df[merged_df['Team_2023'] != merged_df['Team_2024']]\n",
    "print(f\"Number of players with mismatched teams: {len(mismatched_teams)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(merged_df.info())\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop players who aren't playing this year (No Team_2024)\n",
    "merged_df = merged_df.dropna(subset=['Team_2024'])\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert all number columns to int\n",
    "int_columns = ['No', 'Height_Inches', 'Wt', 'Draft_Year', 'D_Round', \n",
    "               'G', 'A', 'Pts', 'plus_minus', 'Sh', 'PIM', 'Games_Played']\n",
    "\n",
    "for col in int_columns:\n",
    "    merged_df[col] = merged_df[col].astype('Int64')\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## OUTPUT CSV TO TEMP FOR INSPECTION\n",
    "merged_df.to_csv(os.path.join(\"..\", \"data\", \"roster_2024_with_2023_stats.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform congressional demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import geopandas as gpd\n",
    "\n",
    "\n",
    "# ## PATHS ##\n",
    "# # ## 118 Congress Shapefile\n",
    "# # shape_path = os.path.join('..', 'data', 'vault', '118th_congress', 'USA_118th_Congressional_Districts.shp')\n",
    "# # ## Load Shapefile\n",
    "# # gdf = gpd.read_file(shape_path)\n",
    "\n",
    "# # Income data table - 5 Year Average 2022\n",
    "# income_path = os.path.join('..', 'data', 'vault', '118th_congress', 'income_data', 'ACSST5Y2022.S1903-Data.csv')\n",
    "# income_df = pd.read_csv(income_path, skiprows=1) # Load Income Data\n",
    "\n",
    "# # Summary table with Populations and Representative Names\n",
    "# summary_path = os.path.join('..', 'data', 'vault', 'USA_118th_Congressional_Districts_info_table.csv')\n",
    "# summary_df = pd.read_csv(summary_path)\n",
    "\n",
    "# # Check \n",
    "# # gdf.head()\n",
    "# # income_df.head()\n",
    "# # summary_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check \n",
    "# gdf.head()\n",
    "# income_df.head()\n",
    "# # summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulate Image icons so they are all 300 x 300 px squares\n",
    "- making sure they are all squares will make resizing issues easier later on\n",
    "    - The aspect ratio is getting screwed up during resizing for icons that are not square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "\n",
    "# Directory where the logos are stored\n",
    "logo_dir = os.path.join('..', 'images', 'logos')\n",
    "\n",
    "# Make logos square by adding transparent space equally on both sides\n",
    "for logo_file in os.listdir(logo_dir):\n",
    "    logo_path = os.path.join(logo_dir, logo_file)\n",
    "    \n",
    "    # Check if the path is a file and not a directory\n",
    "    if os.path.isfile(logo_path):\n",
    "        with Image.open(logo_path) as img:\n",
    "            # Ensure the image has an alpha channel (for transparency)\n",
    "            img = img.convert(\"RGBA\")\n",
    "            \n",
    "            width, height = img.size\n",
    "            \n",
    "            # If the image is already square, no changes are needed\n",
    "            if width == height:\n",
    "                continue\n",
    "            \n",
    "            # Calculate padding to add on the shorter side to make the image square\n",
    "            if width > height:\n",
    "                padding = (width - height) // 2\n",
    "                new_img = ImageOps.expand(img, border=(0, padding, 0, padding), fill=(0, 0, 0, 0))\n",
    "            else:\n",
    "                padding = (height - width) // 2\n",
    "                new_img = ImageOps.expand(img, border=(padding, 0, padding, 0), fill=(0, 0, 0, 0))\n",
    "            \n",
    "            # Save the padded square image, overwriting the original\n",
    "            new_img.save(logo_path)\n",
    "\n",
    "print(\"All logos made square by adding transparent space equally to each side.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the coordinates for the rinks in arena_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dependencies\n",
    "# import os\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Path to arena file\n",
    "# arena_file = os.path.join('..','data', 'arena_school_info.csv')\n",
    "# arena_df = pd.read_csv(arena_file)\n",
    "\n",
    "# # Open Roster File To Clean State/Provences Names\n",
    "# roster_file = os.path.join('..','data', 'roster_2024_current_v2.csv')\n",
    "# roster_df = pd.read_csv(roster_file)\n",
    "\n",
    "# roster_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Get list of Unique State/Province Names\n",
    "# unique_states = roster_df['State_Province'].unique()\n",
    "# unique_states\n",
    "\n",
    "# ## Dictionary to standardize state/province names\n",
    "\n",
    "# standardized_locations = {\n",
    "#     'Ont.': 'Ontario', 'Mich.': 'Michigan', 'Mass.': 'Massachusetts', 'Minn.': 'Minnesota', \n",
    "#     'Wis.': 'Wisconsin', 'Sweden': 'Sweden', 'Germany': 'Germany', 'B.C.': 'British Columbia',\n",
    "#     'N.Y.': 'New York', 'Wash.': 'Washington', 'Que.': 'Quebec', 'Alb.': 'Alberta', \n",
    "#     'N.J.': 'New Jersey', 'Sask.': 'Saskatchewan', 'Conn.': 'Connecticut', 'Mo.': 'Missouri',\n",
    "#     'Texas': 'Texas', 'Calif.': 'California', 'DC': 'District of Columbia', 'Fla.': 'Florida',\n",
    "#     'Ohio': 'Ohio', 'Ill.': 'Illinois', 'Pa.': 'Pennsylvania', 'Ga.': 'Georgia',\n",
    "#     'Mont.': 'Montana', 'Tenn.': 'Tennessee', 'Colo.': 'Colorado', 'Va.': 'Virginia', \n",
    "#     'Vt.': 'Vermont', 'R.I.': 'Rhode Island', 'Md.': 'Maryland', 'Ariz.': 'Arizona', \n",
    "#     'Wisc.': 'Wisconsin', 'Iowa': 'Iowa', 'Man.': 'Manitoba', 'Slovakia': 'Slovakia', \n",
    "#     'N.D.': 'North Dakota', 'N.C.': 'North Carolina', 'P.E.I.': 'Prince Edward Island',\n",
    "#     'N.H.': 'New Hampshire', 'Alaska': 'Alaska', 'Belarus': 'Belarus', 'MB': 'Manitoba',\n",
    "#     'Russia': 'Russia', 'Finland': 'Finland', 'Newf.': 'Newfoundland and Labrador', \n",
    "#     'Hungary': 'Hungary', 'SUI': 'Switzerland', 'S.C.': 'South Carolina', 'Latvia': 'Latvia',\n",
    "#     'Czech Republic': 'Czech Republic', 'N.B.': 'New Brunswick', 'Great Britain': 'United Kingdom', \n",
    "#     'NB': 'New Brunswick', 'Norway': 'Norway', 'N.S.': 'Nova Scotia', 'Ind.': 'Indiana', \n",
    "#     'NWT': 'Northwest Territories', 'AUT': 'Austria', 'Idaho': 'Idaho', 'S.D.': 'South Dakota', \n",
    "#     'Switzerland': 'Switzerland', 'Ore.': 'Oregon', 'Wyo.': 'Wyoming', 'Utah': 'Utah', \n",
    "#     'ITA': 'Italy', 'Slovenia': 'Slovenia', 'YT': 'Yukon', 'Del.': 'Delaware', 'Maine': 'Maine',\n",
    "#     'Poland': 'Poland', 'Yukon': 'Yukon', 'Ukraine': 'Ukraine', 'Japan': 'Japan', 'Neb.': 'Nebraska'\n",
    "# }\n",
    "\n",
    "# ## Apply the standardization to the State/Province column\n",
    "# roster_df['State_Province'] = roster_df['State_Province'].replace(standardized_locations)\n",
    "\n",
    "# # Check the unique values after standardization\n",
    "# roster_df['State_Province'].unique()\n",
    "# print(roster_df['State_Province'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output the cleaned roster to a new CSV file\n",
    "cleaned_roster_file = os.path.join('..','data', 'roster_cleaned_state_prov_2024.csv')\n",
    "roster_df.to_csv(cleaned_roster_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the function to check the location using Google Places API\n",
    "# def check_location(lat, lng, api_key):\n",
    "#     url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "#     params = {\n",
    "#         'location': f'{lat},{lng}',\n",
    "#         'radius': 500,  # Distance in meters from the provided coordinates\n",
    "#         'type': 'stadium',  # Filter search to stadiums/arenas\n",
    "#         'key': api_key\n",
    "#     }\n",
    "    \n",
    "#     # Debugging: Print the URL and parameters\n",
    "#     print(f\"Requesting URL: {url}\")\n",
    "#     print(f\"Parameters: {params}\")\n",
    "    \n",
    "#     response = requests.get(url, params=params)\n",
    "    \n",
    "#     # Debugging: Print the response code and content\n",
    "#     print(f\"Response status code: {response.status_code}\")\n",
    "#     print(f\"Response content: {response.text}\")\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         results = response.json().get('results')\n",
    "#         if results:\n",
    "#             return results[0].get('name'), results[0].get('vicinity')\n",
    "#         else:\n",
    "#             return None, \"No results found\"\n",
    "#     else:\n",
    "#         return None, f\"API request failed with status {response.status_code}\"\n",
    "\n",
    "# # Define the function to verify coordinates in the DataFrame\n",
    "# def verify_coordinates(df, api_key):\n",
    "#     results = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         lat = row['Latitude']\n",
    "#         lng = row['Longitude']\n",
    "#         arena_name = row['Arena']\n",
    "        \n",
    "#         # Debugging: Print the current coordinates and arena being checked\n",
    "#         print(f\"Checking coordinates for arena: {arena_name}\")\n",
    "#         print(f\"Latitude: {lat}, Longitude: {lng}\")\n",
    "        \n",
    "#         # Get the name and vicinity of the nearest stadium/arena\n",
    "#         name, vicinity = check_location(lat, lng, api_key)\n",
    "        \n",
    "#         # Append the original data and verification results\n",
    "#         results.append({\n",
    "#             'Arena': arena_name,\n",
    "#             'Latitude': lat,\n",
    "#             'Longitude': lng,\n",
    "#             'Google Places Name': name,\n",
    "#             'Vicinity': vicinity\n",
    "#         })\n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# # Load your API key\n",
    "\n",
    "\n",
    "# # Assuming arena_df is your DataFrame\n",
    "# verified_df = verify_coordinates(arena_df, api_key)\n",
    "\n",
    "# # Output the results\n",
    "# print(verified_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verified_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Version 2 of Arena Location verifications\n",
    "# ## Returns 5 closest Google Places to coordinates given\n",
    "\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# # Define the function to check the 5 closest places using Google Places API\n",
    "# def check_nearby_places(lat, lng, api_key):\n",
    "#     url = \"https://maps.googleapis.com/maps/api/place/nearbysearch/json\"\n",
    "#     params = {\n",
    "#         'location': f'{lat},{lng}',\n",
    "#         'radius': 500,  # Distance in meters from the provided coordinates\n",
    "#         'key': api_key\n",
    "#     }\n",
    "    \n",
    "#     # Debugging: Print the URL and parameters being sent to the API\n",
    "#     print(f\"Requesting places near lat: {lat}, lng: {lng}\")\n",
    "#     print(f\"Request URL: {url}\")\n",
    "#     print(f\"Parameters: {params}\")\n",
    "    \n",
    "#     response = requests.get(url, params=params)\n",
    "    \n",
    "#     # Debugging: Print the response status and content\n",
    "#     print(f\"Response status code: {response.status_code}\")\n",
    "#     print(f\"Response content: {response.text}\\n\")  # This shows the full response from the API\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         results = response.json().get('results')\n",
    "#         if results:\n",
    "#             # Return the top 5 closest places\n",
    "#             return [(result.get('name'), result.get('vicinity')) for result in results[:5]]\n",
    "#         else:\n",
    "#             return [(\"None\", \"No results found\")]\n",
    "#     else:\n",
    "#         return [(\"None\", f\"API request failed with status {response.status_code}\")]\n",
    "\n",
    "# # Define the function to verify coordinates and return the 5 closest places\n",
    "# def verify_coordinates(df, api_key):\n",
    "#     results = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         lat = row['Latitude']\n",
    "#         lng = row['Longitude']\n",
    "#         arena_name = row['Arena']\n",
    "#         school_name = row['School']\n",
    "        \n",
    "#         # Debugging: Print the current arena and coordinates being checked\n",
    "#         print(f\"\\nChecking nearby places for arena: {arena_name} (School: {school_name})\")\n",
    "#         print(f\"Latitude: {lat}, Longitude: {lng}\")\n",
    "        \n",
    "#         # Get the 5 closest places\n",
    "#         nearby_places = check_nearby_places(lat, lng, api_key)\n",
    "        \n",
    "#         # Add each place to the results, along with the original data\n",
    "#         for place in nearby_places:\n",
    "#             results.append({\n",
    "#                 'Arena': arena_name,\n",
    "#                 'School': school_name,\n",
    "#                 'Latitude': lat,\n",
    "#                 'Longitude': lng,\n",
    "#                 'Google Places Name': place[0],\n",
    "#                 'Vicinity': place[1]\n",
    "#             })\n",
    "            \n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# # Load your API key\n",
    "# api_key = ''\n",
    "\n",
    "# # Assuming arena_df is your DataFrame\n",
    "# verified_df = verify_coordinates(arena_df, api_key)\n",
    "\n",
    "# # Output the results\n",
    "# print(verified_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_df.head(10)\n",
    "\n",
    "## OUTPUT TO TEMP FOLDER FOR MANUAL REVIEW\n",
    "# output_file = os.path.join('..','TEMP', 'arena_school_info_place_checkV3.csv')\n",
    "# verified_df.to_csv(output_file, index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
