{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Game Data Scraper & Cleaner\n",
    "- created 12/9/24 \n",
    "- refactor of legacy code from Game_Data_Scraper_1 and Game_Data_Cleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-09 19:48:32,380 - INFO - Logging Started\n"
     ]
    }
   ],
   "source": [
    "## Dependencies\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os \n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "# Create timestamp string to use in file names\n",
    "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "## FILE PATHS and CONSTANTS\n",
    "#Local Folder paths\n",
    "temp_folder = os.path.join('..', 'TEMP')\n",
    "data_folder = os.path.join('..', 'data')\n",
    "db_folder = os.path.join(data_folder, 'db')\n",
    "log_folder = os.path.join(temp_folder, 'logs')\n",
    "\n",
    "# Check Paths\n",
    "if not os.path.exists(temp_folder):\n",
    "    os.makedirs(temp_folder)\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "if not os.path.exists(db_folder):\n",
    "    os.makedirs(db_folder)\n",
    "\n",
    "# Remote URL\n",
    "base_url = 'https://www.collegehockeynews.com' ## Base usl for box scores and metrics\n",
    "current_year_url = 'https://www.collegehockeynews.com/schedules/?season=20242025' ## Current year schedule\n",
    "\n",
    "\n",
    "## Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('Logging Started')\n",
    "\n",
    "# Database Name\n",
    "DB_FILE_NAME = 'CHN_2024_YTD_Stats.db'\n",
    "# DB_FILE_NAME = '2024_Dec_03_CLEAN.db'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate DB Connection\n",
    "- backup and open DB file if it exists\n",
    "- create new DB file with DB_FILE_NAME if none exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Database Path\n",
    "db_path = os.path.join(db_folder, DB_FILE_NAME)\n",
    "# Create Backup in TEMP folder with datetime stamp\n",
    "backup_db_path = os.path.join(temp_folder, f\"{DB_FILE_NAME}_{timestamp}.backup\")\n",
    "# backup_db_path = os.path.join(temp_folder, f\"{DB_FILE_NAME}.backup\")\n",
    "\n",
    "# Check if the database file exists\n",
    "if os.path.exists(db_path):\n",
    "    # Backup the existing database\n",
    "    logger.info(f\"Database file found at {db_path}. Backing it up to {backup_db_path}.\")\n",
    "    try:\n",
    "        # Perform the backup\n",
    "        with open(db_path, 'rb') as original_db, open(backup_db_path, 'wb') as backup_db:\n",
    "            backup_db.write(original_db.read())\n",
    "        logger.info(f\"Backup successful: {backup_db_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to back up the database: {e}\")\n",
    "else:\n",
    "    logger.info(f\"No database file found at {db_path}. A new database will be created.\")\n",
    "\n",
    "# Create a database connection\n",
    "try:\n",
    "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    logger.info(f\"Database connection established at {db_path}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to establish database connection: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Dictionary of Team names and abbreviations\n",
    "- from arena_school_info table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load school infomation from arena_school_info.csv\n",
    "school_info_df = pd.read_csv(os.path.join(data_folder, 'arena_school_info.csv'))\n",
    "\n",
    "# Create a dictionary for abbreviations to full team names\n",
    "abbreviation_to_fullname = school_info_df.set_index('abv')['School'].to_dict()\n",
    "\n",
    "# Define a function to replace abbreviations in a column with full team names\n",
    "def replace_abbreviations_with_fullnames(df, column_name, abbreviation_dict):\n",
    "    \"\"\"\n",
    "    Replaces abbreviations in the specified column of a DataFrame with full team names.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the column to process.\n",
    "        column_name (str): The column name where abbreviations need to be replaced.\n",
    "        abbreviation_dict (dict): Dictionary mapping abbreviations to full names.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with updated column values.\n",
    "    \"\"\"\n",
    "    df[column_name] = df[column_name].replace(abbreviation_dict)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a table of every game in CHN database for the selected season\n",
    "- output games_df table of every game listed on CHN site\n",
    "- Clean team names of unwanted characters and create unique Game_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to Parse the Current Season Schedule / Results Page\n",
    "def parse_current_season(url):\n",
    "    \"\"\"\n",
    "    Parses the current season schedule/results page.\n",
    "    Args:\n",
    "        url (str): URL of the current season schedule/results page.\n",
    "\n",
    "    Returns:\n",
    "        list: Parsed data as a list of rows.\n",
    "    \"\"\"\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # Initialize variables\n",
    "    current_date, current_conference, game_notes = None, None, None\n",
    "    data = []  # List to store game data\n",
    "\n",
    "    # Fetch the page\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to retrieve data from {url}, status code {response.status_code}\")\n",
    "\n",
    "    # Parse the page with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    rows = soup.find_all('tr')\n",
    "\n",
    "    # Process each row\n",
    "    for row in rows:\n",
    "        row_class = row.get('class')\n",
    "        \n",
    "        if row_class == ['stats-section']:  # Date row\n",
    "            current_date = row.find('td').text.strip()\n",
    "        elif row_class == ['sked-header']:  # Conference row\n",
    "            current_conference = row.find('td').text.strip()\n",
    "        elif len(row.find_all('td')) == 2:  # Game notes row\n",
    "            game_notes = row.find_all('td')[1].text.strip()\n",
    "        elif row.get('valign') == 'top':  # Game data row\n",
    "            game_data = extract_game_data(row, current_date, current_conference, game_notes)\n",
    "            if game_data:\n",
    "                data.append(game_data)\n",
    "            game_notes = None  # Reset game notes for the next row\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_game_data(row, current_date, current_conference, game_notes):\n",
    "    \"\"\"\n",
    "    Extracts game data from a table row.\n",
    "    Args:\n",
    "        row (Tag): BeautifulSoup row tag.\n",
    "        current_date (str): Current date of the game.\n",
    "        current_conference (str): Current conference of the game.\n",
    "        game_notes (str): Notes for the game.\n",
    "\n",
    "    Returns:\n",
    "        list: Extracted game data or None if row is invalid.\n",
    "    \"\"\"\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) < 9:\n",
    "        return None\n",
    "\n",
    "    # Extract data\n",
    "    home_team = clean_team_name(cells[3].text.strip())\n",
    "    home_team_link = cells[3].find('a')['href'] if cells[3].find('a') else None\n",
    "    home_score = cells[4].text.strip()\n",
    "\n",
    "    # home_team = clean_team_name(cells[0].text.strip())\n",
    "    # home_team_link = cells[0].find('a')['href'] if cells[0].find('a') else None\n",
    "    # home_score = cells[1].text.strip()\n",
    "\n",
    "    away_team = clean_team_name(cells[0].text.strip())\n",
    "    away_team_link = cells[0].find('a')['href'] if cells[0].find('a') else None\n",
    "    away_score = cells[1].text.strip()\n",
    "\n",
    "    ot = cells[5].text.strip()\n",
    "    box_link = cells[7].find('a')['href'] if cells[7].find('a') else None\n",
    "    metrics_link = cells[8].find('a')['href'] if cells[8].find('a') else None\n",
    "\n",
    "    # Optional game notes\n",
    "    game_notes_cell = cells[-1].find('small')\n",
    "    game_notes = game_notes_cell.text.strip() if game_notes_cell else game_notes\n",
    "\n",
    "    return [\n",
    "        current_date, current_conference, game_notes,\n",
    "        away_team, away_team_link, away_score,\n",
    "        home_team, home_team_link, home_score,\n",
    "        \n",
    "        ot, box_link, metrics_link\n",
    "    ]\n",
    "\n",
    "\n",
    "def clean_team_name(team_name):\n",
    "    \"\"\"\n",
    "    Cleans the team name by removing unwanted characters.\n",
    "    Args:\n",
    "        team_name (str): Team name.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned team name.\n",
    "    \"\"\"\n",
    "    # Replace unwanted characters with an empty string\n",
    "    return team_name.replace('-', ' ').replace('.', '').replace(\"'\", '').strip()\n",
    "\n",
    "\n",
    "## Call the function\n",
    "parsed_data = parse_current_season(current_year_url)\n",
    "\n",
    "## Create a DataFrame from the parsed data\n",
    "columns = [\n",
    "    'Date', 'Conference', 'Game_Notes', 'Away_Team', 'Away_Team_Link', 'Away_Score',\n",
    "    'Home_Team', 'Home_Team_Link', 'Home_Score',  'OT',\n",
    "    'Box_Link', 'Metrics_Link'\n",
    "]\n",
    "df = pd.DataFrame(parsed_data, columns=columns)\n",
    "\n",
    "## Process the DataFrame\n",
    "# Extract the day of the week and reformat the date\n",
    "df['Day'] = pd.to_datetime(df['Date']).dt.day_name()\n",
    "df['Date'] = pd.to_datetime(df['Date']).dt.strftime('%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_game_id(row):\n",
    "    \"\"\"\n",
    "    Generates a unique Game_ID based on the date, home team, and away team.\n",
    "    Args:\n",
    "        row (pd.Series): A row of the DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        str: A unique Game_ID.\n",
    "    \"\"\"\n",
    "    return f'{row.Date}-{row.Away_Team}-{row.Home_Team}'\n",
    "\n",
    "\n",
    "def clean_column_data(df):\n",
    "    \"\"\"\n",
    "    Cleans team names and ensures consistent formatting for the DataFrame columns.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    # Remove hyphens from team names\n",
    "    df['Home_Team'] = df['Home_Team'].str.replace('-', ' ', regex=False)\n",
    "    df['Away_Team'] = df['Away_Team'].str.replace('-', ' ', regex=False)\n",
    "\n",
    "    # Filter out games that haven't been played yet\n",
    "    df = df[df['Home_Score'] != '']\n",
    "\n",
    "    # Replace NaN values in Metrics_Link with an empty string\n",
    "    df['Metrics_Link'] = df['Metrics_Link'].fillna('')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply the cleaning function to the DataFrame\n",
    "df = clean_column_data(df)\n",
    "\n",
    "# Generate Game_ID column\n",
    "df['Game_ID'] = df.apply(generate_game_id, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare game results table to games already in the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def filter_and_compare_games(df, conn, recent_days=5):\n",
    "    \"\"\"\n",
    "    Compares the Game_ID in the extracted dataframe to the database, filters exhibition games,\n",
    "    and prints a summary of game counts. Handles cases where the database is empty or does not exist.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Extracted games dataframe.\n",
    "        conn (sqlite3.Connection): Database connection.\n",
    "        recent_days (int): Number of recent days to re-scrape games for updates.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame of games to scrape.\n",
    "    \"\"\"\n",
    "    import datetime\n",
    "\n",
    "    # Print initial game count\n",
    "    total_games = len(df)\n",
    "    print(f\"Total games in the extracted dataset: {total_games}\")\n",
    "\n",
    "    # Filter out exhibition games\n",
    "    df = df[df['Conference'] != 'Exhibition']\n",
    "    filtered_games = len(df)\n",
    "    print(f\"Games remaining after filtering exhibition games: {filtered_games}\")\n",
    "\n",
    "    # Check if the database has any data in the game_details table\n",
    "    try:\n",
    "        existing_game_ids_query = \"SELECT DISTINCT Game_ID FROM game_details\"\n",
    "        existing_game_ids = pd.read_sql(existing_game_ids_query, conn)['Game_ID'].tolist()\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Database access failed or no game_details table found: {e}\")\n",
    "        existing_game_ids = []  # Treat as no games in the database\n",
    "\n",
    "    # Handle empty database: scrape all games\n",
    "    if not existing_game_ids:\n",
    "        print(\"Database is empty or non-existent. Scraping all available games.\")\n",
    "        return df\n",
    "\n",
    "    # Find games not in the database\n",
    "    df['In_Database'] = df['Game_ID'].isin(existing_game_ids)\n",
    "    new_games_df = df[~df['In_Database']]\n",
    "    games_not_in_dataset = len(new_games_df)\n",
    "    print(f\"Games not in the database: {games_not_in_dataset}\")\n",
    "\n",
    "    # Add recent games to the scrape list (past `recent_days`)\n",
    "    recent_date_threshold = (datetime.datetime.now() - datetime.timedelta(days=recent_days)).strftime('%Y-%m-%d')\n",
    "    recent_games_df = df[df['Date'] >= recent_date_threshold]\n",
    "    games_to_rescrape = len(recent_games_df)\n",
    "    print(f\"Games to re-scrape from the last {recent_days} days: {games_to_rescrape}\")\n",
    "\n",
    "    # Combine new games and recent games for scraping\n",
    "    games_to_scrape_df = pd.concat([new_games_df, recent_games_df]).drop_duplicates(subset='Game_ID')\n",
    "    print(f\"Total games to scrape: {len(games_to_scrape_df)}\")\n",
    "\n",
    "    return games_to_scrape_df\n",
    "\n",
    "## ORIG CODE, GETS ERROR IF GIVEN NON EXI FILEPATH\n",
    "# def filter_and_compare_games(df, conn, recent_days=5):\n",
    "#     \"\"\"\n",
    "#     Compares the Game_ID in the extracted dataframe to the database, filters exhibition games,\n",
    "#     and prints a summary of game counts.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Extracted games dataframe.\n",
    "#         conn (sqlite3.Connection): Database connection.\n",
    "#         recent_days (int): Number of recent days to re-scrape games for updates.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame: DataFrame of games to scrape.\n",
    "#     \"\"\"\n",
    "#     import datetime\n",
    "\n",
    "#     # Print initial game count\n",
    "#     total_games = len(df)\n",
    "#     print(f\"Total games in the extracted dataset: {total_games}\")\n",
    "\n",
    "#     # Filter out exhibition games\n",
    "#     df = df[df['Conference'] != 'Exhibition']\n",
    "#     filtered_games = len(df)\n",
    "#     print(f\"Games remaining after filtering exhibition games: {filtered_games}\")\n",
    "\n",
    "#     # Retrieve existing Game_IDs from the database\n",
    "#     existing_game_ids_query = \"SELECT DISTINCT Game_ID FROM game_details\"  # Correct table name\n",
    "#     existing_game_ids = pd.read_sql(existing_game_ids_query, conn)['Game_ID'].tolist()\n",
    "#     print(f\"Total games already in the database: {len(existing_game_ids)}\")\n",
    "\n",
    "#     # Find games not in the database\n",
    "#     df['In_Database'] = df['Game_ID'].isin(existing_game_ids)\n",
    "#     new_games_df = df[~df['In_Database']]\n",
    "#     games_not_in_dataset = len(new_games_df)\n",
    "#     print(f\"Games not in the database: {games_not_in_dataset}\")\n",
    "\n",
    "#     # Add recent games to the scrape list (past `recent_days`)\n",
    "#     recent_date_threshold = (datetime.datetime.now() - datetime.timedelta(days=recent_days)).strftime('%Y-%m-%d')\n",
    "#     recent_games_df = df[df['Date'] >= recent_date_threshold]\n",
    "#     games_to_rescrape = len(recent_games_df)\n",
    "#     print(f\"Games to re-scrape from the last {recent_days} days: {games_to_rescrape}\")\n",
    "\n",
    "#     # Combine new games and recent games for scraping\n",
    "#     games_to_scrape_df = pd.concat([new_games_df, recent_games_df]).drop_duplicates(subset='Game_ID')\n",
    "#     print(f\"Total games to scrape: {len(games_to_scrape_df)}\")\n",
    "\n",
    "#     return games_to_scrape_df\n",
    "\n",
    "\n",
    "# Example usage\n",
    "games_to_scrape = filter_and_compare_games(df, conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Functions\n",
    "- currently untouched from legacy code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ##### LEGACY CODE - WORKING FROM G_D_S_1\n",
    "-  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PARSE PLAYER STATS TABLE ####\n",
    "def parse_player_summary(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find the playersums div\n",
    "    playersums_div = soup.find('div', id='playersums')\n",
    "    if playersums_div is None:\n",
    "        return \"Player summaries div not found\"\n",
    "\n",
    "    # Initialize list to store player stats\n",
    "    player_stats = []\n",
    "\n",
    "    # Loop through each playersum div\n",
    "    for player_sum in playersums_div.find_all('div', class_='playersum'):\n",
    "        team = player_sum.find('td').text.strip()\n",
    "        \n",
    "        # Loop through table rows\n",
    "        for row in player_sum.find_all('tr'):\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                player = cols[0].text.strip()\n",
    "                goals = cols[1].text.strip()\n",
    "                assists = cols[2].text.strip()\n",
    "                points = cols[3].text.strip()\n",
    "                plus_minus = cols[4].text.strip()\n",
    "                shots = cols[5].text.strip()\n",
    "                pim = cols[6].text.strip()\n",
    "                fowl = cols[7].text.strip() if len(cols) > 7 else None\n",
    "                \n",
    "                fow, fol = None, None\n",
    "                win_percentage = None\n",
    "                \n",
    "                \n",
    "\n",
    "                try:\n",
    "                    if fowl and '‑' in fowl:  # Checking if it contains a hyphen\n",
    "                        fow, fol = map(int, fowl.split('‑'))\n",
    "                        total_fo = fow + fol\n",
    "                        win_percentage = (fow / total_fo) * 100 if total_fo > 0 else 0\n",
    "                except ValueError:\n",
    "                    fow, fol, win_percentage = None, None, None\n",
    "\n",
    "                \n",
    "\n",
    "                \n",
    "                player_stat = {\n",
    "                    'Team': team,\n",
    "                    'Player': player,\n",
    "                    'G': goals,\n",
    "                    'A': assists,\n",
    "                    'Pt.': points,\n",
    "                    '+/-': plus_minus,\n",
    "                    'Sh': shots,\n",
    "                    'PIM': pim,\n",
    "                    'FOW': fow,\n",
    "                    'FOL': fol,\n",
    "                    'FO%': win_percentage\n",
    "                }\n",
    "                player_stats.append(player_stat)\n",
    "\n",
    "    return pd.DataFrame(player_stats)\n",
    "    \n",
    "\n",
    "\n",
    "############# PARSEING SCORING SUMMARY WITH BS4\n",
    "def parse_scoring_summary(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the scoring div and table\n",
    "    scoring_div = soup.find('div', id='scoring')\n",
    "    if scoring_div is None:\n",
    "        logging.error(\"Scoring div not found\")\n",
    "        return None\n",
    "\n",
    "    scoring_table = scoring_div.find('table')\n",
    "    if scoring_table is None:\n",
    "        logging.error(\"Scoring table not found within the scoring div\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store scoring events\n",
    "    scoring_events = []\n",
    "    period = None\n",
    "\n",
    "    # Loop through table rows\n",
    "    for row in scoring_table.find_all('tr'):\n",
    "        if 'stats-section' in row.get('class', []):\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                period = td.text.strip()\n",
    "            else:\n",
    "                logging.warning(\"Period name not found in 'stats-section' row\")\n",
    "                period = \"Unknown\"\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                try:\n",
    "                    team = cols[0].text.strip()\n",
    "                    team = abbreviation_to_fullname.get(team, team)  # Replace abbreviation\n",
    "                    team = clean_team_name(team)  # Clean team name\n",
    "                    pp = cols[1].text.strip()\n",
    "\n",
    "                    player_data = cols[3].text.strip()\n",
    "                    match = re.match(r\"(.+)\\s\\((\\d+)\\)\", player_data)\n",
    "                    player = match.group(1) if match else player_data\n",
    "                    goals = int(match.group(2)) if match else None\n",
    "\n",
    "                    assist_data_raw = cols[4].text.strip()\n",
    "                    assist_data = assist_data_raw.split(\", \") if assist_data_raw else []\n",
    "                    assist1 = assist_data[0] if len(assist_data) > 0 else None\n",
    "                    assist2 = assist_data[1] if len(assist_data) > 1 else None\n",
    "\n",
    "                    time = cols[5].text.strip()\n",
    "\n",
    "                    scoring_event = {\n",
    "                        'Period': period,\n",
    "                        'Team': team,\n",
    "                        'PP': pp,\n",
    "                        'Player': player,\n",
    "                        'Player_Goals': goals,\n",
    "                        'Assist1': assist1,\n",
    "                        'Assist2': assist2,\n",
    "                        'Time': time\n",
    "                    }\n",
    "                    scoring_events.append(scoring_event)\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"An error occurred while parsing a scoring event row: {e}\")\n",
    "            else:\n",
    "                logging.warning(f\"Insufficient columns in scoring row: {len(cols)}\")\n",
    "\n",
    "    return pd.DataFrame(scoring_events)\n",
    "\n",
    "\n",
    "############# PARSEING PENALTY SUMMARY WITH BS4\n",
    "def parse_penalty_summary(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the penalties div and table\n",
    "    penalties_div = soup.find('div', id='penalties')\n",
    "    if penalties_div is None:\n",
    "        logging.error(\"Penalties div not found\")\n",
    "        return None\n",
    "\n",
    "    penalties_table = penalties_div.find('table')\n",
    "    if penalties_table is None:\n",
    "        logging.error(\"Penalties table not found within the penalties div\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store penalty events\n",
    "    penalty_events = []\n",
    "    period = None\n",
    "\n",
    "    # Loop through table rows\n",
    "    for row in penalties_table.find_all('tr'):\n",
    "        if 'stats-section' in row.get('class', []):\n",
    "            td = row.find('td')\n",
    "            if td:\n",
    "                period = td.text.strip()\n",
    "            else:\n",
    "                logging.warning(\"Period name not found in 'stats-section' row\")\n",
    "                period = \"Unknown\"\n",
    "        else:\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                team = cols[0].text.strip()\n",
    "                team = abbreviation_to_fullname.get(team, team)  # Replace abbreviation\n",
    "                team = clean_team_name(team)  # Clean team name\n",
    "\n",
    "                player = cols[1].text.strip()\n",
    "                pen_length = cols[2].text.strip()\n",
    "                penalty_type = cols[3].text.strip()\n",
    "                time = cols[4].text.strip()\n",
    "\n",
    "                penalty_event = {\n",
    "                    'Period': period,\n",
    "                    'Team': team,\n",
    "                    'Player': player,\n",
    "                    'Pen_Length': pen_length,\n",
    "                    'Penalty_Type': penalty_type,\n",
    "                    'Time': time\n",
    "                }\n",
    "                penalty_events.append(penalty_event)\n",
    "\n",
    "    return pd.DataFrame(penalty_events)\n",
    "\n",
    "\n",
    "############# GOALIE SUMMARY WITH BS4\n",
    "def parse_goalie_stats(html_content):\n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find the goalies div and table\n",
    "    goalies_div = soup.find('div', id='goalies')\n",
    "    if goalies_div is None:\n",
    "        logging.error(\"Goalies div not found\")\n",
    "        return None\n",
    "\n",
    "    goalies_table = goalies_div.find('table')\n",
    "    if goalies_table is None:\n",
    "        logging.error(\"Goalies table not found within the goalies div\")\n",
    "        return None\n",
    "\n",
    "    # Initialize list to store goalie stats\n",
    "    goalie_stats = []\n",
    "    team = None\n",
    "\n",
    "    # Loop through table rows   \n",
    "    for row in goalies_table.find_all('tr'):\n",
    "        if 'stats-header' in row.get('class', []):  # Team header rows\n",
    "            td = row.find('td')\n",
    "            team = td.text.strip() if td else \"Unknown\"\n",
    "            # Replace abbreviation and clean team name\n",
    "            team = abbreviation_to_fullname.get(team, team)\n",
    "            team = clean_team_name(team)\n",
    "        else:  # Data rows\n",
    "            cols = row.find_all('td')\n",
    "            if len(cols) > 1:\n",
    "                goalie = cols[0].text.strip()\n",
    "                sv = cols[1].text.strip()\n",
    "                ga = cols[2].text.strip()\n",
    "                minutes = cols[3].text.strip()\n",
    "\n",
    "                # Build goalie stat dictionary\n",
    "                goalie_stat = {\n",
    "                    'Team': team,\n",
    "                    'Goalie': goalie,\n",
    "                    'SV': sv,\n",
    "                    'GA': ga,\n",
    "                    'Minutes': minutes\n",
    "                }\n",
    "                goalie_stats.append(goalie_stat)\n",
    "\n",
    "    # Convert list to DataFrame\n",
    "    return pd.DataFrame(goalie_stats)\n",
    "\n",
    "\n",
    "#### PARSE THE ADVANCED TEAM METRICS TABLES ####\n",
    "### RETURNS WHOLE ADVANCED METRICS AS SINGLE TABLE\n",
    "####################################\n",
    "# def parse_new_advanced_metrics(html_content):\n",
    "    # Parse HTML content\n",
    "    # soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # # Find all tables with advanced metrics\n",
    "    # tables = soup.find_all('table', {'class': 'sortable metrics'})\n",
    "    \n",
    "    # # List to store all parsed data\n",
    "    # all_data = []\n",
    "    \n",
    "    # for table in tables:\n",
    "    #     # Extract team name from the table header\n",
    "    #     team_name = table.find('td').text.strip()\n",
    "    #     team_name = abbreviation_to_fullname.get(team_name, team_name)  # Replace abbreviation\n",
    "    #     team_name = clean_team_name(team_name)  # Clean team name\n",
    "        \n",
    "    #     # Extract headers (skipping the Player header)\n",
    "    #     headers = [header.text for header in table.find_all('th')][1:]\n",
    "        \n",
    "    #     # Prepare final column headers\n",
    "    #     col_names = ['Team', 'Player']\n",
    "    #     for header in headers:\n",
    "    #         col_names.append(header)\n",
    "        \n",
    "    #     # Extract player data\n",
    "    #     rows = table.find_all('tr')[2:]  # skipping the two header rows\n",
    "    #     for row in rows:\n",
    "    #         player_data = [team_name]  # start with team name\n",
    "    #         cells = row.find_all('td')\n",
    "    #         player_data.append(cells[0].text.strip())  # player name\n",
    "    #         for cell in cells[1:]:\n",
    "    #             player_data.append(cell.text.strip())\n",
    "    #         all_data.append(player_data)\n",
    "    \n",
    "    # # Convert the list of data to a DataFrame\n",
    "    # df = pd.DataFrame(all_data, columns=col_names)\n",
    "    # return df\n",
    "\n",
    "######## NEW TEST ###############  \n",
    "def parse_advanced_metrics_tables(html_content):\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all tables with advanced metrics\n",
    "    tables = soup.find_all('table', {'class': 'sortable metrics'})\n",
    "    \n",
    "    # List to store all parsed data\n",
    "    all_data = []\n",
    "    \n",
    "    for table in tables:\n",
    "        # Extract team name from the table header\n",
    "        team_name = table.find('td').text.strip()\n",
    "        team_name = abbreviation_to_fullname.get(team_name, team_name)  # Replace abbreviation\n",
    "        team_name = clean_team_name(team_name)  # Clean team name\n",
    "        \n",
    "        # Extract headers (skipping the Player header)\n",
    "        headers = [header.text for header in table.find_all('th')][1:]\n",
    "        \n",
    "        # Prepare final column headers\n",
    "        col_names = ['Team', 'Player']\n",
    "        for header in headers:\n",
    "            col_names.append(header)\n",
    "        \n",
    "        # Extract player data\n",
    "        rows = table.find_all('tr')[2:]  # skipping the two header rows\n",
    "        for row in rows:\n",
    "            player_data = [team_name]  # start with team name\n",
    "            cells = row.find_all('td')\n",
    "            player_data.append(cells[0].text.strip())  # player name\n",
    "            for cell in cells[1:]:\n",
    "                player_data.append(cell.text.strip())\n",
    "            all_data.append(player_data)\n",
    "    \n",
    "    # Convert the list of data to a DataFrame\n",
    "    df = pd.DataFrame(all_data, columns=col_names)\n",
    "\n",
    "    # # Rename columns for clairity\n",
    "    # new_names = ['Team', 'Player', 'TOTAL_Block', 'TOTAL_Miss', 'TOTAL_Saved', 'TOTAL_Goals', 'TOTAL_Total_Shots',\n",
    "    #                 'EVEN_Block', 'EVEN_Miss', 'EVEN_Saved', 'EVEN_Goals', 'EVEN_Total_Shots',\n",
    "    #                 'PP_Block', 'PP_Miss', 'PP_Saved', 'PP_Goals', 'PP_Total_Shots',\n",
    "    #                 'CLOSE_Block', 'CLOSE_Miss', 'CLOSE_Saved', 'CLOSE_Goals', 'CLOSE_Total_Shots',\n",
    "\n",
    "    #                 'D_Blocks', 'Faceoffs', 'Game_ID']\n",
    "    # df.columns = new_names\n",
    "\n",
    "    ## Fill all NaN values with 0\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Parsing the line chart information with specific positions for forwards and defensemen.\n",
    "def parse_line_chart(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    line_chart_div = soup.find('div', id='linechart')\n",
    "\n",
    "    if line_chart_div is None:\n",
    "        logging.error(\"Line chart div not found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    line_data = []\n",
    "\n",
    "    for team_div in line_chart_div.find_all('div', recursive=False):\n",
    "        h3 = team_div.find('h3')\n",
    "        if h3 is None:\n",
    "            logging.warning(\"Team name not found\")\n",
    "            continue\n",
    "        \n",
    "        team_name = h3.text.strip()\n",
    "        # Replace abbreviation and clean team name\n",
    "        team_name = abbreviation_to_fullname.get(team_name, team_name)\n",
    "        team_name = clean_team_name(team_name)\n",
    "        \n",
    "        for line_type_div in team_div.find_all('div', recursive=False):\n",
    "            line_type = line_type_div.get('class')[0] if line_type_div.get('class') else None\n",
    "            if line_type is None:\n",
    "                logging.warning(\"Line type not found\")\n",
    "                continue\n",
    "            \n",
    "            if line_type == 'f':\n",
    "                position_types = ['Left Wing', 'Center', 'Right Wing']\n",
    "            elif line_type == 'd':\n",
    "                position_types = ['Left D', 'Right D']\n",
    "            elif line_type == 'x':\n",
    "                position_types = ['Extra']\n",
    "            elif line_type == 'g':\n",
    "                position_types = ['Goalie']\n",
    "                goalie_count = 1  # Initialize goalie count\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            players = line_type_div.find_all('div')\n",
    "            if not players:\n",
    "                logging.warning(f\"No players found for {team_name} in {line_type}\")\n",
    "                continue\n",
    "            \n",
    "            for i, player in enumerate(players):\n",
    "                player_name = player.text.strip()\n",
    "                if line_type == 'x':\n",
    "                    player_name = player_name.split(' ')[0]\n",
    "                if line_type == 'g':\n",
    "                    line_number = f\"Goalie {goalie_count}\"\n",
    "                    goalie_count += 1\n",
    "                else:\n",
    "                    line_number = i // len(position_types) + 1\n",
    "\n",
    "                position = position_types[i % len(position_types)]\n",
    "                line_data.append({\n",
    "                    'Team': team_name,\n",
    "                    'Line': line_number,\n",
    "                    'Position': position,\n",
    "                    'Player': player_name\n",
    "                })\n",
    "\n",
    "    if not line_data:\n",
    "        logging.error(\"No line data was collected\")\n",
    "\n",
    "    df = pd.DataFrame(line_data)\n",
    "    \n",
    "    # # Log DataFrame info for debugging\n",
    "    # if df.empty:\n",
    "    #     logging.warning(\"Generated line chart DataFrame is empty.\")\n",
    "    # else:\n",
    "    #     logging.info(f\"Generated line chart DataFrame with columns: {df.columns.tolist()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "from sqlalchemy import inspect, text\n",
    "\n",
    "def ensure_columns_exist(table_name, columns, engine):\n",
    "    \"\"\"\n",
    "    Ensures that the specified columns exist in the given table. Adds them if they are missing.\n",
    "    \n",
    "    Args:\n",
    "        table_name (str): Name of the table.\n",
    "        columns (list): List of column names to check/add.\n",
    "        engine (sqlalchemy.engine): SQLAlchemy database engine.\n",
    "    \"\"\"\n",
    "    inspector = inspect(engine)\n",
    "    existing_columns = [col['name'] for col in inspector.get_columns(table_name)]\n",
    "\n",
    "    missing_columns = [col for col in columns if col not in existing_columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        with engine.connect() as conn:\n",
    "            for col in missing_columns:\n",
    "                sql = text(f\"ALTER TABLE {table_name} ADD COLUMN {col} INTEGER DEFAULT 0;\")\n",
    "                conn.execute(sql)\n",
    "                logging.info(f\"Added missing column: {col} to table: {table_name}\")\n",
    "\n",
    "### Get the Linescore Elements - Score, shots, ect by period####\n",
    "def parse_linescore(html_content):\n",
    "    \"\"\"\n",
    "    Parses the linescore tables (Goals, Shots, PP) and dynamically handles periods and schema alignment.\n",
    "\n",
    "    Args:\n",
    "        html_content (str): HTML content of the page.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Linescore data as a DataFrame.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    linescore_data = []\n",
    "    max_periods = 0\n",
    "\n",
    "    # Parsing the Goals table\n",
    "    goals_table = soup.select_one(\"#goals table\")\n",
    "    if goals_table is None:\n",
    "        logging.error(\"Goals table not found\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = goals_table.select('tbody tr')\n",
    "    if not rows:\n",
    "        logging.warning(\"No rows found in Goals table\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for row in rows:\n",
    "        team_data = {}\n",
    "        td = row.select_one('td')\n",
    "        if td:\n",
    "            team_data['Team'] = td.text.strip()\n",
    "        else:\n",
    "            logging.warning(\"Team name not found in Goals table\")\n",
    "            continue\n",
    "\n",
    "        goals = row.select('td')[1:]\n",
    "        max_periods = max(max_periods, len(goals))  # Update max periods dynamically\n",
    "        for i, goal in enumerate(goals):\n",
    "            column_name = f'goals{i + 1}' if i < len(goals) - 1 else 'goalsT'\n",
    "            try:\n",
    "                team_data[column_name] = int(goal.text.strip())\n",
    "            except ValueError:\n",
    "                team_data[column_name] = None\n",
    "                logging.warning(f\"Invalid goal value in column {column_name}\")\n",
    "\n",
    "        linescore_data.append(team_data)\n",
    "\n",
    "    # Convert to DataFrame early\n",
    "    df = pd.DataFrame(linescore_data)\n",
    "\n",
    "    # Ensure all columns exist dynamically\n",
    "    expected_columns = [f'goals{i}' for i in range(1, max_periods)] + ['goalsT']\n",
    "    for col in expected_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0  # Add missing columns with default value 0\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to parse game details table\n",
    "def parse_game_details(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    meta_div = soup.find('div', {'id': 'meta'})\n",
    "    if meta_div is None:\n",
    "        logging.error(\"Meta div not found\")\n",
    "        return None\n",
    "    \n",
    "    game_details_div = meta_div.find_all('div')[-1]\n",
    "    if game_details_div is None:\n",
    "        logging.error(\"Game details div not found\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        date_str = game_details_div.h4.string\n",
    "        day_of_week, date = date_str.split(\", \", 1)\n",
    "        \n",
    "        p_elements = game_details_div.find_all('p')\n",
    "        \n",
    "        # Extract conference and location details\n",
    "        for p in p_elements:\n",
    "            if \"Game\" in p.text:  # e.g., \"Big Ten Game\"\n",
    "                details_strs = p.get_text(separator='|').split('|')\n",
    "                conference = details_strs[0]\n",
    "                location = details_strs[-1].split('at ')[-1]\n",
    "                break\n",
    "        else:  # Defaults if not found\n",
    "            conference, location = None, None\n",
    "        \n",
    "        # Extract referees and assistant referees details\n",
    "        for p in p_elements:\n",
    "            if \"Referees\" in p.text:\n",
    "                refs_str = p.strong.next_sibling if p.strong else None\n",
    "                asst_refs_str = p.find_all('strong')[1].next_sibling if len(p.find_all('strong')) > 1 else None\n",
    "                break\n",
    "        else:  # Defaults if not found\n",
    "            refs_str, asst_refs_str = None, None\n",
    "        \n",
    "        refs = refs_str.split(', ') if refs_str else []\n",
    "        asst_refs = asst_refs_str.split(', ') if asst_refs_str else []\n",
    "        refs = [re.sub(r'[^a-zA-Z ]+', '', ref).strip() for ref in refs]\n",
    "        asst_refs = [re.sub(r'[^a-zA-Z ]+', '', ref).strip() for ref in asst_refs]\n",
    "        \n",
    "        # Extract attendance details using regex for better accuracy\n",
    "        attendance_pattern = r\"Attendance:\\s?(\\d+[\\d,]*)\"\n",
    "        attendance_match = re.search(attendance_pattern, html_content)\n",
    "        attendance = int(attendance_match.group(1).replace(',', '')) if attendance_match else None\n",
    "        \n",
    "        # Extract game details (like shootout results)\n",
    "        details = None\n",
    "        for p in p_elements:\n",
    "            if \"shootout\" in p.text:\n",
    "                details = p.text\n",
    "                break\n",
    "        \n",
    "        # Clean details if present\n",
    "        if details and '\\n' in details:\n",
    "            details = details.replace('\\n', '').strip()\n",
    "        if details and '\\t' in details:\n",
    "            details = re.sub('\\t', ' ', details)\n",
    "        \n",
    "        game_details = {\n",
    "            'Day': day_of_week,\n",
    "            'Date': date,\n",
    "            'Conference': conference,\n",
    "            'Details': details,\n",
    "            'Location': location,\n",
    "            'Ref1': refs[0] if refs else None,\n",
    "            'Ref2': refs[1] if len(refs) > 1 else None,\n",
    "            'Asst_Ref1': asst_refs[0] if asst_refs else None,\n",
    "            'Asst_Ref2': asst_refs[1] if len(asst_refs) > 1 else None,\n",
    "            'Attendance': attendance\n",
    "        }\n",
    "        \n",
    "        game_details_df = pd.DataFrame([game_details])\n",
    "        return game_details_df\n",
    "\n",
    "    except (AttributeError, IndexError, ValueError) as e:\n",
    "        logging.error(f\"Error while parsing game details: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Parse the box score page - player stats table (G, A, Pt, +/-, Sh, PIM)\n",
    "def parse_box_score(box_score_html):\n",
    "    # Initialize DataFrames to None\n",
    "    scoring_summary = penalty_summary = goalie_stats = player_stats = line_chart = linescore = game_details = None\n",
    "    \n",
    "    try:\n",
    "        scoring_summary = parse_scoring_summary(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_scoring_summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        penalty_summary = parse_penalty_summary(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_penalty_summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        goalie_stats = parse_goalie_stats(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_goalie_stats: {e}\")\n",
    "    \n",
    "    try:\n",
    "        player_stats = parse_player_summary(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_player_summary: {e}\")\n",
    "    \n",
    "    try:\n",
    "        line_chart = parse_line_chart(box_score_html)\n",
    "        if line_chart.empty:\n",
    "            logging.info(\"Line chart is empty. Skipping the insert for this game.\")\n",
    "        else:\n",
    "            logging.info(f\"Line chart DataFrame structure: {line_chart.dtypes}\")\n",
    "\n",
    "        # Insert into database (make sure this part works as expected)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in parse_line_chart: {e}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        linescore_df = parse_linescore(box_score_html)\n",
    "\n",
    "        if not linescore_df.empty:\n",
    "            process_and_save_linescore(linescore_df, engine, table_name=\"linescore\")\n",
    "        else:\n",
    "            logging.warning(\"No linescore data to save for this game.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing linescore: {e}\")\n",
    "\n",
    "    \n",
    "    try:\n",
    "        game_details = parse_game_details(box_score_html)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in parse_game_details: {e}\")\n",
    "    \n",
    "    # Combine DataFrames into a list\n",
    "    all_dfs = [game_details, scoring_summary, penalty_summary, goalie_stats, player_stats, line_chart, linescore]\n",
    "    \n",
    "    return all_dfs\n",
    "\n",
    "def rename_duplicate_columns(df):\n",
    "    \"\"\"\n",
    "    Renames duplicate columns in a DataFrame to make them unique.\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with unique column names.\n",
    "    \"\"\"\n",
    "    cols = pd.Series(df.columns)\n",
    "    for dup in df.columns[df.columns.duplicated()].unique(): \n",
    "        cols[df.columns.get_loc(dup)] = [f\"{dup}_{i}\" if i != 0 else dup for i in range(sum(df.columns == dup))]\n",
    "    df.columns = cols\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End Legacy Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect, text\n",
    "\n",
    "def ensure_columns_exist(table_name, columns, engine):\n",
    "    \"\"\"\n",
    "    Ensures that the specified columns exist in the given table. Adds them if missing.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the table.\n",
    "        columns (list): List of column names to check/add.\n",
    "        engine (sqlalchemy.engine): SQLAlchemy database engine.\n",
    "    \"\"\"\n",
    "    inspector = inspect(engine)\n",
    "    existing_columns = [col['name'] for col in inspector.get_columns(table_name)]\n",
    "\n",
    "    missing_columns = [col for col in columns if col not in existing_columns]\n",
    "\n",
    "    if missing_columns:\n",
    "        with engine.connect() as conn:\n",
    "            for col in missing_columns:\n",
    "                sql = text(f\"ALTER TABLE {table_name} ADD COLUMN {col} INTEGER DEFAULT 0;\")\n",
    "                conn.execute(sql)\n",
    "                logging.info(f\"Added missing column: {col} to table: {table_name}\")\n",
    "\n",
    "def process_and_save_linescore(linescore_df, engine, table_name=\"linescore\"):\n",
    "    \"\"\"\n",
    "    Process and save the linescore DataFrame to the database, ensuring schema alignment.\n",
    "\n",
    "    Args:\n",
    "        linescore_df (pd.DataFrame): Linescore DataFrame.\n",
    "        engine (sqlalchemy.engine): SQLAlchemy database engine.\n",
    "        table_name (str): Table name in the database.\n",
    "    \"\"\"\n",
    "    # Ensure columns exist dynamically in the database table\n",
    "    ensure_columns_exist(table_name, linescore_df.columns, engine)\n",
    "\n",
    "    # Save the DataFrame to the database\n",
    "    try:\n",
    "        linescore_df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "        logging.info(f\"Linescore data saved to table: {table_name}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving linescore to table {table_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_and_save_data(row, base_url, game_id, conn):\n",
    "    \"\"\"\n",
    "    Fetches and parses data for a single game, then saves it to the database.\n",
    "    Args:\n",
    "        row (pd.Series): Row from the games DataFrame containing game details.\n",
    "        base_url (str): Base URL for the scraping website.\n",
    "        game_id (str): Unique game identifier.\n",
    "        conn (sqlite3.Connection): SQLite connection to the database.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        box_score_url = f\"{base_url}{row['Box_Link']}\"\n",
    "        metrics_url = f\"{base_url}{row['Metrics_Link']}\" if row['Metrics_Link'] else None\n",
    "\n",
    "        # Fetch HTML for box score\n",
    "        box_score_response = requests.get(box_score_url, timeout=10)\n",
    "        box_score_response.raise_for_status()\n",
    "        box_score_html = box_score_response.text\n",
    "\n",
    "        # Parse box score data\n",
    "        box_score_dfs = parse_box_score(box_score_html)\n",
    "\n",
    "        # Fetch and parse advanced metrics if available\n",
    "        if metrics_url:\n",
    "            metrics_response = requests.get(metrics_url, timeout=10)\n",
    "            metrics_response.raise_for_status()\n",
    "            metrics_html = metrics_response.text\n",
    "            advanced_metrics_df = parse_advanced_metrics_tables(metrics_html)\n",
    "        else:\n",
    "            advanced_metrics_df = pd.DataFrame()\n",
    "\n",
    "        # Combine all DataFrames\n",
    "        all_dfs = box_score_dfs + [advanced_metrics_df]\n",
    "\n",
    "        # Apply Game_ID and remove duplicate columns\n",
    "        for df in all_dfs:\n",
    "            if df is not None and not df.empty:\n",
    "                df['Game_ID'] = game_id\n",
    "                df = rename_duplicate_columns(df)\n",
    "\n",
    "        # Save data to database\n",
    "        table_names = [\n",
    "            'game_details', 'scoring_summary', 'penalty_summary',\n",
    "            'goalie_stats', 'player_stats', 'line_chart', 'linescore', 'advanced_metrics'\n",
    "        ]\n",
    "\n",
    "        for df, table in zip(all_dfs, table_names):\n",
    "            if df is not None and not df.empty:\n",
    "                df.to_sql(table, conn, if_exists='append', index=False)\n",
    "\n",
    "        logging.info(f\"Successfully scraped and stored data for game: {game_id}\")\n",
    "        return True\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Network error for game {game_id}: {e}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing game {game_id}: {e}\")\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def scrape_games_and_store(sampled_games, base_url, conn):\n",
    "    \"\"\"\n",
    "    Main function to scrape and store data for a list of games.\n",
    "    Args:\n",
    "        sampled_games (pd.DataFrame): DataFrame of games to scrape.\n",
    "        base_url (str): Base URL for the scraping website.\n",
    "        conn (sqlite3.Connection): SQLite connection to the database.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    error_count = 0\n",
    "    error_games = []\n",
    "\n",
    "    for _, row in tqdm(sampled_games.iterrows(), total=sampled_games.shape[0], desc=\"Scraping games\"):\n",
    "        game_id = f\"{row['Date']}-{row['Home_Team']}-{row['Away_Team']}\"\n",
    "        retries = 3\n",
    "        success = False\n",
    "\n",
    "        while retries > 0 and not success:\n",
    "            success = fetch_and_save_data(row, base_url, game_id, conn)\n",
    "            if not success:\n",
    "                retries -= 1\n",
    "                time.sleep(5)  # Wait before retrying\n",
    "\n",
    "        if not success:\n",
    "            error_count += 1\n",
    "            error_games.append(game_id)\n",
    "\n",
    "    logging.info(f\"Scraping completed with {error_count} errors.\")\n",
    "    if error_games:\n",
    "        logging.warning(f\"Failed games: {error_games}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Functions to Perform scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call the function to scrape and store data from the games\n",
    "# Example: Scraping games and storing results\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up database connection\n",
    "    # db_path = \"../TEMP/CHN_Scrape_TEST_7.db\"\n",
    "    # conn = sqlite3.connect(db_path)\n",
    "\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "\n",
    "    \n",
    "    # Base URL\n",
    "    base_url = \"https://www.collegehockeynews.com\"\n",
    "\n",
    "    # Scrape and store games\n",
    "    scrape_games_and_store(games_to_scrape, base_url, conn)\n",
    "\n",
    "    # Close database connection\n",
    "    # conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the Roster data from the CSV to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the database connection\n",
    "# conn = sqlite3.connect(db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## SET THE ROSTER DATAFRAME TO THE CORRECT YEAR ####################\n",
    "## Load the roster data from CSV\n",
    "roster_filename = 'roster_2024_current_v3.csv'\n",
    "# Load to DataFrame\n",
    "roster_df = pd.read_csv(f'../data/{roster_filename}')\n",
    "\n",
    "# Set the SeasonYear in the database_roster\n",
    "season_year_setting = 2024\n",
    "\n",
    "## MATCH THE DATAFRAME NAMES\n",
    "df_master_roster = roster_df.copy()\n",
    "\n",
    "## Season Year Value\n",
    "season_year = season_year_setting\n",
    "\n",
    "# Clean up the name formats for joining\n",
    "# Master roster: Convert \"Last Name, First Name\" to \"First Name Last Name\"\n",
    "# df_master_roster['Clean_Name'] = df_master_roster['Player'].apply(lambda x: ' '.join(reversed(x.split(', '))))\n",
    "\n",
    "# Rename Player to Clean_Name\n",
    "df_master_roster.rename(columns={'Player': 'Clean_Name'}, inplace=True)\n",
    "# Rename School to Team\n",
    "df_master_roster.rename(columns={'School': 'Team'}, inplace=True)\n",
    "\n",
    "# Clean up the Team column, remove '-' and replace with ' '\n",
    "# df_master_roster['School'] = df_master_roster['Team'].apply(lambda x: x.replace('-', ' '))\n",
    "\n",
    "## If there are an period in the column names, remove them\n",
    "df_master_roster.columns = df_master_roster.columns.str.replace('.', '')\n",
    "\n",
    "### Finally add the roster to the database as it's own table\n",
    "\n",
    "df_master_roster['SeasonYear'] = season_year\n",
    "\n",
    "# Save the roster data as a new table in the database\n",
    "roster_table_name = 'master_roster'\n",
    "df_master_roster.to_sql(roster_table_name, conn, if_exists='replace', index=False)\n",
    "############################################################\n",
    "\n",
    "# Verify by listing all the tables in the database again\n",
    "tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = conn.execute(tables_query).fetchall()\n",
    "table_names_updated = [table[0] for table in tables]\n",
    "table_names_updated\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up The Column Names and extra header rows in the Player Stats table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_stats_df = pd.read_sql_query(\"SELECT * FROM player_stats\", conn)\n",
    "\n",
    "# Define a dictionary for column renaming\n",
    "column_renames = {\n",
    "    'Pt.': 'Pts',\n",
    "    '+/-': 'plus_minus'\n",
    "}\n",
    "\n",
    "# Rename columns based on the dictionary\n",
    "player_stats_df.rename(columns=column_renames, inplace=True)\n",
    "\n",
    "\n",
    "# Drop rows where Team name is in the Player column\n",
    "player_stats_df = player_stats_df[player_stats_df['Team'] != player_stats_df['Player']]\n",
    "\n",
    "## Change the Column names to be easy to work with\n",
    "############ 'Pt.' should be 'Pts' and '+/-' should be 'plus_minus'\n",
    "#################################\n",
    "player_stats_df = pd.read_sql_query(\"SELECT * FROM player_stats\", conn)\n",
    "\n",
    "if 'Pt.' in player_stats_df.columns:\n",
    "    player_stats_df.rename(columns={'Pt.': 'Pts'}, inplace=True)\n",
    "else:\n",
    "    print(\"Column 'Pt.' not found.\")\n",
    "\n",
    "if '+/-' in player_stats_df.columns:\n",
    "    player_stats_df.rename(columns={'+/-': 'plus_minus'}, inplace=True)\n",
    "else:\n",
    "    print(\"Column '+/-' not found.\")\n",
    "\n",
    "print(len(player_stats_df))\n",
    "\n",
    "# Drop rows if Team name is in the player column\n",
    "# If ['Team'] is the same as ['Player'] then drop that row\n",
    "player_stats_df = player_stats_df[player_stats_df['Team'] != player_stats_df['Player']]\n",
    "\n",
    "# add the dataframe back to the database\n",
    "player_stats_df.to_sql('player_stats', conn, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE A NEW TABLE WITH AGGRIGATED PLAYER STATS YEAR TO DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the player_stats table into df_player_stats\n",
    "\n",
    "# Query to load the player_stats table\n",
    "player_stats_query = \"SELECT * FROM player_stats\"\n",
    "df_player_stats = pd.read_sql(player_stats_query, conn)\n",
    "\n",
    "# Address problem where header rows of each game were added to the table\n",
    "# If Team and Player columns match remove the row\n",
    "df_player_stats = df_player_stats[df_player_stats['Team'] != 'Team']\n",
    "\n",
    "# Replace the non-breaking space with a regular space\n",
    "df_player_stats['Player'] = df_player_stats['Player'].apply(lambda x: x.replace('\\xa0', ' '))\n",
    "\n",
    "# Convert relevant columns to integers for correct aggregation\n",
    "cols_to_convert = ['G', 'A', 'Pts', 'plus_minus', 'Sh', 'PIM']\n",
    "for col in cols_to_convert:\n",
    "    df_player_stats[col] = pd.to_numeric(df_player_stats[col], errors='coerce')\n",
    "\n",
    "# Aggregate the data for year-to-date stats\n",
    "# Add a column for counting the number of games each player has played\n",
    "agg_player_stats_corrected_with_games = df_player_stats.groupby(['Player', 'Team']).agg({\n",
    "    'G': 'sum',\n",
    "    'A': 'sum',\n",
    "    'Pts': 'sum',\n",
    "    'plus_minus': 'sum',\n",
    "    'Sh': 'sum',\n",
    "    'PIM': 'sum',\n",
    "    'Game_ID': 'count'  # Counting the number of unique Game_IDs for each player\n",
    "}).reset_index()\n",
    "\n",
    "# Rename the Game_ID column to Games_Played\n",
    "agg_player_stats_corrected_with_games.rename(columns={'Game_ID': 'Games_Played'}, inplace=True)\n",
    "\n",
    "# Save the updated aggregated data back to the database, replacing the existing table\n",
    "agg_player_stats_corrected_with_games.to_sql('player_stats_ytd', conn, if_exists='replace', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up / Rename COlumns in Advanced Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NEW Handling of Advanced Stats\n",
    "# Create dataframe from SQL query\n",
    "df = pd.read_sql_query(\"SELECT * FROM advanced_metrics\", conn)\n",
    "\n",
    "# Rename columns\n",
    "new_names = ['Team', 'Player', 'TOTAL_Block', 'TOTAL_Miss', 'TOTAL_Saved', 'TOTAL_Goals', 'TOTAL_Total_Shots',\n",
    "                'EVEN_Block', 'EVEN_Miss', 'EVEN_Saved', 'EVEN_Goals', 'EVEN_Total_Shots',\n",
    "                'PP_Block', 'PP_Miss', 'PP_Saved', 'PP_Goals', 'PP_Total_Shots',\n",
    "                'CLOSE_Block', 'CLOSE_Miss', 'CLOSE_Saved', 'CLOSE_Goals', 'CLOSE_Total_Shots',\n",
    "\n",
    "                'D_Blocks', 'Faceoffs', 'Game_ID']\n",
    "\n",
    "df.columns = new_names\n",
    "\n",
    "# Remove all rows where Player = TOTAL\n",
    "df = df[df['Player'] != 'TOTAL']\n",
    "\n",
    "# # Apply the matched_dict to the Team column\n",
    "# df['Team'] = df['Team'].apply(lambda x: matched_dict[x])\n",
    "\n",
    "## Fill all NaN values with 0\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Display the dataframe\n",
    "df.head()\n",
    "\n",
    "# Save back to the database\n",
    "df.to_sql('advanced_metrics', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scoring_summary',\n",
       " 'penalty_summary',\n",
       " 'goalie_stats',\n",
       " 'line_chart',\n",
       " 'master_roster',\n",
       " 'player_stats',\n",
       " 'player_stats_ytd',\n",
       " 'advanced_metrics',\n",
       " 'game_details']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TEMP CODE\n",
    "# Database Name\n",
    "DB_FILE_NAME = 'CHN_2024_YTD_Stats.db'\n",
    "# Database Path\n",
    "db_path = f'../data/db/{DB_FILE_NAME}'\n",
    "# Open the database connection\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# print table names\n",
    "tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = conn.execute(tables_query).fetchall()\n",
    "table_names = [table[0] for table in tables]\n",
    "table_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Home_Team and Away_Team Columns to tables where needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read the game_details table into a DataFrame\n",
    "df_game_details = pd.read_sql(\"SELECT * FROM game_details\", conn)\n",
    "\n",
    "# Step 2: Create new columns for Home and Away Teams by parsing Game_ID\n",
    "df_game_details['Away_Team'] = df_game_details['Game_ID'].apply(lambda x: x.split('-')[4])\n",
    "df_game_details['Home_Team'] = df_game_details['Game_ID'].apply(lambda x: x.split('-')[3])\n",
    "\n",
    "# Step 3: Write this updated DataFrame back to the game_details table\n",
    "df_game_details.to_sql('game_details', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2728"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Select scoring_summary table into a DataFrame and add the Home_Team and Away_Team columns\n",
    "df_scoring_summary = pd.read_sql(\"SELECT * FROM scoring_summary\", conn)\n",
    "\n",
    "# Add Home_Team and Away_Team columns\n",
    "df_scoring_summary['Away_Team'] = df_scoring_summary['Game_ID'].apply(lambda x: x.split('-')[4])\n",
    "df_scoring_summary['Home_Team'] = df_scoring_summary['Game_ID'].apply(lambda x: x.split('-')[3])\n",
    "\n",
    "# Write the updated DataFrame back to the scoring_summary table\n",
    "df_scoring_summary.to_sql('scoring_summary', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# shutdown logging\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
