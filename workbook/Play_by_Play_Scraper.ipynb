{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NCAA.com Play-by-play Data Scraper\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example schedule URL\n",
    "## First Day of season\n",
    "# https://www.ncaa.com/scoreboard/icehockey-men/d1/2024/10/04/all-conf\n",
    "\n",
    "# Last Regular Season Day\n",
    "# https://www.ncaa.com/scoreboard/icehockey-men/d1/2025/03/08/all-conf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "from config import recent_clean_db, last_game_date\n",
    "\n",
    "# File paths\n",
    "data_folder = os.path.join('..', 'data/') # Data Folder Path\n",
    "temp_folder = os.path.join('..', 'TEMP/',) # Temp Folder Path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedule_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the NCAA.com schedule section\n",
    "- Creates a dataframe with Data - Teams and Game_ID_Number\n",
    "\n",
    "- Turned off because it takes 6-7 minutes to run and we can use a previously scraped and locally stored schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Base URL for NCAA schedule\n",
    "# base_url = \"https://www.ncaa.com/scoreboard/icehockey-men/d1\"\n",
    "\n",
    "# # Function to scrape a single day's schedule with rate limiting\n",
    "# def scrape_schedule(date):\n",
    "#     url = f\"{base_url}/{date}/all-conf\"\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code != 200:\n",
    "#         print(f\"Failed to fetch data for {date}: {response.status_code}\")\n",
    "#         return []\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "#     games = []\n",
    "\n",
    "#     # Locate game containers based on the provided HTML structure\n",
    "#     game_containers = soup.select('#scoreboardGames .gamePod')\n",
    "#     for game in game_containers:\n",
    "#         try:\n",
    "#             game_id = game.select_one('a.gamePod-link')['href'].split('/')[-1]\n",
    "#             teams = game.select('ul.gamePod-game-teams li')\n",
    "            \n",
    "#             home_team = teams[0].select_one('span.gamePod-game-team-name').text.strip()\n",
    "#             away_team = teams[1].select_one('span.gamePod-game-team-name').text.strip()\n",
    "            \n",
    "#             games.append({\n",
    "#                 'Date': date,\n",
    "#                 'Home_Team': home_team,\n",
    "#                 'Away_Team': away_team,            \n",
    "\n",
    "#                 'game_id_number': game_id\n",
    "#             })\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing game: {e}\")\n",
    "\n",
    "#     return games\n",
    "\n",
    "# # Function to scrape a range of dates with rate limiting and progress bar\n",
    "# def scrape_schedule_range(start_date, end_date):\n",
    "#     date_range = pd.date_range(start=start_date, end=end_date).strftime('%Y/%m/%d')\n",
    "#     all_games = []\n",
    "    \n",
    "#     # Progress bar setup\n",
    "#     for date in tqdm(date_range, desc=\"Scraping schedule\", unit=\"day\"):\n",
    "#         games = scrape_schedule(date)\n",
    "#         all_games.extend(games)\n",
    "#         time.sleep(1)  # Rate limiter: 1-second delay between requests\n",
    "\n",
    "#     return pd.DataFrame(all_games)\n",
    "\n",
    "# # Example usage\n",
    "# start_date = \"2024-10-04\"  # First day of the season\n",
    "# end_date = \"2025-03-08\"    # Last regular season day\n",
    "# schedule_df = scrape_schedule_range(start_date, end_date)\n",
    "\n",
    "# # Display the resulting dataframe\n",
    "# schedule_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / Load Local Copy of Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save the schedule to a CSV file for later use\n",
    "# schedule_df.to_csv(os.path.join(data_folder, 'schedule_from_ncaa_with_game_number.csv'), index=False)\n",
    "\n",
    "# Load the locally stored schedule to avoid having to scrape again\n",
    "schedule_df = pd.read_csv(os.path.join(data_folder, 'schedule_from_ncaa_with_game_number.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transgformation\n",
    "- NOT NESS IF WORKING WITH NEW SCRAPE \n",
    "    - Seperate team column into Home_Team, Away_Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate team column into Home_Team, Away_Team\n",
    "\n",
    "def handle_home_away(schedule_df):\n",
    "    # Split Home_Team_Away_Team into Home_Team and Away_Team\n",
    "    schedule_df[['Away_Team', 'Home_Team']] = schedule_df['Home_Team_Away_Team'].str.split(' vs ', expand=True)\n",
    "    \n",
    "    # Remove punctuation and strip whitespace\n",
    "    schedule_df['Home_Team'] = schedule_df['Home_Team'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x).strip())\n",
    "    schedule_df['Away_Team'] = schedule_df['Away_Team'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x).strip())\n",
    "\n",
    "    # Drop the original column\n",
    "    schedule_df = schedule_df.drop(columns=['Home_Team_Away_Team'])\n",
    "    return schedule_df\n",
    "\n",
    "# call the function\n",
    "schedule_df = handle_home_away(schedule_df)\n",
    "schedule_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load School info and replace ncaa_names with standard Team names from existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load School info \n",
    "school_info_path = os.path.join(data_folder, 'arena_school_info.csv')\n",
    "school_info_df = pd.read_csv(school_info_path)\n",
    "# school_info_df.head() # Check data\n",
    "\n",
    "# Function to map team names to standardized names\n",
    "def map_team_names(schedule_df, school_info_df):\n",
    "    # Create a mapping dictionary from school_info_df\n",
    "    team_mapping = {\n",
    "        re.sub(r'[^\\w\\s]', '', row['ncaa_name']).strip(): row['Team']\n",
    "        for _, row in school_info_df.iterrows()\n",
    "    }\n",
    "\n",
    "    # Map Home_Team and Away_Team to standardized names\n",
    "    schedule_df['Home_Team'] = schedule_df['Home_Team'].apply(lambda x: team_mapping.get(re.sub(r'[^\\w\\s]', '', x).strip(), x))\n",
    "    schedule_df['Away_Team'] = schedule_df['Away_Team'].apply(lambda x: team_mapping.get(re.sub(r'[^\\w\\s]', '', x).strip(), x))\n",
    "\n",
    "    return schedule_df\n",
    "\n",
    "# Call the function\n",
    "schedule_df = map_team_names(schedule_df, school_info_df)\n",
    "\n",
    "# Check the data\n",
    "schedule_df.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a New Column with Game_ID to match with the rest of the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a unique Game_ID\n",
    "def create_game_id(schedule_df):\n",
    "    schedule_df['Game_ID'] = schedule_df.apply(\n",
    "        lambda row: f\"{row['Date'].replace('/', '-')}-{row['Away_Team']}-{row['Home_Team']}\", axis=1\n",
    "    )\n",
    "    return schedule_df\n",
    "\n",
    "# Call the function\n",
    "schedule_df = create_game_id(schedule_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schedule_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Play By Play JSONs\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breakpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Custom API to Call NCAA.com\n",
    "- project developed by henrygd - https://github.com/henrygd/ncaa-api\n",
    "\n",
    "Uses his custom built API to get JSON response from NCAA.com\n",
    "- can host own server for large projects for now I am using his public link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Base URL for the custom API\n",
    "# base_url = \"https://ncaa-api.henrygd.me/game\"\n",
    "\n",
    "# # Function to get play-by-play JSON for a single game\n",
    "# def get_play_by_play(game_id_number):\n",
    "#     url = f\"{base_url}/{game_id_number}/play-by-play\"\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()  # Raise HTTPError for bad responses\n",
    "#         return response.json()\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         print(f\"Error fetching data for Game ID {game_id_number}: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # Function to fetch JSON data for all completed games\n",
    "# def fetch_play_by_play_data(schedule_df):\n",
    "#     # Filter for games that have already taken place\n",
    "#     today = datetime.now().strftime('%Y-%m-%d')\n",
    "#     completed_games = schedule_df[schedule_df['Date'] < today].copy()\n",
    "\n",
    "#     # Initialize a new column for play-by-play JSON\n",
    "#     completed_games['Play_By_Play_JSON'] = None\n",
    "\n",
    "#     for index, row in completed_games.iterrows():\n",
    "#         game_id_number = row['game_id_number']\n",
    "#         json_data = get_play_by_play(game_id_number)\n",
    "#         completed_games.at[index, 'Play_By_Play_JSON'] = json_data\n",
    "\n",
    "#     return completed_games\n",
    "\n",
    "# # Fetch and update the dataframe with play-by-play JSONs\n",
    "# updated_schedule_df = fetch_play_by_play_data(schedule_df)\n",
    "\n",
    "# # Check the updated dataframe\n",
    "# updated_schedule_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the dataframe in a way that doesn't cut off the json data - CSV cuts off the json data\n",
    "## Use Pickle\n",
    "# Save the updated dataframe to a pickle file\n",
    "# updated_schedule_df.to_pickle(os.path.join(temp_folder, 'schedule_with_play_by_play.pkl'))\n",
    "\n",
    "## Load pickle file to avoid having to scrape again\n",
    "# Load the updated dataframe from a pickle file\n",
    "updated_schedule_df = pd.read_pickle(os.path.join(temp_folder, 'schedule_with_play_by_play.pkl'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Play by Play JSONs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract all decriptions into a text file to study\n",
    "- Use this to make an abbr and alternate name dictionary for a find and replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Team Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create team name mapping from school_info_df\n",
    "team_mapping = {}\n",
    "for _, row in school_info_df.iterrows():\n",
    "    standard_name = row['Team']\n",
    "    alternatives = [a.strip() for a in row['ncaa_data_alts'].split(',')]\n",
    "    for alt in alternatives:\n",
    "        team_mapping[alt.lower()] = standard_name\n",
    "\n",
    "team_mapping\n",
    "team_map = team_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def clean_player_name(name):\n",
    "#     \"\"\"Handle special characters and formatting in player names\"\"\"\n",
    "#     if pd.isna(name) or not isinstance(name, str):\n",
    "#         return None\n",
    "#     try:\n",
    "#         decoded = name.encode('latin-1').decode('utf-8')\n",
    "#     except:\n",
    "#         decoded = name\n",
    "#     return normalize('NFKD', decoded.strip()).encode('ascii', 'ignore').decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def standardize_teams(text):\n",
    "#     \"\"\"Replace team name variations with standardized abbreviations\"\"\"\n",
    "#     text_lower = text.lower()\n",
    "#     for variation, standard in team_mapping.items():\n",
    "#         text_lower = re.sub(r'\\b' + re.escape(variation) + r'\\b', standard, text_lower, flags=re.IGNORECASE)\n",
    "#     return text_lower.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_description(description):\n",
    "#     \"\"\"Improved parsing function with team standardization\"\"\"\n",
    "#     parsed = {\n",
    "#         \"Event_type\": \"Other\",\n",
    "#         \"Primary_player\": None,\n",
    "#         \"Primary_team\": None,\n",
    "#         \"Secondary_player\": None,\n",
    "#         \"Secondary_team\": None,\n",
    "#         \"Outcome\": None,\n",
    "#         \"Penalty_duration\": None,\n",
    "#         \"Penalty_type\": None\n",
    "#     }\n",
    "\n",
    "#     if not isinstance(description, str):\n",
    "#         return parsed\n",
    "\n",
    "#     # Standardize team names first\n",
    "#     desc = standardize_teams(description)\n",
    "    \n",
    "#     # Faceoff parsing\n",
    "#     faceoff_match = re.search(\n",
    "#         r\"Faceoff\\s+([A-Za-zÀ-ÖØ-öø-ÿ' ,.-]+)\\s+vs\\s+([A-Za-zÀ-ÖØ-öø-ÿ' ,.-]+)\\s+won by\\s+([A-Z]{3,5})\",\n",
    "#         desc\n",
    "#     )\n",
    "#     if faceoff_match:\n",
    "#         parsed.update({\n",
    "#             \"Event_type\": \"Faceoff\",\n",
    "#             \"Primary_player\": clean_player_name(faceoff_match.group(1).replace(',', '').title()),\n",
    "#             \"Secondary_player\": clean_player_name(faceoff_match.group(2).replace(',', '').title()),\n",
    "#             \"Primary_team\": faceoff_match.group(3),\n",
    "#             \"Outcome\": \"won\"\n",
    "#         })\n",
    "#         return parsed\n",
    "\n",
    "#     # Goal parsing\n",
    "#     goal_match = re.search(r\"Goal by\\s+([A-Za-zÀ-ÖØ-öø-ÿ' ,.-]+)\\s*\\(\", desc)\n",
    "#     if goal_match:\n",
    "#         parsed.update({\n",
    "#             \"Event_type\": \"Goal\",\n",
    "#             \"Primary_player\": clean_player_name(goal_match.group(1).replace(',', '').title())\n",
    "#         })\n",
    "#         return parsed\n",
    "\n",
    "#     # Penalty parsing\n",
    "#     penalty_match = re.search(\n",
    "#         r\"Penalty on\\s+([A-Za-zÀ-ÖØ-öø-ÿ' ,.-]+)\\s+([A-Z]{3,5})\\s+(\\d+)\\s+minutes? for\\s+(.+)\",\n",
    "#         desc\n",
    "#     )\n",
    "#     if penalty_match:\n",
    "#         parsed.update({\n",
    "#             \"Event_type\": \"Penalty\",\n",
    "#             \"Primary_player\": clean_player_name(penalty_match.group(1).replace(',', '').title()),\n",
    "#             \"Primary_team\": penalty_match.group(2),\n",
    "#             \"Penalty_duration\": penalty_match.group(3),\n",
    "#             \"Penalty_type\": penalty_match.group(4).strip()\n",
    "#         })\n",
    "#         return parsed\n",
    "\n",
    "#     # Shot parsing\n",
    "#     shot_match = re.search(\n",
    "#         r\"Shot by\\s+([A-Z]{3,5})\\s+([A-Za-zÀ-ÖØ-öø-ÿ' ,.-]+)\\s+(MISSED|WIDE|BLOCKED|SAVE)\",\n",
    "#         desc\n",
    "#     )\n",
    "#     if shot_match:\n",
    "#         parsed.update({\n",
    "#             \"Event_type\": \"Shot\",\n",
    "#             \"Primary_team\": shot_match.group(1),\n",
    "#             \"Primary_player\": clean_player_name(shot_match.group(2).replace(',', '').title())\n",
    "#         })\n",
    "#         return parsed\n",
    "\n",
    "#     return parsed\n",
    "\n",
    "# def convert_to_continuous_time(period, time_str):\n",
    "#     \"\"\"Convert period and game clock to continuous seconds\"\"\"\n",
    "#     period = int(period) if str(period).isdigit() else 4  # Handle OT as period 4\n",
    "#     minutes, seconds = map(int, time_str.split(':'))\n",
    "#     return (20 * 60 * (period - 1)) + ((20 - minutes) * 60) - seconds\n",
    "\n",
    "# def process_game_data(json_path):\n",
    "#     \"\"\"Full processing pipeline\"\"\"\n",
    "#     with open(json_path) as f:\n",
    "#         data = json.load(f)\n",
    "    \n",
    "#     rows = []\n",
    "#     for period in data['periods']:\n",
    "#         period_num = period['periodNumber']\n",
    "#         for play in period['playStats']:\n",
    "#             description = play['visitorText'] or play['homeText'] or ''\n",
    "#             rows.append({\n",
    "#                 'Period': period_num,\n",
    "#                 'Time': play['time'],\n",
    "#                 'Description': description,\n",
    "#                 'Score': play['score']\n",
    "#             })\n",
    "    \n",
    "#     df = pd.DataFrame(rows)\n",
    "    \n",
    "#     # Convert time\n",
    "#     df['Period'] = df['Period'].replace({'1st': '1', '2nd': '2', '3rd': '3', 'OT': '4'})\n",
    "#     df['Continuous_Time'] = df.apply(\n",
    "#         lambda x: convert_to_continuous_time(x['Period'], x['Time']), axis=1\n",
    "#     )\n",
    "    \n",
    "#     # Parse descriptions\n",
    "#     parsed_data = df['Description'].apply(parse_description)\n",
    "#     parsed_df = pd.DataFrame(parsed_data.tolist())\n",
    "    \n",
    "#     # Combine data\n",
    "#     final_df = pd.concat([df, parsed_df], axis=1)\n",
    "#     return final_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_description(description, team_mapping):\n",
    "#     \"\"\"Improved parser with team standardization and better pattern matching\"\"\"\n",
    "#     # Standardize team names first\n",
    "#     desc_clean = standardize_teams(description, team_mapping)\n",
    "    \n",
    "#     parsed = {\n",
    "#         \"Event_type\": \"Other\",\n",
    "#         \"Primary_player\": None,\n",
    "#         \"Primary_team\": None,\n",
    "#         \"Secondary_player\": None,\n",
    "#         \"Secondary_team\": None,\n",
    "#         \"Outcome\": None,\n",
    "#         \"Penalty_duration\": None,\n",
    "#         \"Penalty_type\": None\n",
    "#     }\n",
    "\n",
    "#     # Faceoff pattern (handles standardized team names)\n",
    "#     if \"faceoff\" in desc_clean.lower():\n",
    "#         faceoff_pattern = r\"Faceoff (.+?) vs (.+?) won by (.+?)\\.\"\n",
    "#         if (match := re.search(faceoff_pattern, desc_clean, re.IGNORECASE)):\n",
    "#             parsed.update({\n",
    "#                 \"Event_type\": \"Faceoff\",\n",
    "#                 \"Primary_player\": clean_player_name(match.group(1)),\n",
    "#                 \"Secondary_player\": clean_player_name(match.group(2)),\n",
    "#                 \"Primary_team\": match.group(3).strip(),\n",
    "#                 \"Outcome\": \"won\"\n",
    "#             })\n",
    "#         return parsed\n",
    "\n",
    "#     # Goal pattern (handles special characters)\n",
    "#     if \"goal by\" in desc_clean.lower():\n",
    "#         goal_pattern = r\"Goal by (.+?) \\(\"\n",
    "#         if (match := re.search(goal_pattern, desc_clean)):\n",
    "#             parsed.update({\n",
    "#                 \"Event_type\": \"Goal\",\n",
    "#                 \"Primary_player\": clean_player_name(match.group(1))\n",
    "#             })\n",
    "#         return parsed\n",
    "\n",
    "#     # Penalty pattern (handles different formats)\n",
    "#     if \"penalty on\" in desc_clean.lower():\n",
    "#         penalty_pattern = r\"Penalty on (.+?) (\\w+) (\\d+) minutes for (.+)\"\n",
    "#         if (match := re.search(penalty_pattern, desc_clean, re.IGNORECASE)):\n",
    "#             parsed.update({\n",
    "#                 \"Event_type\": \"Penalty\",\n",
    "#                 \"Primary_player\": clean_player_name(match.group(1)),\n",
    "#                 \"Primary_team\": match.group(2),\n",
    "#                 \"Penalty_duration\": match.group(3),\n",
    "#                 \"Penalty_type\": match.group(4).strip()\n",
    "#             })\n",
    "#         return parsed\n",
    "\n",
    "#     # Shot pattern (handles shot types and goalie info)\n",
    "#     if \"shot by\" in desc_clean.lower():\n",
    "#         shot_pattern = r\"Shot by (\\w+) (.+?)(?:, save|$)\"\n",
    "#         if (match := re.search(shot_pattern, desc_clean, re.IGNORECASE)):\n",
    "#             parsed.update({\n",
    "#                 \"Event_type\": \"Shot\",\n",
    "#                 \"Primary_team\": match.group(1),\n",
    "#                 \"Primary_player\": clean_player_name(match.group(2))\n",
    "#             })\n",
    "#         return parsed\n",
    "\n",
    "#     return parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unicodedata import normalize\n",
    "\n",
    "# def clean_player_name(name):\n",
    "#     \"\"\"Handle international characters and formatting\"\"\"\n",
    "#     if pd.isna(name): return None\n",
    "#     return normalize('NFKD', name.strip()).encode('ascii', 'ignore').decode()\n",
    "\n",
    "\n",
    "# # # Function to convert period and time to continuous time\n",
    "# def convert_to_continuous_time(period, time):\n",
    "#     period_offsets = {'1': 0, '2': 20, '3': 40, 'OT': 60}\n",
    "#     minutes, seconds = map(int, time.split(':'))\n",
    "#     elapsed_time = (20 - minutes) * 60 + -seconds\n",
    "#     offset = period_offsets.get(period, 0) * 60\n",
    "#     return offset + elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transform pipeline\n",
    "# def transform_game_data(json_data, game_id, team_mapping):\n",
    "#     # Initial dataframe creation\n",
    "#     game_df = pd.DataFrame([\n",
    "#         {\n",
    "#             'Game_ID': game_id,\n",
    "#             'Period': period['periodNumber'],\n",
    "#             'Time': play['time'],\n",
    "#             'Description': play['visitorText'] or play['homeText'],\n",
    "#             'Score': play['score']\n",
    "#         }\n",
    "#         for period in json_data['periods']\n",
    "#         for play in period['playStats']\n",
    "#     ])\n",
    "    \n",
    "#     # Add continuous time\n",
    "#     game_df['Continuous_Time'] = game_df.apply(\n",
    "#         lambda r: convert_to_continuous_time(r['Period'], r['Time']), \n",
    "#         axis=1\n",
    "#     )\n",
    "    \n",
    "#     # Parse descriptions with team standardization\n",
    "#     parsed_data = game_df['Description'].apply(\n",
    "#         lambda d: parse_description(d, team_mapping)\n",
    "#     )\n",
    "#     return pd.concat([game_df, pd.DataFrame(parsed_data.tolist())], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load your JSON data\n",
    "# with open('../TEMP/play_by_play.json') as f:\n",
    "#     game_data = json.load(f)\n",
    "\n",
    "# # Process the data\n",
    "# df = transform_game_data(game_data, \"2024_LSSU_vs_MICHST\", team_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# school_info_df.head()\n",
    "# Create a dictionary mapping the alternate names (ncaa_data_alts) to the standardized names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_schedule_df\n",
    "\n",
    "# Output Play by Play JSON to a raw text file\n",
    "# Function to save JSON data to a text file\n",
    "def save_json_to_file(json_data, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(json_data, file)\n",
    "\n",
    "## Call the function for the first row\n",
    "save_json_to_file(updated_schedule_df.iloc[0]['Play_By_Play_JSON'], os.path.join(temp_folder, 'play_by_play.json'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChatGPT attempt at parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create team Map for name subsitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a mapping of standardized team names to their alternative names\n",
    "# team_map = {\n",
    "#     row['Team']: row['ncaa_data_alts']\n",
    "#     for _, row in school_info_df.iterrows() if not pd.isnull(row['ncaa_data_alts'])\n",
    "# }\n",
    "\n",
    "# # Standardize the format of the mapping to make it usable in regex substitutions\n",
    "# for key in team_map:\n",
    "#     team_map[key] = team_map[key].replace(', ', '|')\n",
    "\n",
    "# # Display the resulting team mapping\n",
    "# team_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final correction: Preprocess multi-word team names globally before tokenization\n",
    "# def standardize_team_names_corrected(description, team_map):\n",
    "#     \"\"\"\n",
    "#     Replace alternate team names in a description with preprocessing for multi-word names.\n",
    "#     Ensures consistent handling of multi-word abbreviations before tokenized adjustments.\n",
    "#     \"\"\"\n",
    "#     # Preprocess multi-word team names first\n",
    "#     for std_name, alt_names in team_map.items():\n",
    "#         description = re.sub(rf'\\b({alt_names})\\b', std_name, description, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Tokenized replacement for any remaining cases (if needed)\n",
    "#     tokens = description.split()\n",
    "#     for i, token in enumerate(tokens):\n",
    "#         for std_name, alt_names in team_map.items():\n",
    "#             if re.fullmatch(rf'({alt_names})', token.strip('.,'), flags=re.IGNORECASE):\n",
    "#                 tokens[i] = std_name\n",
    "#                 break\n",
    "#     return \" \".join(tokens)\n",
    "\n",
    "# # Apply the corrected function to the problematic test case\n",
    "# # Test on a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Function to convert period and time to continuous time\n",
    "def convert_to_continuous_time(period, time):\n",
    "    period_offsets = {'1': 0, '2': 20, '3': 40, 'OT': 60}\n",
    "    minutes, seconds = map(int, time.split(':'))\n",
    "    elapsed_time = (20 - minutes) * 60 + -seconds\n",
    "    offset = period_offsets.get(period, 0) * 60\n",
    "    return offset + elapsed_time\n",
    "\n",
    "# Function to normalize names to handle accents and special characters\n",
    "def normalize_name(name):\n",
    "    if not name:\n",
    "        return None\n",
    "    # Normalize Unicode accents and remove non-ASCII characters\n",
    "    normalized = unicodedata.normalize('NFKD', name).encode('ascii', 'ignore').decode('utf-8')\n",
    "    return normalized\n",
    "\n",
    "# Enhanced player name formatting function\n",
    "def clean_player_name(name):\n",
    "    \"\"\"\n",
    "    Converts a name from \"Last, First\" to \"First Last\", handling punctuation and normalization.\n",
    "    Example: \"Hughes, T.J.\" -> \"T.J. Hughes\"\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return None\n",
    "    name = normalize_name(name)\n",
    "    parts = [p.strip() for p in name.split(',')]\n",
    "    if len(parts) == 2:\n",
    "        last, first = parts\n",
    "        return f\"{first} {last}\".strip()\n",
    "    return name\n",
    "\n",
    "# Function to parse play-by-play descriptions\n",
    "# Improved to handle team abbreviations and player names with issues\n",
    "def parse_description(description):\n",
    "    \"\"\"\n",
    "    Parse a single play-by-play description into structured fields.\n",
    "    \"\"\"\n",
    "    desc_lower = description.lower().strip()\n",
    "    parsed = {\n",
    "        \"Event_type\": \"Other\",\n",
    "        \"Primary_player\": None,\n",
    "        \"Primary_team\": None,\n",
    "        \"Secondary_player\": None,\n",
    "        \"Secondary_team\": None,\n",
    "        \"Outcome\": None,\n",
    "    }\n",
    "\n",
    "    # Normalize known team abbreviations\n",
    "    team_map = {\n",
    "        'michst': 'MICHST',\n",
    "        'lake sup': 'LK SUP',\n",
    "        'lk sup': 'LK SUP',\n",
    "        'michigan state': 'MICHST',\n",
    "        'lake superior': 'LK SUP'\n",
    "    }\n",
    "\n",
    "    for key, value in team_map.items():\n",
    "        desc_lower = desc_lower.replace(key, value.lower())\n",
    "\n",
    "    # --- Faceoff ---\n",
    "    if \"faceoff\" in desc_lower:\n",
    "        parsed[\"Event_type\"] = \"Faceoff\"\n",
    "        faceoff_pattern = (\n",
    "            r\"Faceoff\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+\"\n",
    "            r\"vs\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+\"\n",
    "            r\"won by\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\.\"\n",
    "        )\n",
    "        match = re.search(faceoff_pattern, description, re.IGNORECASE)\n",
    "        if match:\n",
    "            parsed[\"Primary_player\"] = clean_player_name(match.group(1))\n",
    "            parsed[\"Secondary_player\"] = clean_player_name(match.group(2))\n",
    "            parsed[\"Primary_team\"] = match.group(3).strip()\n",
    "            parsed[\"Outcome\"] = \"won\"\n",
    "        return parsed\n",
    "\n",
    "    # --- Goal ---\n",
    "    if \"goal by\" in desc_lower:\n",
    "        parsed[\"Event_type\"] = \"Goal\"\n",
    "        goal_scorer_pattern = r\"Goal by\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+, [A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\"\n",
    "        match = re.search(goal_scorer_pattern, description, re.IGNORECASE)\n",
    "        if match:\n",
    "            parsed[\"Primary_player\"] = clean_player_name(match.group(1))\n",
    "        return parsed\n",
    "\n",
    "    # --- Penalty ---\n",
    "    if desc_lower.startswith(\"penalty on\"):\n",
    "        parsed[\"Event_type\"] = \"Penalty\"\n",
    "        penalty_pattern = (\n",
    "            r\"Penalty on\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+, [A-Za-zÀ-ÖØ-ÿ'\\.\\- ]+)\\s+\"\n",
    "            r\"([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+(\\d+) minutes for (.+)\"\n",
    "        )\n",
    "        match = re.search(penalty_pattern, description, re.IGNORECASE)\n",
    "        if match:\n",
    "            parsed[\"Primary_player\"] = clean_player_name(match.group(1))\n",
    "            parsed[\"Primary_team\"] = match.group(2).strip()\n",
    "            parsed[\"Penalty_duration\"] = match.group(3).strip()\n",
    "            parsed[\"Penalty_type\"] = match.group(4).strip()\n",
    "        return parsed\n",
    "\n",
    "    # --- Shot ---\n",
    "    if \"shot by\" in desc_lower:\n",
    "        parsed[\"Event_type\"] = \"Shot\"\n",
    "        shot_pattern = r\"Shot by\\s+([A-Za-zÀ-ÖØ-öø-ÿ'\\.\\- ]+)\\s+(.+)\"\n",
    "        match = re.search(shot_pattern, description, re.IGNORECASE)\n",
    "        if match:\n",
    "            parsed[\"Primary_player\"] = clean_player_name(match.group(1))\n",
    "            parsed[\"Primary_team\"] = match.group(2).strip()\n",
    "        return parsed\n",
    "\n",
    "    return parsed\n",
    "\n",
    "# Function to transform a single game's JSON data into a dataframe\n",
    "def transform_single_game(json_data, game_id):\n",
    "    rows = []\n",
    "\n",
    "    for period in json_data['periods']:\n",
    "        period_number = period['periodNumber']\n",
    "        for play in period['playStats']:\n",
    "            row = {\n",
    "                'Game_ID': game_id,\n",
    "                'Period': period_number,\n",
    "                'Time': play['time'],\n",
    "                'Description': play['visitorText'] or play['homeText'],\n",
    "                'Score': play['score']\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    game_df = pd.DataFrame(rows)\n",
    "\n",
    "    # Convert period and time to continuous time\n",
    "    game_df['Period'] = game_df['Period'].replace({'1st': '1', '2nd': '2', '3rd': '3', 'OT': 'OT'})\n",
    "    game_df['Time'] = game_df.apply(lambda row: convert_to_continuous_time(row['Period'], row['Time']), axis=1)\n",
    "\n",
    "    # Parse descriptions\n",
    "    parsed_descriptions = game_df['Description'].apply(parse_description)\n",
    "    parsed_df = pd.DataFrame(parsed_descriptions.tolist())\n",
    "\n",
    "    # Combine with original game_df\n",
    "    return pd.concat([game_df, parsed_df], axis=1)\n",
    "\n",
    "# Function to process all games and combine into a single dataframe\n",
    "def process_all_games(schedule_df):\n",
    "    all_games = []\n",
    "\n",
    "    for _, row in schedule_df.iterrows():\n",
    "        game_id = row['Game_ID']\n",
    "        json_data = row['Play_By_Play_JSON']\n",
    "\n",
    "        if json_data:\n",
    "            game_df = transform_single_game(json_data, game_id)\n",
    "            all_games.append(game_df)\n",
    "\n",
    "    return pd.concat(all_games, ignore_index=True)\n",
    "\n",
    "# Example usage\n",
    "# Assuming `updated_schedule_df` is the dataframe containing the JSON play-by-play data\n",
    "final_pbp_df = process_all_games(updated_schedule_df)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "# final_pbp_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## look at tail\n",
    "# final_pbp_df.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle 'SAVE' case\n",
    "def move_save(row):\n",
    "    if pd.notnull(row['Primary_team']) and ', save' in row['Primary_team']:\n",
    "        row['Secondary_player'] = row['Primary_team'].split(', save')[1].strip()\n",
    "        row['Primary_team'] = row['Primary_team'].split(', save')[0].strip()\n",
    "\n",
    "\n",
    "        row['Primary_team'] = row['Primary_team'].replace(', save', '').strip()\n",
    "    return row\n",
    "\n",
    "# Function to handle 'BLOCKED' case\n",
    "def move_blocked(row):\n",
    "    if pd.notnull(row['Primary_team']) and 'BLOCKED' in row['Primary_team']:\n",
    "        blocked_match = re.search(r'BLOCKED by (.+)', row['Primary_team'])\n",
    "        if blocked_match:\n",
    "            row['Secondary_player'] = blocked_match.group(1).strip()\n",
    "            row['Primary_team'] = re.sub(r'BLOCKED by .+', 'BLOCKED', row['Primary_team']).strip()\n",
    "    return row\n",
    "\n",
    "# Function to extract and move the outcome to 'Outcome' column\n",
    "def move_outcome(row):\n",
    "    if pd.notnull(row['Primary_team']):\n",
    "        outcome_match = re.search(r'\\b(MISSED|WIDE|BLOCKED|SAVE)\\b', row['Primary_team'])\n",
    "        if outcome_match:\n",
    "            row['Outcome'] = outcome_match.group(1)\n",
    "            row['Primary_team'] = re.sub(r'\\b(MISSED|WIDE|BLOCKED|SAVE)\\b', '', row['Primary_team']).strip()\n",
    "    return row\n",
    "\n",
    "# Apply transformations sequentially\n",
    "final_pbp_df = final_pbp_df.apply(move_save, axis=1)\n",
    "final_pbp_df = final_pbp_df.apply(move_blocked, axis=1)\n",
    "final_pbp_df = final_pbp_df.apply(move_outcome, axis=1)\n",
    "\n",
    "# # Display the first few rows of the cleaned dataframe\n",
    "\n",
    "\n",
    "# # # Notes for second step of transformation\n",
    "# # # Faceoff Seem to be working as intended\n",
    "# # # Goal - Primary_player is working as intended\n",
    "# #     # - Primary Team is not being captured - probably because the team name is used and not the abbreviation\n",
    "# # # Shots - Primary_player actually contains the team abbreviation\n",
    "# #     # - Primary_team includes the player name and still includes the shot outcome WIDE, BLOCKED, MISSED in the\n",
    "# #     #  - UPPER CASE - Need to remove the outcome from the team name and move to outcome column\n",
    "# #     #  - MISSED IS THE SAME AS SAVED - also includes the goalie name after the outcome\n",
    "# #     #  - BLOCKED also includes the secondary player name after the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Next Steps \n",
    "# For all Event_type: Shot swap the Primary_player and Primary_team values\n",
    "\n",
    "# Function to swap 'Primary_player' and 'Primary_team' for 'Shot' events\n",
    "def swap_shot_columns(row):\n",
    "    if row['Event_type'] == 'Shot':\n",
    "        row['Primary_player'], row['Primary_team'] = row['Primary_team'], row['Primary_player']\n",
    "    return row\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "final_pbp_df = final_pbp_df.apply(swap_shot_columns, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to deal with foriegn names like Tommi Mannisto (which has accents and appears like MÃ£Â„nnistÃ£Â–, Tommi. in the data\n",
    "\n",
    "def fix_encoding_issues(df, columns):\n",
    "    \"\"\"\n",
    "    Fix encoding issues in specified columns of a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing columns with text issues.\n",
    "        columns (list): List of column names to fix.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with fixed text in specified columns.\n",
    "    \"\"\"\n",
    "    def decode_text(text):\n",
    "        try:\n",
    "            # Decode from 'latin1' and re-encode to 'utf-8'\n",
    "            return text.encode('latin1').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError, AttributeError):\n",
    "            # Return text as is if decoding fails\n",
    "            return text\n",
    "    \n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(decode_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "columns_to_fix = ['Primary_player', 'Secondary_player']\n",
    "\n",
    "final_pbp_df = fix_encoding_issues(final_pbp_df, columns_to_fix)# Display the first few rows of the cleaned dataframe\n",
    "# final_pbp_df.head(12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_names(df, columns):\n",
    "    \"\"\"\n",
    "    Standardize player names in the specified columns to 'First Last' format.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing player name columns.\n",
    "        columns (list): List of column names to standardize.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with standardized player names.\n",
    "    \"\"\"\n",
    "    def fix_name_format(name):\n",
    "        if pd.isnull(name):  # Handle missing values\n",
    "            return name\n",
    "        name = name.replace(\".\", \"\")  # Remove periods\n",
    "        if \",\" in name:  # If the name is in 'Last, First' format\n",
    "            parts = name.split(\",\")\n",
    "            return f\"{parts[1].strip()} {parts[0].strip()}\"  # Rearrange to 'First Last'\n",
    "        return name.strip()  # Return as is if already in 'First Last' format\n",
    "\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(fix_name_format)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "final_pbp_df = standardize_names(final_pbp_df, columns_to_fix)\n",
    "# Display the first few rows of the cleaned dataframe\n",
    "final_pbp_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_goal_events(df):\n",
    "#     \"\"\"\n",
    "#     Parse 'Goal' event descriptions to extract Primary_player (goal scorer)\n",
    "#     and Primary_team (team that scored) based on the \"On ice for\" section.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The dataframe containing 'Event_type', 'Description', 'Primary_player', and 'Primary_team' columns.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: The updated dataframe with parsed Primary_player and Primary_team.\n",
    "#     \"\"\"\n",
    "#     def extract_goal_info(row):\n",
    "#         if row['Event_type'] == 'Goal':  # Only process rows with Event_type 'Goal'\n",
    "#             description = row['Description']\n",
    "            \n",
    "#             # Extract the goal scorer (Primary_player) in \"Last, First\" format\n",
    "#             goal_match = re.search(r\"Goal by (.+?) \\(\", description)\n",
    "#             if goal_match:\n",
    "#                 last_first_name = goal_match.group(1).strip()\n",
    "#                 # Convert \"Last, First\" to \"First Last\"\n",
    "#                 parts = last_first_name.split(\", \")\n",
    "#                 first_last_name = f\"{parts[1]} {parts[0]}\" if len(parts) == 2 else last_first_name\n",
    "#                 row['Primary_player'] = first_last_name\n",
    "            \n",
    "#             # Extract teams and players under \"On ice for\"\n",
    "#             on_ice_sections = re.findall(r\"On ice for ([A-Z ]+): (.+?)(?=(?:[A-Z ]+:|$))\", description)\n",
    "#             if on_ice_sections:\n",
    "#                 for team, players in on_ice_sections:\n",
    "#                     player_list = [player.strip() for player in players.split(\";\")]\n",
    "#                     if row['Primary_player'] in player_list:\n",
    "#                         row['Primary_team'] = team.strip()  # Assign team where goal scorer is listed\n",
    "#                         return row  # Exit once the correct team is found\n",
    "            \n",
    "#             # If no match is found, leave Primary_team as blank\n",
    "#             row['Primary_team'] = None\n",
    "#         return row\n",
    "\n",
    "#     # Apply the extraction function row-wise\n",
    "#     df = df.apply(extract_goal_info, axis=1)\n",
    "#     return df\n",
    "\n",
    "# # Apply the function to parse goal events\n",
    "# final_pbp_df = parse_goal_events(final_pbp_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# def parse_goal_events_with_inferred_away_team(df):\n",
    "#     \"\"\"\n",
    "#     Parse 'Goal' event descriptions to extract Primary_player (goal scorer)\n",
    "#     and Primary_team (team that scored), inferring the Away_team from the Game_ID.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The dataframe containing 'Event_type', 'Description', 'Primary_player', \n",
    "#                            'Primary_team', and 'Game_ID' columns.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: The updated dataframe with parsed Primary_player and Primary_team.\n",
    "#     \"\"\"\n",
    "#     def extract_goal_info(row):\n",
    "#         if row['Event_type'] == 'Goal':  # Only process rows with Event_type 'Goal'\n",
    "#             description = row['Description']\n",
    "            \n",
    "#             # Extract the goal scorer (Primary_player) in \"Last, First\" format\n",
    "#             goal_match = re.search(r\"Goal by (.+?) \\(\", description)\n",
    "#             if goal_match:\n",
    "#                 last_first_name = goal_match.group(1).strip()\n",
    "#                 # Convert \"Last, First\" to \"First Last\"\n",
    "#                 parts = last_first_name.split(\", \")\n",
    "#                 first_last_name = f\"{parts[1]} {parts[0]}\" if len(parts) == 2 else last_first_name\n",
    "#                 row['Primary_player'] = first_last_name\n",
    "            \n",
    "#             # Extract teams and players under \"On ice for\"\n",
    "#             on_ice_sections = re.findall(r\"On ice for ([A-Z ]+): (.+?)(?=(?:[A-Z ]+:|$))\", description)\n",
    "#             if on_ice_sections:\n",
    "#                 for team, players in on_ice_sections:\n",
    "#                     player_list = [player.strip() for player in players.split(\";\")]\n",
    "#                     if row['Primary_player'] in player_list:\n",
    "#                         row['Primary_team'] = team.strip()  # Assign team where goal scorer is listed\n",
    "#                         break\n",
    "        \n",
    "#         return row\n",
    "\n",
    "#     # Apply the extraction function row-wise to parse goals\n",
    "#     df = df.apply(extract_goal_info, axis=1)\n",
    "\n",
    "#     # Infer Away_team from Game_ID\n",
    "#     def infer_away_team(game_id):\n",
    "#         try:\n",
    "#             return game_id.split(\"-\")[3]\n",
    "#         except IndexError:\n",
    "#             return None\n",
    "\n",
    "#     df['Away_team'] = df['Game_ID'].apply(infer_away_team)\n",
    "\n",
    "#     # Fill missing Primary_team values with the inferred Away_team\n",
    "#     df['Primary_team'] = df['Primary_team'].fillna(df['Away_team'])\n",
    "    \n",
    "#     # Drop the temporary 'Away_team' column if it’s no longer needed\n",
    "#     df = df.drop(columns=['Away_team'])\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Apply the function to parse goal events and infer away team\n",
    "# final_pbp_df = parse_goal_events_with_inferred_away_team(final_pbp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# def parse_goal_events_with_fill(df):\n",
    "#     \"\"\"\n",
    "#     Parse 'Goal' event descriptions to extract Primary_player (goal scorer)\n",
    "#     and Primary_team (team that scored), filling missing Primary_team values with the away team.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The dataframe containing 'Event_type', 'Description', 'Primary_player', \n",
    "#                            'Primary_team', and 'Home_team'/'Away_team' columns.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: The updated dataframe with parsed Primary_player and Primary_team.\n",
    "#     \"\"\"\n",
    "#     def extract_goal_info(row):\n",
    "#         if row['Event_type'] == 'Goal':  # Only process rows with Event_type 'Goal'\n",
    "#             description = row['Description']\n",
    "            \n",
    "#             # Extract the goal scorer (Primary_player) in \"Last, First\" format\n",
    "#             goal_match = re.search(r\"Goal by (.+?) \\(\", description)\n",
    "#             if goal_match:\n",
    "#                 last_first_name = goal_match.group(1).strip()\n",
    "#                 # Convert \"Last, First\" to \"First Last\"\n",
    "#                 parts = last_first_name.split(\", \")\n",
    "#                 first_last_name = f\"{parts[1]} {parts[0]}\" if len(parts) == 2 else last_first_name\n",
    "#                 row['Primary_player'] = first_last_name\n",
    "            \n",
    "#             # Extract teams and players under \"On ice for\"\n",
    "#             on_ice_sections = re.findall(r\"On ice for ([A-Z ]+): (.+?)(?=(?:[A-Z ]+:|$))\", description)\n",
    "#             if on_ice_sections:\n",
    "#                 for team, players in on_ice_sections:\n",
    "#                     player_list = [player.strip() for player in players.split(\";\")]\n",
    "#                     if row['Primary_player'] in player_list:\n",
    "#                         row['Primary_team'] = team.strip()  # Assign team where goal scorer is listed\n",
    "#                         break\n",
    "#         return row\n",
    "\n",
    "#     # Apply the extraction function row-wise to parse goals\n",
    "#     df = df.apply(extract_goal_info, axis=1)\n",
    "\n",
    "#     # Fill missing Primary_team values with the Away_team\n",
    "#     df['Primary_team'] = df['Primary_team'].fillna(df['Away_team'])\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Apply the function to parse goal events and fill missing values\n",
    "# final_pbp_df = parse_goal_events_with_fill(final_pbp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def classify_power_play_events(df):\n",
    "    \"\"\"\n",
    "    Classify 'Other' Event_type as 'PP - Start' or 'PP - End' based on the Description,\n",
    "    and extract the team abbreviation to the Primary_team column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing 'Event_type', 'Description', and 'Primary_team' columns.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with classified 'Event_type' and filled 'Primary_team'.\n",
    "    \"\"\"\n",
    "    def classify_event(row):\n",
    "        if row['Event_type'] == 'Other':\n",
    "            description = row['Description']\n",
    "            # Check for \"Start power play for\"\n",
    "            if re.search(r\"Start power play for\", description):\n",
    "                row['Event_type'] = 'PP - Start'\n",
    "                row['Primary_team'] = description.split('for')[-1].strip().rstrip('.')\n",
    "            # Check for \"End power play for\"\n",
    "            elif re.search(r\"End power play for\", description):\n",
    "                row['Event_type'] = 'PP - End'\n",
    "                row['Primary_team'] = description.split('for')[-1].strip().rstrip('.')\n",
    "        return row\n",
    "\n",
    "    # Apply the classification function row-wise\n",
    "    df = df.apply(classify_event, axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply the function to classify power play events\n",
    "final_pbp_df = classify_power_play_events(final_pbp_df)\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "# final_pbp_df.head(22)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data\n",
    "final_pbp_df.info()\n",
    "# Value counts\n",
    "# final_pbp_df['Event_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deal with Goalie change / info rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def classify_goalie_moves(df):\n",
    "    \"\"\"\n",
    "    Parse goalie moves from the Description column and classify them as 'Goalie Move'.\n",
    "    Extract the goalie name as Primary_player and the team name as Primary_team.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing 'Event_type', 'Description', 'Primary_player', and 'Primary_team' columns.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with classified 'Event_type', 'Primary_player', and 'Primary_team'.\n",
    "    \"\"\"\n",
    "    def parse_goalie_move(row):\n",
    "        if row['Event_type'] == 'Other':  # Only process rows marked as 'Other'\n",
    "            description = row['Description']\n",
    "            # Look for patterns like 'Name at goalie for Team'\n",
    "            match = re.match(r\"(.+?) at goalie for (.+?)\\.\", description)\n",
    "            if match:\n",
    "                row['Event_type'] = 'Goalie Move'\n",
    "                row['Primary_player'] = match.group(1).strip()  # Extract the goalie's name\n",
    "                row['Primary_team'] = match.group(2).strip()  # Extract the team name\n",
    "        return row\n",
    "\n",
    "    # Apply the parsing function row-wise\n",
    "    df = df.apply(parse_goalie_move, axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply the function to classify goalie moves\n",
    "final_pbp_df = classify_goalie_moves(final_pbp_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## In the Outcome column relabel MISSED to SAVED for clairty\n",
    "def relabel_missed_to_saved(df):\n",
    "    \"\"\"\n",
    "    Relabel 'MISSED' to 'SAVED' in the Outcome column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the Outcome column.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with relabeled outcomes.\n",
    "    \"\"\"\n",
    "    df['Outcome'] = df['Outcome'].replace('MISSED', 'SAVED')\n",
    "    return df\n",
    "\n",
    "# Apply the relabeling function\n",
    "final_pbp_df = relabel_missed_to_saved(final_pbp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the Penalty_type column to remove periods and any leading/trailing whitespace\n",
    "def clean_penalty_type(df):\n",
    "    \"\"\"\n",
    "    Clean the Penalty_type column by removing periods and extra whitespace.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the Penalty_type column.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with cleaned Penalty_type.\n",
    "    \"\"\"\n",
    "    df['Penalty_type'] = df['Penalty_type'].str.replace('.', '', regex=False).str.strip()\n",
    "    return df\n",
    "\n",
    "# Apply the cleaning function\n",
    "final_pbp_df = clean_penalty_type(final_pbp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show 5 goal events\n",
    "final_pbp_df[final_pbp_df['Event_type'] == 'Goal'].head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Wed 1-29\n",
    "- clean the goal events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grab the Goal Conditions within the parenthesis in the definition\n",
    "### Put in a new column called Goal_Conditions\n",
    "def extract_goal_conditions(df):\n",
    "    \"\"\"\n",
    "    Extract goal conditions from the Description column and add them to a new column called Goal_Conditions.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing the Description column.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with the Goal_Conditions column.\n",
    "    \"\"\"\n",
    "    def extract_conditions(description):\n",
    "        match = re.search(r'\\((.*?)\\)', description)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "    df['Goal_Conditions'] = df['Description'].apply(extract_conditions)\n",
    "    return df\n",
    "\n",
    "# Apply the extraction function\n",
    "final_pbp_df = extract_goal_conditions(final_pbp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to just Goal events to check the Goal_Conditions column\n",
    "final_pbp_df[final_pbp_df['Event_type'] == 'Goal'].head(12)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_primary_team(df):\n",
    "    \"\"\"\n",
    "    Assigns the correct 'Primary_team' to goal events by analyzing score changes.\n",
    "    \"\"\"\n",
    "    # Extract away and home team names from Game_ID\n",
    "    def extract_teams(game_id):\n",
    "        parts = game_id.split('-')\n",
    "        away_team = parts[3]\n",
    "        home_team = parts[4]\n",
    "        return away_team, home_team\n",
    "\n",
    "    # Filter only goal events\n",
    "    goal_df = df[df['Event_type'] == 'Goal'].copy()\n",
    "\n",
    "    # Sort by game and time sequence\n",
    "    goal_df.sort_values(by=['Game_ID', 'Period', 'Time'], inplace=True)\n",
    "\n",
    "    # Initialize previous scores dictionary\n",
    "    prev_scores = {}\n",
    "\n",
    "    # Iterate over goal events\n",
    "    for idx, row in goal_df.iterrows():\n",
    "        game_id = row['Game_ID']\n",
    "        score_str = row['Score']\n",
    "        \n",
    "        if pd.isna(score_str):\n",
    "            continue\n",
    "\n",
    "        # Parse score into integers\n",
    "        away_score, home_score = map(int, score_str.split('-'))\n",
    "\n",
    "        # Extract teams\n",
    "        away_team, home_team = extract_teams(game_id)\n",
    "\n",
    "        # Check previous score to determine which team scored\n",
    "        if game_id in prev_scores:\n",
    "            prev_away, prev_home = prev_scores[game_id]\n",
    "\n",
    "            if away_score > prev_away:\n",
    "                goal_df.at[idx, 'Primary_team'] = away_team\n",
    "            elif home_score > prev_home:\n",
    "                goal_df.at[idx, 'Primary_team'] = home_team\n",
    "        else:\n",
    "            # First goal of the game, determine scorer by score value\n",
    "            if away_score > home_score:\n",
    "                goal_df.at[idx, 'Primary_team'] = away_team\n",
    "            else:\n",
    "                goal_df.at[idx, 'Primary_team'] = home_team\n",
    "\n",
    "        # Update previous score\n",
    "        prev_scores[game_id] = (away_score, home_score)\n",
    "\n",
    "    # Merge updated Primary_team back into original dataframe\n",
    "    df.update(goal_df[['Primary_team']])\n",
    "    return df\n",
    "\n",
    "# Apply function to the dataframe\n",
    "final_pbp_df = assign_primary_team(final_pbp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Not working correctly, only getting values when home team scores\n",
    "# def transform_primary_player(name):\n",
    "#     \"\"\"Convert 'Last, First' to 'First Last'.\"\"\"\n",
    "#     if pd.isna(name) or ',' not in name:\n",
    "#         return name\n",
    "#     last, first = name.split(', ', 1)\n",
    "#     return f\"{first} {last}\"\n",
    "\n",
    "# def extract_correct_primary_team_v2(description, primary_player):\n",
    "#     \"\"\"\n",
    "#     Improved method to correctly extract the goal-scoring team abbreviation.\n",
    "#     - Finds where the primary player's name appears.\n",
    "#     - Identifies the corresponding team abbreviation.\n",
    "#     - Ensures it works for both home and away teams.\n",
    "#     \"\"\"\n",
    "#     if pd.isna(description) or pd.isna(primary_player):\n",
    "#         return None\n",
    "\n",
    "#     # # Convert primary player format for matching\n",
    "#     # primary_player = transform_primary_player(primary_player)\n",
    "\n",
    "#     # Find all team sections in the format \"TEAM: player1; player2; ...\"\n",
    "#     team_sections = re.findall(r'([A-Z\\s]+):\\s([^:]+)', description)\n",
    "\n",
    "#     # Iterate over both teams\n",
    "#     for team_abbr, player_list in team_sections:\n",
    "#         team_abbr = team_abbr.strip()\n",
    "\n",
    "#         # Check if the goal scorer's name is in this team's player list\n",
    "#         player_names = [transform_primary_player(name.strip()) for name in player_list.split(';')]\n",
    "#         if primary_player in player_names:\n",
    "#             return team_abbr\n",
    "\n",
    "#     return None\n",
    "\n",
    "# # Call the function to extract the correct primary team\n",
    "# final_pbp_df['Primary_team'] = final_pbp_df.apply(\n",
    "#     lambda row: extract_correct_primary_team_v2(row['Description'], row['Primary_player']), axis=1\n",
    "# )\n",
    "\n",
    "# # Display the first few rows of the updated dataframe\n",
    "# final_pbp_df[final_pbp_df['Event_type'] == 'Goal'].head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pbp_df[final_pbp_df['Event_type'] == 'Goal'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEAM_MAP is in memory (created earlier from school_info_df)\n",
    "### This block and be used to add additional team mappings before doing the substitution\n",
    "\n",
    "### NEED TO ADDRESS MINNESOTA DUTITH, Amerrican International, St Lawerence - THE PARSING IS COMPLETELY FAILING IN AT LEAST SOME OF THEIR GAME\n",
    "#### ST is also a mess with mutiple teams . Alas is also showing up for bothe Alaska teams\n",
    "# add 'michigan st': 'Michigan State', to the team_map\n",
    "\n",
    "team_map['michigan st'] = 'Michigan State'\n",
    "team_map['linwod'] = 'Lindenwood'\n",
    "team_map['sup'] = 'Lake Superior'\n",
    "team_map['afa'] = 'Air Force'\n",
    "team_map['anc'] = 'Alaska Anchorage'\n",
    "team_map['asu'] = 'Arizona State'\n",
    "team_map['aug'] = 'Augustana'\n",
    "team_map['ben'] = 'Bentley'\n",
    "team_map['bgsu santa'] = 'Bowling Green'\n",
    "team_map['brown st.'] = 'Brown'\n",
    "team_map['can'] = 'Canisius'\n",
    "team_map['clk'] = 'Clarkson'\n",
    "team_map['col'] = 'Colgate'\n",
    "team_map['dak'] = 'North Dakota'\n",
    "team_map['dame'] = 'Notre Dame'\n",
    "team_map['den'] = 'Denver'\n",
    "team_map['dul'] = 'Minnesota Duluth'\n",
    "team_map['fsu'] = 'Ferris State'\n",
    "team_map['har'] = 'Harvard'\n",
    "team_map['har st.'] = 'Harvard'\n",
    "team_map['int'] = 'American Intl'\n",
    "team_map['lin'] = 'Lindenwood'\n",
    "team_map['lwu'] = 'Lindenwood'\n",
    "team_map['mia'] = 'Maine'\n",
    "team_map['michst a'] = 'Michigan State'\n",
    "team_map['minn pa'] = 'Minnesota'\n",
    "team_map['neu'] = 'Northeastern'\n",
    "team_map['no dak jamernik'] = 'North Dakota'\n",
    "team_map['oh'] = 'Miami'\n",
    "team_map['omaha van'] = 'Omaha'\n",
    "team_map['pri'] = 'Princeton'\n",
    "team_map['pri de la'] = 'Princeton'\n",
    "team_map['prince de la'] = 'Princeton'\n",
    "team_map['qui'] = 'Quinnipiac'\n",
    "team_map['scs'] = 'St. Cloud State'\n",
    "team_map['sd'] = 'Augustana'\n",
    "team_map['shu'] = 'Sacred Heart'\n",
    "team_map['slu'] = 'St. Lawrence'\n",
    "team_map['stc'] = 'Stonehill'\n",
    "team_map['sup'] = 'Lake Superior'\n",
    "team_map['u-m'] = 'Michigan'\n",
    "team_map['uma'] = 'Massachusetts'\n",
    "team_map['umd'] = 'Minnesota Duluth'\n",
    "team_map['und'] = 'Notre Dame'\n",
    "team_map['uni'] = 'Union'\n",
    "team_map['ust'] = 'St. Thomas'\n",
    "team_map['vermnt la'] = 'Vermont'\n",
    "team_map['wis'] = 'Wisconsin'\n",
    "team_map['wmu'] = 'Western Michigan'\n",
    "team_map['yal'] = 'Yale'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# team_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_primary_team(df, team_map):\n",
    "    \"\"\"\n",
    "    Standardizes the 'Primary_team' column using the provided team_map.\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and map to standardized names\n",
    "    df['Primary_team'] = df['Primary_team'].str.lower().map(team_map).fillna(df['Primary_team'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Run the function to standardize the 'Primary_team' column\n",
    "final_pbp_df = standardize_primary_team(final_pbp_df, team_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pbp_df.sample(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the dataframe to a CSV file\n",
    "final_pbp_df.to_csv(os.path.join(temp_folder, 'pbp_data_test_3.2.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save Updated Schedule DF (With PbP JSONs) to csv to avoid scraping for new tests\n",
    "\n",
    "updated_schedule_df.to_csv(os.path.join(data_folder, 'schedule_from_ncaa_with_PbP_JSON.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_viz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
